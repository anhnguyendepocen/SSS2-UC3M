# Cluster analysis {#cluster}

Cluster analysis is the collection of techniques designed to find subgroups or *clusters* in a dataset of variables $X_1,\ldots,X_k$. Depending on the similarities between the observations, these are partitioned in homogeneous groups as separated as possible between them. Clustering methods can be classified into two main categories:

- **Partition methods**. Given a fixed number of cluster $k$, these methods aim to assign each observation of $X_1,\ldots,X_k$ to a unique cluster, in such a way that the *within-cluster variation* is as small as possible (the clusters are as homogeneous as possible) while the *between cluster variation* is as large as possible (the clusters are as separated as possible).
- **Hierarchical methods**. These methods construct a hierarchy for the observations in terms of their similitudes. This results in a tree-based representation of the data in terms of a *dendogram*, which depicts how the observations are clustered at different levels -- from the smallest groups of one element to the largest representing the whole dataset.

We will see the basics of the most well-known partition method, namely *$k$-means clustering*, and of the *agglomerative hierarchical clustering*.

## $k$-means clustering {#cluster-kmeans}

The $k$-means clustering looks for **$k$ clusters in the data such that they are as compact as possible and as separated as possible**. In clustering terminology, the clusters minimize the *with-in cluster variation* with respect to the cluster centroid while they maximize the *between cluster variation* among clusters. The distance used for measuring proximity is the usual **Euclidean distance** between points. As a consequence, this clustering method tend to yield spherical or rounded clusters and is not adequate for categorical variables.

```{r, kmeans, echo = FALSE, results = 'hide', out.width = '70%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.pos = 'h!', fig.cap = 'The $k$-means partitions for a two-dimensional dataset with $k=1,2,3,4$. Centers of each cluster are displayed with an asterisk.', cache = TRUE}
# Data with 3 clusters
set.seed(23456789)
n <- 20
x <- rbind(matrix(rnorm(n, sd = 0.3), ncol = 2),
           cbind(rnorm(n, sd = 0.3), rnorm(n, mean = 2, sd = 0.3)),
           matrix(rnorm(n, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")

par(mfrow = c(2, 2))
for (k in 1:4) {
  set.seed(23456789)
  cl <- kmeans(x, centers = k, nstart = 20)
  plot(x, col = cl$cluster, pch = 16, cex = 0.75, main = paste("k =", k))
  points(cl$centers, col = 1:k, pch = 8, cex = 2)
}
```
Let's analyze the possible clusters in a smaller subset of *La Liga 2015/2016* ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/la-liga-2015-2016.xlsx)) dataset, where the results can be easily visualized. To that end, import the data as `laliga`.

```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
library(RcmdrMisc)
library(car)
laliga <- readXL("datasets/la-liga-2015-2016.xlsx")
rownames(laliga) <- laliga$Team
laliga$Team <- NULL
```

```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.pos = 'h!', fig.asp = 1, cache = TRUE}
# We consider only a smaller dataset (Points and Yellow.cards)
head(laliga, 2)
pointsCards <-  laliga[, c(1, 17)]
plot(pointsCards)

# kmeans uses a random initialization of the clusters, so the results may vary
# from one call to another. We use set.seed() to have reproducible outputs.
set.seed(2345678)

# kmeans call:
# - centers is the k, the number of clusters.
# - nstart indicates how many different starting assignments should be considered
# (useful for avoiding suboptimal clusterings)
k <- 2
km <- kmeans(pointsCards, centers = k, nstart = 20)

# What is inside km?
km
str(km)

# between_SS / total_SS gives a criterion to select k similar to PCA.
# Recall that between_SS / total_SS = 100% if k = n

# Centroids of each cluster
km$centers

# Assignments of observations to the k clusters
km$cluster

# Plot data with colors according to clusters
plot(pointsCards, col = km$cluster)

# Add the names of the observations above the points
text(x = pointsCards, labels = rownames(pointsCards), col = km$cluster,
     pos = 3, cex = 0.75)

# Clustering with k = 3
k <- 3
set.seed(2345678)
km <- kmeans(pointsCards, centers = k, nstart = 20)
plot(pointsCards, col = km$cluster)
text(x = pointsCards, labels = rownames(pointsCards), col = km$cluster,
     pos = 3, cex = 0.75)

# Clustering with k = 4
k <- 4
set.seed(2345678)
km <- kmeans(pointsCards, centers = k, nstart = 20)
plot(pointsCards, col = km$cluster)
text(x = pointsCards, labels = rownames(pointsCards), col = km$cluster,
     pos = 3, cex = 0.75)
```

So far, we have only taken the information of two variables for performing clustering. Using PCA, we can visualize the clustering performed with all the available variables in the dataset.

By default, `kmeans` **does not standardize variables, which will affect the clustering result**. As a consequence, the clustering of a dataset will be different if one variable is expressed in millions or in tenths. If you want to avoid this distortion, you can use `scale` to automatically center and standardize a data frame (the result will be a matrix, so you need to transform it to a data frame again).

```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.pos = 'h!', fig.asp = 1, cache = TRUE}
# Work with standardized data (and remove Matches)
laligaStd <- data.frame(scale(laliga[, -2]))

# Clustering with all the variables - unstandardized data
set.seed(345678)
kme <- kmeans(laliga, centers = 3, nstart = 20)
kme$cluster
table(kme$cluster)

# Clustering with all the variables - standardized data
set.seed(345678)
kme <- kmeans(laligaStd, centers = 3, nstart = 20)
kme$cluster
table(kme$cluster)

# PCA
pca <- princomp(laliga[, -2], cor = TRUE)
summary(pca)

# Biplot (the scores of the first two PCs)
biplot(pca)

# Redo the biplot with colors indicating the cluster assignments
plot(pca$scores[, 1:2], col = kme$cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = kme$cluster)

# Recall: this is a visualization with PC1 and PC2 of the clustering done with
# all the variables, not just PC1 and PC2

# Clustering with only the first two PCs - different and less accurate result,
# but still insightful
set.seed(345678)
kme2 <- kmeans(pca$scores[, 1:2], centers = 3, nstart = 20)
plot(pca$scores[, 1:2], col = kme2$cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = kme2$cluster)
```

$k$-means can also be performed through the help of `R Commander`. To do so, go to `'Statistics' -> 'Dimensional Analysis' -> 'Clustering' -> 'k-means cluster analysis...'`. If you do this for the `USArrests` dataset after rescaling it, select to `'Assign clusters to the data set'` and name the `'Assignment variable'` as `'KMeans'`, you should get something like this:
```{r, collapse = TRUE, cache = TRUE, out.width = '70%', fig.align = 'center', fig.pos = 'h!', fig.asp = 1}
# Load data and scale it
data(USArrests)
USArrests <- as.data.frame(scale(USArrests))

# Statistics -> Dimensional Analysis -> Clustering -> k-means cluster analysis...
.cluster <-  KMeans(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop, USArrests),
  centers = 2, iter.max = 10, num.seeds = 10)
.cluster$size # Cluster Sizes
.cluster$centers # Cluster Centroids
.cluster$withinss # Within Cluster Sum of Squares
.cluster$tot.withinss # Total Within Sum of Squares
.cluster$betweenss # Between Cluster Sum of Squares
remove(.cluster)
.cluster <-  KMeans(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop, USArrests),
  centers = 2, iter.max = 10, num.seeds = 10)
.cluster$size # Cluster Sizes
.cluster$centers # Cluster Centroids
.cluster$withinss # Within Cluster Sum of Squares
.cluster$tot.withinss # Total Within Sum of Squares
.cluster$betweenss # Between Cluster Sum of Squares
biplot(princomp(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop, USArrests)),
  xlabs = as.character(.cluster$cluster))
USArrests$KMeans <- assignCluster(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop,
  USArrests), USArrests, .cluster$cluster)
remove(.cluster)
```

```{block, type = 'rmdcaution'}
How many clusters $k$ do we need in practice? There is not a single answer: the advice is to try several and compare. Inspecting the `'between_SS / total_SS'` for a good trade-off between the number of clusters and the percentage of total variation explained usually gives a good starting point for deciding on $k$.
```

```{block, type = 'rmdexercise'}
For the `iris` dataset, do sequentially:

1. Apply `scale` to the dataset and save it as `irisStd`. Note: the fifth variable is a factor, so you must skip it.
2. Fix the seed to 625365712.
3. Run $k$-means with 20 runs for $k=2,3,4$. Save the results as `km2`, `km3` and `km4`.
4. Compute the PCA of `irisStd`.
5. Plot the first two scores, colored by the assignments of `km2`.
6. Do the same for `km3` and `km4`.
7. Which $k$ do you think it gives the most sensible partition based on the previous plots?

```

## Agglomerative hierarchical clustering {#cluster-hierarchical}

```{r, trees, echo = FALSE, results = 'hide', out.width = '70%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.pos = 'h!', fig.cap = 'The hierarchical clustering for a two-dimensional dataset with complete, single and average linkages.', cache = TRUE}
# Data with 3 clusters
set.seed(23456789)
n <- 10
x <- rbind(matrix(rnorm(n, sd = 0.3), ncol = 2),
           cbind(rnorm(n, sd = 0.3), rnorm(n, mean = 2, sd = 0.3)),
           matrix(rnorm(n, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")

par(mfrow = c(2, 2))
plot(x, type = "n", cex = 0.75, main = "Data")
text(x, labels = as.character(1:(2 * n)))
for (method in c("average", "complete", "single")) {
  set.seed(23456789)
  cl <- hclust(dist(x), method = method)
  plot(cl, main = method)
}
```

Hierarchical clustering starts by considering that each observation is its own cluster, and then merges sequentially the clusters with a lower degree of *disimilarity* $d$ (the lower the similarity, the larger the similarity). For example, if there are three clusters, $A$, $B$ and $C$, and their dissimilarities are $d(A,B)=0.1$, $d(A,C)=0.5$, $d(B,C)=0.9$, then the three clusters will be reduced to just two: $(A,B)$ and $C$.

The advantages of hierarchical clustering are several:

- We do not need to specify a fixed number of clusters $k$.
- The clusters are naturally nested within each other, something that does not happen in $k$-means. Is possible to visualize this nested structure throughout the *dendogram*.
- It can deal with categorical variables, throughout the specification of proper dissimilarity measures. In particular, it can deal with numerical variables using the Euclidean distance.

The linkage employed by hierarchical clustering refers to how the cluster are fused:

- **Complete**. Takes the *maximal dissimilarity* between all the pairwise dissimilarities
between the observations in cluster A and cluster B.
- **Single**. Takes the *minimal dissimilarity* between all the pairwise dissimilarities
between the observations in cluster A and cluster B.
- **Average**. Takes the *average dissimilarity* between all the pairwise dissimilarities
between the observations in cluster A and cluster B.

Hierarchical clustering is quite sensible to the kind of dissimilarity employed and the kind of *linkage* used. In addition, the hierarchical property might force the clusters to unnatural behaviors. Particularly, *single* linkage may result in extended, chained clusters in which a single observation is added at a new level. As a consequence, *complete* and *average* are usually recommended in practice.

Let's illustrate how to perform hierarchical clustering in `laligaStd`.
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.pos = 'h!', fig.asp = 1, warning = FALSE, cache = TRUE}
# Compute dissimilarity matrix - in this case Euclidean distance
d <- dist(laligaStd)

# Hierarchical clustering with complete linkage
treeComp <- hclust(d, method = "complete")
plot(treeComp)

# With average linkage
treeAve <- hclust(d, method = "average")
plot(treeAve)

# With single linkage
treeSingle <- hclust(d, method = "single")
plot(treeSingle) # Chaining

# Set the number of clusters after inspecting visually the dendrogram for "long"
# groups of hanging leaves
# These are the cluster assignments
cutree(treeComp, k = 2) # (Barcelona, Real Madrid) and (rest)
cutree(treeComp, k = 3) # (Barcelona, Real Madrid), (AtlÃ©tico Madrid) and (rest)

# Compare differences - treeComp makes more sense than treeAve
cutree(treeComp, k = 4)
cutree(treeAve, k = 4)

# We can plot the results in the first two PCs, as we did in k-means
cluster <- cutree(treeComp, k = 2)
plot(pca$scores[, 1:2], col = cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = cluster)

cluster <- cutree(treeComp, k = 3)
plot(pca$scores[, 1:2], col = cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = cluster)

cluster <- cutree(treeComp, k = 4)
plot(pca$scores[, 1:2], col = cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = cluster)
```
If categorical variables are present, replace `dist` by `daisy` from the `cluster` package (you need to do first `library(cluster)`). For example, let's cluster the `iris` dataset.
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.pos = 'h!', fig.asp = 1, warning = FALSE, cache = TRUE}
# Load data
data(iris)

# The fifth variable is a factor
head(iris)

# Compute dissimilarity matrix using the Gower dissimilarity measure
# This dissimilarity is able to handle both numerical and categorical variables
# daisy automatically detects whether there are factors present in the data and
# applies Gower (otherwise it applies the Euclidean distance)
library(cluster)
d <- daisy(iris)
tree <- hclust(d)

# 3 main clusters
plot(tree)

# The clusters correspond to the Species
cutree(tree, k = 3)
table(iris$Species, cutree(tree, k = 3))
```

```{block, type = 'rmdcaution'}
Performing hierarchical clustering in practice depends on several decisions that may have big consequences on the final output:

  - What kind of dissimilarity and linkage should be employed? Not a single answer: try several and compare.
  - Where to cut the dendrogram? The general advice is to look for groups of branches hanging for a long space and cut on their top.

Despite the general advice, there is not a single and best solution for the previous questions. What is advisable in practice is to analyze several choices, report the general patterns that arise and the different features of the data the methods expose.
```

Hierarchical clustering can also be performed through the help of `R Commander`. To do so, go to `'Statistics' -> 'Dimensional Analysis' -> 'Clustering' -> 'Hierar...'`. If you do this for the `USArrests` dataset after rescaling, you should get something like this:
```{r, collapse = TRUE, cache = TRUE}
HClust.1 <- hclust(dist(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop,
                                     USArrests)), method = "complete")
plot(HClust.1, main = "Cluster Dendrogram for Solution HClust.1",
     xlab = "Observation Number in Data Set USArrests",
     sub = "Method=complete; Distance=euclidian")
```

```{block, type = 'rmdexercise'}
Import the `eurojob` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/eurojob.txt)) dataset and standardize it properly. Perform a hierarchical clustering analysis for the three kind of linkages seen.
```
