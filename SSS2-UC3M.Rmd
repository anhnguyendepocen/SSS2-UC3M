---
title: "Lab notes for Statistics for Social Sciences II: Multivariate Techniques"
subtitle: "BSc in International Studies and BSc in International Studies	&	Political	Science, Carlos III University of Madrid"
author: "Eduardo García Portugués"
date: "`r Sys.Date()`, v12.2"
knit: "bookdown::render_book"
documentclass: book
bibliography: SSS2-UC3M.bib
biblio-style: apalike
link-citations: yes
site: bookdown::bookdown_site
---

# Introduction

<!--
(Move up once decided what options)
cover-image: no
description: "Lab notes for Statistics for Social Sciences II: Multivariate Techniques"
github-repo: egarpor/SSS2-UC3M
apple-touch-icon: "touch-icon.png"
apple-touch-icon-size: 120
favicon: "favicon.ico"
-->

Welcome to the lab notes for *Statistics for Social Sciences II: Multivariate Techniques*. Along these notes we will see how to effectively implement the statistical methods presented in the lectures. The exposition we will follow is based on learning by analyzing datasets and **real-case studies**, always with the help of **statistical software**. While doing so, we will illustrate the key insights of some multivariate techniques and the adequate use of advanced statistical software.

Be advised that these notes are *neither an exhaustive, rigorous nor comprehensive treatment* of the broad statistical branch know as *Multivariate Analysis*. They are just a helpful resource to implement the specific topics covered in this limited course.

## Some course logistics

- Lessons.
    - International Studies, group 55. **Mondays 16:15-17:45 at INF-15.S.06**.
    - International Studies, group 56. **Tuesdays 18:00-19:30 at INF-10.0.29**.
    - International Studies & Political Science, group 45. **Mondays 18:00-19:30 at INF-15.S.04**.
- Office hours. **Tuesdays 11:00-13:00 at office 10.0.10** (access through 10.0.7 in *Campomanes* building). If they are incompatible with your schedule, send me an email to get an alternative appointment (preferable) or just drop by my office to see if I am available (I will remove this option if I get overwhelmed with queries).
- Grading.
    - Continuous evaluation is 60% of the final grade. Scored by 2 partial exams and 1 group project, each accounting for a 20%.
    - Final exam is 40%, covers contents mostly from the theoretical lectures.
- Partials. Cover contents from labs and lectures. The first will (likely) cover Topics 1-2. The second, Topics 3-5.
- Group project. The students must team up in groups of 4 ($\pm1$) in order to produce a report. This report must apply the methodology presented to a dataset of their choice. As a rule of thumb, all students in a group will be graded evenly and according to the ratio "quality project"/"group size". Specific details in Appendix \@ref(group-project).

## Software employed

The software we will employ in this course is available in all UC3M computer labs, including INF-15.S.04, INF-15.S.06 and INF-10.0.29 ~~(the first week only in INF-15.S.04). We will use two pieces of software:

- [`R`](https://www.r-project.org/). A free open-source software environment for statistical computing and graphics. Virtually all the statistical methods you can think of are available in `R`. Currently, it is *the* dominant statistical software (at least among statisticians).

- [`R Commander`](http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/). A Graphical User Interface (GUI) designed to make `R` accessible to non-specialists through friendly menus. Essentially, it translates simple menu instructions into `R` code.

The only thing you need to do to run `R Commander` in any UC3M computer is:

1. Run `'Start' -> 'R3.3.1' -> 'R3.3.1 (consola)'`.  A black console will open. Do not panic!
2. Type inside

    ```{r, eval = FALSE}
    library(Rcmdr)
    ```

    Congratulations on your first piece of `R` code, you have just loaded a package!

3. If `Rcmdr` is installed, then `R Commander` will open automatically and you are ready to go. In case you accidentally close `R Commander`, type

    ```{r, eval = FALSE}
    Commander()
    ```

    If `Rcmdr` is not installed, then type

    ```{r, eval = FALSE}
    install.packages("Rcmdr", dep = TRUE)
    ```

    and say `'Yes'` to the next pop-ups regarding the installation of the personal library. This will download and install `Rcmdr` and all the related packages from a CRAN mirror (`'Spain (A Coruña) [https]'`, usually works fine -- try a different one if you experience problems). Wait for the downloading and installation of the packages. When it is done, just type
    ```{r, eval = FALSE}
    library(Rcmdr)
    ```
    and you are ready to go.

An important warning about UC3M computer labs:

```{block, type = 'rmdcaution'}
Every file you save locally (including installed packages) will be wiped out after you close your session. So be sure to save your valuable files at the end of the lesson.

The **exception** is the folder `'C:/TEMP'`, where all the files you save will be accessible for *everyone* that logs in the computer!
```

```{block2, type = 'rmdcaution'}
In UC3M computers, `R` and `R Commander` are only available in Spanish. To have them in English, you need to do a workaround:

1. Create a shortcut to `R` in your desktop. To do so, go to `'C:/Archivos de programa/R/R-3.3.1/bin/', right-click in `'R.exe'` and choose `'Enviar a' -> 'Escritorio (crear acceso directo)'`.
2. Modify the properties of the shortcut. Right-click on the shortcut and choose `'Propiedades'`. Then **append** to the `'Destino'` field the text `Language=en` (separated by a space, see Figure \@ref(fig:lang)). Click `'Aplicar'` and then `'OK'`.
3. Run that shortcut and then type `library(Rcmdr)`.

Tip: if you save the modified shortcut in `'C:/TEMP'`, it could be available the next time you log in.
```

Alternatively, you can bring your own laptop and save all your files in it, see Section \@ref(laptop).

## Why this software?

There are many advanced commercial statistical software, such as `SPSS`, `Excel` (with commercial add-ons), `Minitab`, `Stata`, `SAS`, etc. We will rely on the combo `R` [@R-base] + `R Commander` [@R-Rcmdr] due to some noteworthy advantages:

1. **Free and open-source**. (Free as in beer, free as in speech.) No software licenses are needed. This means that you can readily use it outside UC3M computer labs, without limitations on the period or purpose of use.

2. **Scalable complexity and extensibility**. `R Commander` creates `R` code that you can see, and eventually understand. Once you begin to get a feeling of it, you will realize that is faster to type the right commands than to navigate through menus. In addition, `R Commander` has 39 high-quality plug-ins (September, 2016), so the procedures available through menus will not fall short easily.

3. **`R` is the leading computer language in statistics**. Any statistical analysis that you can imagine is already available in `R` through its almost 9000 free packages (September, 2016). Some of them contain a good number of ready-to-use datasets or methods for data acquisition from accredited sources.

4. `R Commander` produces **high-quality graphs easily**. `R Commander`, through the plug-in `KMggplot2`, interfaces the `ggplot2` library, which delivers high-quality, publication-level graphs ([sample gallery](http://www.r-graph-gallery.com/portfolio/ggplot2-package/)). It is considered as one of the best and more elegant graphing packages nowadays.

5. **Great report generation**. `R Commander` integrates `R Markdown`, which is a framework able to create `.html`, `.pdf` and `.docx` reports directly from the outputs of `R`. That means you can deliver high-quality, reproducible and beautiful reports with a little amount of effort. For example, these notes have been created with an extension of `R Markdown`.

In summary, `R Commander` eases the learning curve of `R` and provides a powerful way of creating and reporting statistical analyses. An intermediate knowledge in `R Commander` + `R` will improve notably your quantitative skills, therefore making an **important distinction in your graduate profile** (it is a fact that many social scientists tend to lack a proper quantitative formation). So I encourage you to take full advantage of this great opportunity!

## Installation in your own computer {#laptop}

You are allowed to bring your own laptop to the labs. This may have a series of benefits, such as admin privileges, saving all your files locally and a deeper familiarization with the software. But keep in mind:

```{block, type = 'rmdcaution'}
If you plan to use your personal laptop, **you** are responsible for the right setup of the software (and laptop) *prior* to the lab lesson.
```

Regardless of your choice, at some point you will probably need to run the software outside UC3M computer labs. This is what you have to do in order to install `R` + `R Commander` in your own computer:

1. In Mac OS X, download and install first [`XQuartz`](https://www.xquartz.org/) and log out and back on your Mac OS X account (this is an **important** step). Be sure that your Mac OS X system is up-to-date.
2. Download the latest version of `R` for [Windows](https://cran.r-project.org/bin/windows/base/release.html) or [Mac OS X](https://cran.r-project.org/bin/macosx/R-3.3.2.pkg).
3. Install `R`. In Windows, be sure to select the `'Startup options'` and then choose `'SDI'` in the `'Display Mode'` options. Leave the rest of installation options as default.
4. Open `R` (`'R x64 X.X.X'` in 64-bit Windows, `'R i386 X.X.X'` in 32-bit Windows and `'R.app'` in Mac OS X) and type:

    ```{r, eval = FALSE}
    install.packages(c("Rcmdr", "RcmdrMisc", "RcmdrPlugin.TeachingDemos",
                       "RcmdrPlugin.FactoMineR", "RcmdrPlugin.KMggplot2"),
                     dep = TRUE)
    ```

    Say `'Yes'` to the pop-ups regarding the installation of the personal library and choose the CRAN mirror (the server from which you will download packages). `'Spain (A Coruña) [https]'`, usually works fine -- try a different one if you experience problems.

5. To launch the `R Commander`, run `R` and then

    ```{r, eval = FALSE}
    library(Rcmdr)
    ```

```{block, type = 'rmdcaution'}
**Mac OS X users**. To prevent an occasional freezing of `R` and `R Commander` by the OS, go to `'Tools' -> 'Manage Mac OS X app nap for R.app...'` and select `'off (recommended)'`.
```

If there is any Linux user, kindly follow the corresponding instructions [here](https://cran.r-project.org/) and [here](http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html).

By default `R` and `R Commander` will have menus and messages in the language of your OS. If you want them in English, a simple option is to change the OS language to English and reboot. If you want to stick with your OS language, another options are:

- **Windows**. Create a shortcut in your desktop, either to `'R x64 X.X.X'` or to `'R i386 X.X.X'`. Add a distinctive descriptor to its name, for example `'R x64 X.X.X ENGLISH'`. Then `'Right mouse click'` on it, select `'Properties'` and **append** to the `'Target'` field the text `Language=en` (separated by a space, see Figure \@ref(fig:lang)). Analogously, use `Language=es` for Spanish, `Language=it` for Italian, etc. Use this shortcut to launch `R` (and then `R Commander`) in the chosen language. Click `'Apply'` and then `'OK'`.

    ```{r, lang, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Modification of the `R` shortcut properties in Windows.', cache = TRUE}
  knitr::include_graphics("images/screenshots/lang.png")
    ```

- **Mac OS X**. Open `R.app` and simply run

    ```{r, eval = FALSE}
    system("defaults write org.R-project.R force.LANG en_GB.UTF-8")
    ```

  Then close `'R.app'` and relaunch it. Analogously, replace `en_GB` above by `es_ES` or `it_IT` if you want to switch back to Spanish or Italian, for example.

## `R Commander` basics

```{r, ScreenshotRcmdr, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Main window of `R Commander`, with the plug-ins `RcmdrPlugin.FactoMineR`, `RcmdrPlugin.KMggplot2` and `RcmdrPlugin.Demos` loaded.', cache = TRUE}
knitr::include_graphics("images/screenshots/Rcmdr2.png")
```

When you start `R Commander` you will see a window similar to Figure \@ref(fig:ScreenshotRcmdr). This GUI has the following items:

1. **Drop-down menus**. They are pretty self-explanatory. A quick summary:
    - `'File'`. Saving options for different files (`.R`, `.Rmd`, `.txt` output and `.RData`). The latter corresponds to saving the *workspace* of the session.
    - `'Edit'`. Basic edition within the GUI text boxes (`'Copy'`, `'Paste'`, `'Undo'`, ...).
    - `'Data'`. Import, manage and manipulate datasets.
    - `'Statistics'`. Perform statistical analyses, such as `'Summaries'` and `'Fit models'`.
    - `'Graphs'`. Compute the available graphs (depending on the kind of variables) for the active dataset.
    - `'Models'`. Graphical, numerical and inferential analyses for the active model.
    - `'Distributions'`. Operations for continuous/discrete distributions: sampling, computation of probabilities and quantiles, and plotting of density and distribution functions.
    - `'Tools'`. Options for `R Commander`. Here you can `'Load Rcmdr plug-in(s)...'`, which enables to expand the number of menus available via  plug-ins<!--^[For example, `'FactoMineR'` (collection of multivariate techniques), `'KMggplot2'` (produces elegant graphs using `ggplot2`) and `'Demos'` (set of pedagogical demos; `'Simple linear regression'` is specially relevant for the next chapter).]-->. `R Commander` will need to restart prior to loading a plug-in. (A minor inconvenience is that the text boxes in the GUI will be wiped out after restarting. The workspace is kept after restarting, so the models and datasets will be available -- but not selected as active.)
    - `'Help'`. Several help resources.
2. **Dataset manipulation**. Select the active dataset among the list of loaded datasets. Edit (very basic) and view the active dataset.
3. **Model selector**. Select an active model among the available ones to work with.
4. **Switch to script or report mode**. Switch between the generated `R` code and the associated `R Markdown` code.
5. **Input panel**. The `R` code generated by the drop-down menus appears here and is passed to the output console. You can type code here and run it without using the drop-down menus.
6. **Submit/Generate report buttom**. Allows to pass and run *selected* `R` code to the output console (keyboard shortcut: `'Control' + 'R'`).
7. **Output console**. Shows the commands that were run (red) and their visible result (blue), if any.
8. **Messages**. Displays possible error messages, warnings and notes.

When you close `R Commander` you will be asked on what files to save: `'script file'` and `'R Markdown file'` (contents in the two tabs of panel 5, respectively), and `'output file'` (panel 7). If you want to save the workspace, you have to do it through the `'File'` menu.

```{block, type = 'rmdtip'}
Focus on **understanding the purpose of each element in the GUI**. When performing the real-case studies we will take care of explaining the many features of `R Commander` step by step.
```

## Datasets for the course

This is a handy list with a small description and download link for all the relevant datasets used in the course. To download them, simply **save the link as a file** in your browser.

- `pisa.csv` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/pisa.csv)). Contains 65 rows corresponding to the countries that took part on the PISA study. Each row has the variables `Country`, `MeanMath`,` MathShareLow`, `MathShareTop`, `ReadingMean`, `ScienceMean`, `GDPp`, `logGDPp` and `HighIncome`. The `logGDPp` is the logarithm of the `GDPp`, which is taken in order to avoid scale distortions.

- `US_apportionment.xls` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/US_apportionment.xls)). Contains the 50 US states entitled to representation in the US House of Representatives. The recorded variables are `State`, `Population2010` and `Seats2013–2023`.

- `EU_apportionment.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/EU_apportionment.txt)). Contains 28 rows with the member states for the EU (`Country`), the number of seats assigned under different years (`Seats2011`, `Seats2014`), the Cambridge Compromise apportionment (`CamCom2011`), and the states population (`Population2010`, `Population2013`).

- `least-squares.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/least-squares.RData)). Contains a single `data.frame`, named `leastSquares`, with 50 observations of the variables `x`, `yLin`, `yQua` and `yExp`. These are generated as $X\sim\mathcal{N}(0,1)$, $Y_\mathrm{lin}=-0.5+1.5X+\varepsilon$, $Y_\mathrm{qua}=-0.5+1.5X^2+\varepsilon$ and $Y_\mathrm{exp}=-0.5+1.5\cdot2^X+\varepsilon$, with $\varepsilon\sim\mathcal{N}(0,0.5^2)$. The purpose of the dataset is to illustrate the least squares fitting.

- `assumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/assumptions.RData)). Contains the data frame `assumptions` with 200 observations of the variables `x1`, ..., `x9` and `y1`, ..., `y9`. The purpose of the dataset is to identify which regression `y1 ~ x1`, ..., `y9 ~ x9` fulfills the assumptions of the linear model. The dataset `moreAssumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/moreAssumptions.RData)) has the same structure.

- `cpus.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/cpus.txt)) and `gpus.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/gpus.txt)). The datasets contain 102 and 35 rows, respectively, of commercial CPUs and GPUs appeared since the first models up to nowadays. The variables in the datasets are `Processor`, `Transistor count`, `Date of introduction`, `Manufacturer`, `Process` and `Area`.

- `hap.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/hap.txt)). Contains data for 20 advanced economies in the time period 1946–2009, measured for 31 variables. Among those, the variable `dRGDP` represents the real GDP growth (as a percentage) and `debtgdp` represents the percentage of public debt with respect to the GDP.

- `wine.csv` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/wine.csv)). The dataset is formed by the auction `Price` of 27 red Bordeaux vintages, four vintage descriptors (`WinterRain`, `AGST`, `HarvestRain`, `Age`) and the population of France in the year of the vintage, `FrancePop`.

- `Boston.xlsx` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/Boston.xlsx)). The dataset contains 14 variables describing 506 suburbs in Boston. Among those variables, `medv` is the median house value, `rm` is the average number of rooms per house and `crim` is the per capita crime rate. The full description is available in `?Boston`.

- `assumptions3D.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/assumptions3D.RData)). Contains the data frame `assumptions3D` with 200 observations of the variables `x1.1`, ..., `x1.8`, `x2.1`, ..., `x2.8` and `y.1`, ..., `y.8`. The purpose of the dataset is to identify which regression `y.1 ~ x1.1 + x2.1`, ..., `y.8 ~ x1.8 + x2.8` fulfills the assumptions of the linear model.

- `challenger.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/challenger.txt)). Contains data for 23 Space-Shuttle launches. The data consists of 23 shuttle flights. There are 8 variables. Among them: `temp`, the temperature in Celsius degrees at the time of launch, and `fail.field`	and `fail.nozzle`, indicators of whether there were an incidents in the O-rings of the field joints and nozzles of the solid rocket boosters.

- `eurojob.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/eurojob.txt)). Contains data for employment in 26 European countries. There are 9 variables, giving the percentage of employments in 9 sectors: `Agr` (Agriculture), `Min` (Mining), `Man` (Manufacture), `Pow` (Power), `Con` (Construction), `Ser` (Sevices), `Fin` (Finance), `Soc` (Social) and `Tra` (Transport).

- `Chile.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/Chile.txt)). Contains data for 2700 respondents on a survey for the voting intentions in the 1988 Chilean national plebiscite. There are 8 variables: `region`, `population`, `sex`, `age`, `education`, `income`, `statusquo` (scale of support for the status quo) and `vote`. `vote` is a factor with levels `A` (abstention), `N` (against Pinochet), `U` (undecided), `Y` (for Pinochet). Available in `R` through the package `car` and `data(Chile)`.

- `USArrests.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/USArrests.txt)). Arrest statistics for `Assault`, `Murder` and `Rape` in each of the 50 US states in 1973. The percent of the population living in urban areas, `UrbanPop`, is also given. Available in `R` through `data(USArrests)`.

- `USJudgeRatings.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/USJudgeRatings.txt)). Lawyers' ratings of state judges in the US Superior Court. The dataset contains 43 observations of 12 variables measuring the performance of the judge when conducting a trial. Available in `R` through `data(USJudgeRatings)`.

- `la-liga-2015-2016.xlsx` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/la-liga-2015-2016.xlsx)). Contains 19 performance metrics for the 20 football teams in La Liga 2015/2016.

- `pisaUS2009.csv` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/pisaUS2009.csv)). Reading score of 3663 US students in the PISA test, with 23 variables informing about the student profile and family background.

## Main references and credits

The following great reference books have been used extensively for preparing these notes:

- @James2013 (linear regression, logistic regression, PCA, clustering),
- @Pena2002 (linear regression, logistic regression, PCA, clustering),
- @Bartholomew2008 (PCA).

The icons used in the notes were designed by [madebyoliver](http://www.flaticon.com/authors/madebyoliver), [freepik](http://www.flaticon.com/authors/freepik) and [roundicons](http://www.flaticon.com/authors/roundicons) from [Flaticon](http://www.flaticon.com/).

All material in these notes is licensed under [CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/).

<!--chapter:end:index.Rmd-->


# Simple linear regression {#simp}

The simple linear regression is a *simple* but useful statistical model. In short, it allows to analyse the (assumed) linear relation between two variables, $X$ and $Y$ in a proper way. It does it by considering the model
\[
Y=\beta_0+\beta_1X+\varepsilon
\]
which in Chapter \@ref(mult) will be extended to multiple linear regression.

To convince you why simple linear regression is useful, let's begin by seeing what it can do in real-case scenarios!

## Examples and applications

### Case study I: PISA scores and GDPp {#pisa}

The Programme for International Student Assessment (PISA) is a study carried out by the Organisation for Economic Co-operation and Development (OECD) in 65 countries with the purpose of evaluating the performance of 15-year-old pupils on mathematics, science, and reading. A phenomena observed over years is that *wealthy countries tend to achieve larger average scores*. The purpose of this case study, motivated by the @PISA inform, is to answer two questions related with the previous statement:

- Q1. *Is the educational level of a country influenced by its economic wealth?*
- Q2. *If so, up to what precise extent?*

The `pisa.csv` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/pisa.csv)) contains 65 rows corresponding to the countries that took part on the PISA study. The data was obtained merging the [statlink](http://dx.doi.org/10.1787/888932937035) in @PISA2012 with @WB data. Each row has the following variables: `Country`; `MathMean`, `ReadingMean` and `ScienceMean` (the average performance of the students in mathematics, reading and science); `MathShareLow` and `MathShareTop` (percentages of students with a low and top performance in mathematics); `GDPp` and `logGDPp` (the Gross Domestic Product per capita and its logarithm); `HighIncome` (whether the country has a GDPp larger than 20000\$ or not). The GDPp of a country is a measure of how many economic resources are available per citizen. The `logGDPp` is the logarithm of the GDPp, taken in order to avoid scale distortions. A small subset of the data is shown in Table \@ref(tab:pisatable).

```{r, pisatable, echo = FALSE, out.width = '90%', fig.align = 'center', cache = TRUE}
pisa <- read.csv(file = "datasets/pisa.csv", header = TRUE)
knitr::kable(
  head(pisa[, c(1, 2, 5, 6, 8, 9)], 10),
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'First 10 rows of the `pisa` dataset for a selection of variables. Note the `NA` (*Not Available*) in Chinese Taipei (or Taiwan).'
)
```

We definitely need a way of **summarizing this ammount of information**!

We are going to do the following. First, import the data into `R Commander` and do a basic manipulation of it. Second, fit a linear model and interpret its output. Finally, visualize the fitted line and the data.

1. **Import the data into `R Commander`**.

    * Go to `'Data' -> 'Import data' -> 'from text file, clipboard, or URL...'`. A window like Figure \@ref(fig:read) will pop-up. Select the appropiate formatting options of the data file: whether the first row contains the name of the variables, what is the indicator for missing data, what is the field separator, and what is the decimal point character. Then click `'OK'`.

        ```{block, type = 'rmdtip'}
        Inspecting the data file in a text editor will give you the right formatting choices for importing the data.
        ```

        ```{r, read, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Data importation options.', cache = TRUE}
        row.names(pisa) <- as.character(pisa$Country)
        pisa$Country <- NULL
        knitr::include_graphics("images/screenshots/read.png")
        ```

    * Click on `'View data set'` to check that the importation was fine. If the data looks weird, then recheck the structure of the data file and restart from the above point.

    * Since each row corresponds to a different country, we are going to name the rows as the value of the variable `Country`. To that end, go to `'Data' -> 'Active data set' -> 'Set case names...'` and select the variable `Country` and click `'OK'`. The dataset should look like Figure \@ref(fig:view).

        ```{r, view, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Correct importation of the `pisa` dataset.', cache = TRUE}
        knitr::include_graphics("images/screenshots/view.png")
        ```

        ```{block, type = 'rmdcaution'}
        In UC3M computers, **altering the location of a downloaded file may cause errors** in its importation to `R Commander`!

        Example:

        - Default download path: `'C:/Users/g15s4021/Downloads/pisa.csv'`. Importation from that path works fine.
        - If you **move the file another location** (e.g. to `'C:/Users/g15s4021/Desktop/pisa.csv'`). Importation generates an **error**.
        ```

2. **Fit a simple linear regression**.

    * Go to `'Statistics' -> 'Fit models' -> 'Linear regression...'`. A window like Figure \@ref(fig:lm) will pop-up.

        ```{r, lm, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Window for performing simple linear regression.', cache = TRUE}
        knitr::include_graphics("images/screenshots/lm.png")
        ```

        Select the *response variable*. This is the variable denoted by $Y$ that we want to predict/explain. Then select the *explanatory variable* (also known as the *predictor*). It is denoted by $X$ and is the variable used to predict/explain $Y$. Recall the form of the linear model:
        \begin{align*}
        Y=\beta_0+\beta_1X+\varepsilon
        \end{align*}

        In our case $Y=$`MathMean` and  $X=$`logGDPp`, so select them and click `'OK'`^[In principle, you could pick more than one explanatory variables using the `'Control'` or `'Shift'` keys, but that corresponds to the *multiple linear regression* (covered in Chapter \@ref(mult)).].

        ```{block, type = 'rmdtip'}
        If you want to deselect an option in an `R Commander` menu, use `'Control' + 'Mouse click'`.
        ```

        ```{block, type = 'rmdtip'}
        Four buttons are common in the menus of `R Commander`:

        - `'OK'`: executes the selected action, then closes the window.
        - `'Apply'`: executes the selected action but leaves the window open. Useful if you are experimenting with different options.
        - `'Reset'`: resets the fields and boxes of the window to their defaults.
        - `'Cancel'`: exits the window without performing any action.
        ```

    * The window in Figure \@ref(fig:lm) generates this code and output:

        ```{r, cache = TRUE}
        pisaLinearModel <- lm(MathMean ~ logGDPp, data = pisa)
        summary(pisaLinearModel)
        ```

        This is the linear model of `MathMean` regressed on `logGDPp` (first line) and its summary (second line). The summary gives the coefficients of the line and the $R^2$ (`'Multiple R-squared'`), which -- as we will see in Section \@ref(fit) -- it can be regarded as an *indicator of the strength of the linear relation between the variables*. ($R^2=1$ is a perfect linear fit -- all the points lay in a line -- and $R^2=0$ is the poorest fit.)

        The fitted regression line is `MathMean` $= 185.16 + 28.79\,\times$ `logGDPp`. The slope coefficient is positive, which indicates that there is a positive correlation between the wealth of a country and its performance in the PISA Mathematics test (this answers Q1). Hence, the evidence that *wealthy countries tend to achieve larger average scores* is indeed true (at least for the Mathematics test). We can be more precise on the effect of the wealth of a country. According to the fitted linear model, an increase of 1 unit in the `logGDPp` of a country is associated with achieving, on average, 28.79 additional points in the test (Q2).

3. **Visualize the fitted regression line**.

    * Go to `'Graphs' -> 'Scatterplot...'`. A window with two panels will pop-up (Figures \@ref(fig:scatter1) and \@ref(fig:scatter2)).

        ```{r, scatter1, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot window, `\'Data\'` panel.', cache = TRUE}
        knitr::include_graphics("images/screenshots/scatterplot1.png")
        ```
        ```{r, scatter2, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot window, `\'Options\'` panel. Remember to tick the `\'Least-squares line\'` box in order to display the fitted regression line.', cache = TRUE}
        knitr::include_graphics("images/screenshots/scatterplot2.png")
        ```

        On the `'Data'` panel, select the $X$ and $Y$ variables to be displayed in the scatterplot. On the `'Options'` panel, check the `'Least-squares line'` box and choose to identify `'3'` points `'Automatically'`^[The decision of which points are the most *different* from the rest is done automatically by a method known as the *Mahalanobis depth*.]. This will identify what are the three^[The default GUI option is set to identify `'2'` points. However, we know after a preliminary plot that there are three very different points in the dataset, hence this particular choice.] most different observations of the data.

    * The following `R` code will be generated. It produces a scatterplot of `MathMean` vs `logGDPp`, with its scorresponding regression line.
        ```{r, echo = FALSE, warning = FALSE, cache = TRUE}
        library(splines)
        library(car)
        library(sandwich)
        ```
        ```{r, out.width = '90%', fig.asp = 1, fig.align = 'center', cache = TRUE}
        scatterplot(MathMean ~ logGDPp, reg.line = lm, smooth = FALSE, spread = FALSE,
                    id.method = 'mahal', id.n = 3, boxplots = FALSE, span = 0.5,
                    ellipse = FALSE, levels = c(.5, .9),
                    main = "Average Math score vs. logGDPp", pch = c(16), data = pisa)
        ```

        There are three clear *outliers*^[The outliers have a considerable impact on the regression line, as we will see later.]: Vietnam, Shanghai-China and Qatar. The first two are non high-income economies that perform exceptionally well in the test (although Shanghai-China is a cherry-picked region of China). On the other hand, Qatar is a high-income economy that has really poor scores.

        We can identify countries that are above and below the linear trend in the plot. This is particularly interesting: we can assess whether a country is performing better or worse with respect to its *expected PISA score according to its economic status* (this adds more insight into Q2). To do so, we want to display the text labels in the points of the scatterplot. We can take a shortcut: copy and run in the input panel the next piece of code. It is a slightly modified version of the previous code (what are the differences?).

        ```{r, out.width = '90%', fig.asp = 1, fig.align = 'center', results = 'hide', cache = TRUE}
        scatterplot(MathMean ~ logGDPp, reg.line = lm, smooth = FALSE, spread = FALSE,
                    id.method = 'mahal', id.n = 65, id.cex = 0.75, boxplots = FALSE,
                    span = 0.5, ellipse = FALSE, levels = c(.5, .9),
                    main = "Average Math score vs. logGDPp", pch = c(16), cex = 0.75,
                    data = pisa)
        ```

If you understood the previous analysis, then you should be able to perform the next ones on your own.

```{block, type = 'rmdexercise'}
Repeat the regression analysis (steps 2--3) for:

- `ReadingMean` regressed on `logGDPp`. Are the results similar to `MathMean` on `logGDPp`?
- `MathMean` regressed on `ReadingMean`. Compare it with `MathMean` on `ScienceMean`. Which pair of variables has the highest linear relation? Is that something expected?

Save the new models with different names to avoid overwriting the previous models!
```

### Case study II: Apportionment in the EU and US {#euus}

> Apportionment is the process by which seats in a legislative body are distributed among administrative divisions entitled to representation.
>
> --- Wikipedia article on [Apportionment (politics)](https://en.wikipedia.org/wiki/Apportionment_(politics))

The *European Parliament* and the *US House of Representatives* are two of the most important macro legislative bodies in the world. The distribution of seats in both cameras is designed to represent the different states that conform the federation (US) or union (EU). Both chambers were created under very different historical and political circumstances, which is reflected in the kinds of apportionment that they present. More specifically:

- In the **US**, the apportionment is neatly *fixed by the US Constitution*. Each of the 50 states is apportioned a number of seats that corresponds to its share of the total population of the 50 states, according to the most recent decennial census. Every state is guaranteed *at least 1 seat*. There are 435 seats.

- Until now, the apportionment in the **EU** was set by *treaties* (Nice, Lisbon), in which *negotiations* between countries took place. The [last accepted composition](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32013D0312) gives an allocation of seats based on the principle of "degressive proportionality"^[Less populated states are given more weight than its corresponding proportional share.] and somehow vague guidelines. It concludes with a commitment to establish a system to "allocate the seats between Member States in an objective, fair, durable and transparent way, translating the principle of degressive proportionality". The Cambridge Compromise [@CamCom] was a proposal in that direction that was not effectively implemented. Currently, every state is guaranteed a *minimum of 6 seats* and a *maximum of 96* for a grand total of 750 seats.

We know that there exist qualitative dissimilarities between both chambers, but we can not be more specific with the description at hand. The purpose of this case study is to quantify and visualize what are the differences between the apportionments of the two chambers, and how the simple linear regression can add insights on what is actually going on with the EU apportionment. The questions we want to answer are:

- Q1. *Can we quantify which chamber is more proportional?*
- Q2. *What are the over-represented and under-represented states in both chambers?*
- Q3. *How can we quantify the 'degressive proportionality' in the EU approportionment system? Was the Cambridge Compromise proposing a fairer representation?*

Let's begin by reading the data:

1. The `US_apportionment.xlsx` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/US_apportionment.xlsx)) contains the 50 US states entitled to representation. The variables are `State`, `Population2010` (from the last census) and `Seats2013–2023`. This is an Excel file that we can read using `'Data' -> 'Import data' -> 'from Excel file...'`. A window will pop-up, asking for the right options. We set them as in Figure \@ref(fig:excel), since we want the variable `State` to be the case names. After clicking in `'View dataset'`, the data should look like Figure \@ref(fig:US).

    ```{r, excel, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Importation of an Excel file.', cache = TRUE}
    knitr::include_graphics("images/screenshots/excel.png")
    ```

    ```{r, US, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Correct importation of the `US` dataset.', cache = TRUE}
    knitr::include_graphics("images/screenshots/US.png")
    ```

2. The `EU_apportionment.txt` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/EU_apportionment.txt)) contains 28 rows with the member states of the EU (`Country`), the number of seats assigned under different years (`Seats2011`, `Seats2014`), the Cambridge Compromise apportionment (`CamCom2011`), and the countries population^[According to [EuroStat](http://ec.europa.eu/eurostat/tgm/table.do?tab=table&init=1&language=en&pcode=tps00001) and the population stated in the Cambridge Compromise report.] (`Population2010`,`Population2013`).

    ```{block, type = 'rmdexercise'}
    For this file, you should know how to:

    1. Inspect the file in a text editor and determine its formatting.
    2. Decide the right importation options and load it with the name `EU`.
    3. Set the case names as the variable `Country`.
    ```

    ```{r, EU, echo = FALSE, cache = TRUE}
    US <- RcmdrMisc::readXL("datasets/US_apportionment.xlsx", rownames = TRUE, header = TRUE, na = "", sheet = "Hoja1", stringsAsFactors = TRUE)
    EU <- read.table("datasets/EU_apportionment.txt", header = TRUE, na = "-", sep = "\t")
    row.names(EU) <- EU$Country
    EU$Country <- NULL
    knitr::kable(
      EU,
      booktabs = TRUE,
      longtable = TRUE,
      caption = 'The `EU` dataset with `Country` set as the case names.'
    )
    ```

We start by analyzing the `US` dataset. If there is indeed a direct proportionality in the apportionment, we would expect a direct, 1:1, relation between the *ratios of seats* and the *population per state*. Let's start by constructing these variables:

1. Switch the active dataset to `US`. An alternative way to do so is by `'Data' -> 'Active data set' -> 'Select active data set...'`.
2. Go to `'Data' -> 'Manage variables in active dataset...' -> 'Compute new variable...'`.
3. Create the variable `RatioSeats2013.2023` as shown in Figure \@ref(fig:newvar). Be **careful to not overwrite** the variable `Seats2013.2023`.

    ```{r, newvar, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Creation of the new variable `RatioSeats2013.2023`. The expression to compute is `Seats2013.2023/sum(Seats2013.2023)`.', cache = TRUE}
    knitr::include_graphics("images/screenshots/newvar.png")
    ```

4. `'View dataset'` to check that the new variable is available.

```{block, type = 'rmdexercise'}
Repeat steps 1--3, conveniently adapted, to create the new variable `RatioPopulation2010`.
```
```{r, echo = FALSE, cache = TRUE}
US$RatioSeats2013.2023 <- US$Seats2013.2023 / sum(US$Seats2013.2023)
US$RatioPopulation2010 <- US$Population2010 / sum(US$Population2010)
```

Let's fit a regression line to the `US` data, with `RatioSeats2013.2023` as the response and `RatioPopulation2010` as the explanatory variable. If we name the model as `appUS`, you should get the following code and output:
```{r, cache = TRUE}
appUS <- lm(RatioSeats2013.2023 ~ RatioPopulation2010, data = US)
summary(appUS)
```
The fitted regression line is `RatioSeats2013.2023`$=0.000+1.005\,\times$`RatioPopulation2010` and has an $R^2=0.9991$ (`'Multiple R-squared'`), which means that the data is almost perfectly linearly distributed. Furthermore, the *intercept coefficient is not significant for the regression*. This is seen in the column `'Pr(>|t|)'`, which gives the $p$-values for the null hypotheses $H_0:\beta_0=0$ and $H_0:\beta_1=0$, respectively. The null hypothesis $H_0:\beta_0=0$ is *not rejected* ($p\text{-value}=0.407$; non-significant) whereas $H_0:\beta_1=0$ is rejected ($p\text{-value}=0$; significant)^[We will be able to say more about how these test are performed after Section \@ref(inference).]. Hence, we can conclude that the appropriation of seats in the US House of Representatives is indeed directly proportional to the population of each state (partially answers Q1).

If we make the scatterplot for the `US` dataset, we can see the almost perfect (up to integer rounding) 1:1 relation between the ratios "state seats"/"total seats" and "state population"/"aggregated population". We can set the scatterplot to automatically label the `'25'` most different points (select the numeric box with the mouse and type `'25'` -- the arrow buttons are limited to `'10'`) with their case names. As it is seen in Figure \@ref(fig:ratios), there is no state clearly over- or under-represented (Q2).

```{r, ratios, echo = FALSE, out.width = '90%', fig.asp = 1, results = 'hide', fig.align = 'center', fig.cap = 'The apportionment in the US House of Representatives compared with a linear fit.', cache = TRUE}
scatterplot(RatioSeats2013.2023 ~ RatioPopulation2010, reg.line = lm, smooth = FALSE,
            spread = FALSE, id.method = 'mahal', id.n = 25, id.cex = 0.75,
            boxplots = FALSE, span = 0.5, ellipse = FALSE, levels = c(.5, .9),
            main = "Approportionment in the US House of Representatives", pch = c(16),
            cex = 0.75, data = US)
```

Let's switch to the `EU` dataset, for which we will focus on the 2011 variables. A quick way of visualizing this dataset and, in general, of **visualizing multivariate data** (up to a moderate number of dimensions) is to use a *matrix scatterplot*. Essentially, it displays the scatterplots between all the pairs of variables. To do it, go to `'Graphs' -> 'Scatterplot matrix...'` and select the number of variables to be displayed. If you select them as in Figures \@ref(fig:mscatter1a) and \@ref(fig:mscatter1b), you should get an output like Figure \@ref(fig:mscatter2).

```{r, mscatter1a, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot matrix window, `\'Data\'` panel.', cache = TRUE}
knitr::include_graphics("images/screenshots/mscatter1a.png")
```
```{r, mscatter1b, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot matrix window, `\'Options\'` panel. Be sure to tick the `\'Least-squares line\'` box in order to display the fitted regression line.', cache = TRUE}
knitr::include_graphics("images/screenshots/mscatter1b.png")
```
```{r, mscatter2, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Scatterplot matrix for the variables `CamCom2011`, `Population2010` and `Seats2011` of `EU` dataset with boxplots in the central panels.', cache = TRUE}
knitr::include_graphics("images/screenshots/mscatter2.png")
```

```{block, type = 'rmdexercise'}
The scatterplot matrix has a central panel displaying one-variable summary plots: histogram, density estimate, boxplot and QQ-plot. Experiment and understand them.
```

The most interesting panels in Figure \@ref(fig:mscatter2) for our study are `CamCom2011` vs `Population2010` -- panel (1,2) -- and `Seats2011` vs `Population2010` -- panel (3,2). At first sight, it seems that the Cambridge Compromise was favoring a fairer allocation of seats than what it was actually being used in the EU parliament in 2011 (recall the step-wise patterns in (3,2)). Let's explore in depth the scatterplot `Seats2011` vs `Population2010`.

```{r, echo = FALSE, results = 'hide', out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = '`Seats2011` vs `Population2010` in the `EU` dataset.', cache = TRUE}
scatterplot(Seats2011 ~ Population2010, reg.line = lm, smooth = FALSE, spread = FALSE,
            id.method = 'mahal', id.n = 25, id.cex = 0.75, boxplots = FALSE, span = 0.5,
            ellipse = FALSE, levels = c(.5, .9),
            main = "Approportionment in the EU parliament in 2011", pch = c(16),
            cex = 0.75, data = EU)
```

There are some countries clearly detrimented and benefited by this apportionment. For example, France and Spain are under-represented and, on the other hand, Germany, Hungary and Czech Republic are over-represented (Q2).

Let's compute the regression line of `Seats2011` on `Population2010`, which we save in the model `appEU2011`.

```{r, cache = TRUE}
appEU2011 <- lm(Seats2011 ~ Population2010, data = EU)
summary(appEU2011)
```

The fitted line is `Seats2011`$=7.91+1.078\,\times10^{-6}\,\times$`Population2010`. The intercept is not zero and, indeed, the fitted intercept is significantly different from zero. Therefore, **there is no proportionality** in the apportionment. Recall that the fitted slope, despite being very small (why?), is also significantly different from zero. The $R^2$ is slightly smaller than in the `US` dataset, but definitely very high. Two conclusions stem from this analysis:

- The US House of Representatives is a proportional chamber whereas the EU parliament is definitely not, but is close to perfect linearity (completes Q1).

- The principle of **digressive proportionality**, in practice, means an almost **linear allocation** of seats with respect to population (Q3). The main point is the presence of a **non-zero intercept** -- that is, a minimum number of seats corresponding to a country -- in order to over-represent smaller countries with respect to its corresponding proportional share.

The question that remains to be answered is whether the Cambridge Compromise was favoring a fairer allocation of seats than the 2011 official agreement. In Figure \@ref(fig:mscatter2) we can see that indeed it seems like that, but there is an outlier outside the linear pattern: Germany. There is an explanation for that: the EU commission imposed a cap to the maximum number of seats per country, 96, to the development of the Cambridge Compromise. With this rule, Germany is notably under-represented.

In order to avoid this distortion, we will exclude Germany from our comparison. To do so, we specify in the `'Subset expression'` field, of either `'Linear regression...'` or `'Scatterplot...'`, a `'-1'`. This tells `R` to exclude the first row of `EU` dataset, corresponding to Germany. Then, we compare the linear models for the official allocation, `appEUNoGer2011`, and the Cambridge Compromise, `appCamComNoGer2011`. The outputs are the following.

```{r, cache = TRUE}
appEUNoGer2011 <- lm(Seats2011 ~ Population2010, data = EU, subset = -1)
summary(appEUNoGer2011)
```
```{r, cache = TRUE}
appCamComNoGer2011 <- lm(CamCom2011 ~ Population2010, data = EU, subset = -1)
summary(appCamComNoGer2011)
```

We see that the Cambridge Compromise has a larger $R^2$ and a lower intercept than the official allocation of seats. This means that it favors a more proportional allocation, which is fairer in the sense that the deviations from the linear trend are smaller (Q3). We conclude the case study by illustrating both fits.

```{r, echo = FALSE, results = 'hide', out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = '`Seats2011` vs `Population2010` in the `EU` dataset, Germany excluded.', cache = TRUE}
scatterplot(Seats2011 ~ Population2010, reg.line = lm, smooth = FALSE, spread = FALSE,
            id.method = 'mahal', id.n = 25, id.cex = 0.75, boxplots = FALSE, span = 0.5,
            ellipse = FALSE, levels = c(.5, .9),
            main = "Approportionment in the EU parliament (Germany excl.)",
            pch = c(16), cex = 0.75, data = EU, subset = -1)
```

```{r, echo = FALSE, results = 'hide', out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = '`CamCom2011` vs `Population2010` in the `EU` dataset, Germany excluded.', cache = TRUE}
scatterplot(CamCom2011 ~ Population2010, reg.line = lm, smooth = FALSE, spread = FALSE,
            id.method = 'mahal', id.n = 25, id.cex = 0.75, boxplots = FALSE, span = 0.5,
            ellipse = FALSE, levels = c(.5, .9),
            main = "Proposal for approportionment in the EU parliament (Germany excl.)",
            pch = c(16), cex = 0.75, data = EU, subset = -1)
```

```{block, type = 'rmdexercise'}
In 2014 it was negotiated a new EU apportionment, collected in `Seats2014`, according to the population of 2013, `Population2013`, and due to the inclusion of Croatia in the EU. Answer these questions:

- Which countries were the most favored and unfavored by such apportionment?
- Was the apportionment proportional?
- Was the degree of linearity higher or lower than the 2011 apportionment? (Exclude Germany.)
- Was the degree of linearity higher or lower than the Cambridge Compromise for 2011? (Exclude Germany.)

```

***

We have performed a decent number of operations in `R Commander`. If we have to exit the session, we can save the data and models in an `.RData` file, which contains all the objects we have computed so far (but **not** the code -- this has to be saved differently).

```{block, type = 'rmdtip'}
To exit `R Commander`, save all your progress and reload it later, do:

1. Save `.RData` file. Go to `'File' -> 'Save R workspace as...'`.
2. Save `.R` file. Go to `'File' -> 'Save script as...'`.
3. Exit `R Commander` + `R`. Go to `'File' -> 'Exit' -> 'From Commander and R'`. Choose to not save any file.
4. Start `R Commander` and load your files:

    - `.RData` file in `'Data' -> 'Load data set...'`,
    - `.R` file in `'File' -> 'Open script file...'`.

```

```{block, type = 'rmdtip'}
If you just want to save a dataset, you have two options:

- `'Data' -> 'Active data set' -> 'Save active data set...'`: it will be saved as an `.RData` file. The easiest way of importing it back to `R`.
- `'Data' -> 'Active data set' -> 'Export active data set...'`: it will be saved as a text file with the format that you choose. Useful for exporting data to other programs.

```

## Some `R` basics

By this time you probably had realized that some pieces of `R` code are repeated over and over, and that it is simpler to just modify them than to navigate the menus. For example, the codes `lm` and `scatterplot` always appear related with linear models and scatterplots. It is important to know some of the `R` basics in order to understand what are these pieces of text actually doing. Do not worry, the menus will always be there to generate the proper code for you -- but you need to have a *general idea* of the code.

In the following sections, **type** -- not copy and paste systematically -- the code in the `'R Script'` panel and send it to the output panel (on the selected expression, either with the `'Submit'` button or with `'Control' + 'R'`).

We begin with the `lm` function, since it is the one you are more used to. In the following, you should get the same outputs (which are preceded by `## [1]`).

### The `lm` function

We are going to employ the `EU` dataset from Section \@ref(euus), with the case names set as the `Country`. In case you do not have it loaded, you can download it [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/EU.RData) as an `.RData` file.

```{r, collapse = TRUE, error = TRUE, cache = TRUE}
# First of all, this is a comment. Its purpose is to explain what the code is doing
# Comments are preceeded by a #

# lm has the syntax: lm(formula = response ~ explanatory, data = data)
# For example (you need to load first the EU dataset)
mod <- lm(formula = Seats2011 ~ Population2010, data = EU)

# We have saved the linear model into mod, which now contains all the output of lm
# You can see it by typing
mod

# mod is indeed a list of objects whose names are
names(mod)

# We can access these elements by $
# For example
mod$coefficients

# The residuals
mod$residuals

# The fitted values
mod$fitted.values

# Summary of the model
sumMod <- summary(mod)
sumMod
```


The following table contains a handy cheat sheet of equivalences between `R` code and some of the statistical concepts associated to linear regression.

|              `R`                       |        Statistical concept          |
|:---------------------------------------|:------------------------------------|
| `x` | Predictor $X_1,\ldots,X_n$    |
| `y` | Response $Y_1,\ldots,Y_n$    |
| `data <- data.frame(x = x, y = y)` | Sample $(X_1,Y_1),\ldots,(X_n,Y_n)$    |
| `model <- lm(y ~ x, data = data)` | Fitted linear model |
| `model$coefficients` | Fitted coefficients $\hat\beta_0,\hat\beta_1$ |
| `model$residuals` | Fitted residuals $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$ |
| `model$fitted.values` | Fitted values $\hat Y_1,\ldots,\hat Y_n$ |
| `model$df.residual` | Degrees of freedom $n-2$ |
| `summaryModel <- summary(model)` | Summary of the fitted linear model |
| `summaryModel$sigma` | Fitted residual standard deviation $\hat\sigma$ |
| `summaryModel$r.squared` | Coefficient of determination $R^2$ |
| `summaryModel$fstatistic` | $F$-test |
| `anova(model)` | ANOVA table |

```{block, type = 'rmdexercise'}
Do the following:

- Compute the regression of `CamCom2011` into `Population2010`. Save that model as the variable `myModel`.
- Access the objects `residuals` and `coefficients` of `myModel`.
- Compute the summary of `myModel` and store it as the variable `summaryMyModel`.
- Access the object `sigma` of `myModel`.
- Repeat the previous steps changing the names of `myModel` and `summaryMyModel` to `otherMod` and `infoOtherMod`, respectively.
```

Now you know how to fit and summarize a linear model with a few keystrokes. Let's see more of the basics of `R` -- it will be useful for the next sections.

### Simple computations

```{r, echo = TRUE, error = TRUE, collapse = TRUE, cache = TRUE}
# These are some simple operations
# The console can act as a simple calculator
1.0 + 1.1
2 * 2
3/2
2^3
1/0
0/0

# Use ; for performing several operations in the same line
(1 + 3) * 2 - 1; 1 + 3 * 2 - 1

# Mathematical functions
sqrt(2); 2^0.5
sqrt(-1)
exp(1)
log(10); log10(10); log2(10)
sin(pi); cos(0); asin(0)
```

```{r, echo = TRUE, error = TRUE, collapse = TRUE}
# Remember to complete the expressions
1 +
(1 + 3
```

### Variables and assignment

```{r, echo = TRUE, error = TRUE, collapse = TRUE, cache = TRUE}
# Any operation that you perform in R can be stored in a variable (or object)
# with the assignment operator "<-"
a <- 1

# To see the value of a variable, we simply type it
a

# A variable can be overwritten
a <- 1 + 1

# Now the value of a is 2 and not 1, as before
a

# Careful with capitalization
A

# Different
A <- 3
a; A

# The variables are stored in your workspace (.RData file)
# A handy tip to see what variables are in the workspace
ls()
# Now you know which variables can be accessed!

# Remove variables
rm(a)
a
```

```{block, type = 'rmdexercise'}
Do the following:

- Store $-123$ in the variable `b`.
- Get the log of the square of `b`. (Answer: `9.624369`)
- Remove variable `b`.
```

### Vectors

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# These are vectors - arrays of numbers
# We combine numbers with the function c
c(1, 3)
c(1.5, 0, 5, -3.4)

# A handy way of creating sequences is the operator :
# Sequence from 1 to 5
1:5

# Storing some vectors
myData <- c(1, 2)
myData2 <- c(-4.12, 0, 1.1, 1, 3, 4)
myData
myData2

# Entry-wise operations
myData + 1
myData^2

# If you want to access a position of a vector, use [position]
myData[1]
myData2[6]

# You also can change elements
myData[1] <- 0
myData

# Think on what you want to access...
myData2[7]
myData2[0]

# If you want to access all the elements except a position, use [-position]
myData2[-1]
myData2[-2]

# Also with vectors as indexes
myData2[1:2]
myData2[myData]

# And also
myData2[-c(1, 2)]

# But do not mix positive and negative indexes!
myData2[c(-1, 2)]
```

```{block, type = 'rmdexercise'}
Do the following:

- Create the vector $x=(1, 7, 3, 4)$.
- Create the vector $y=(100, 99, 98, ..., 2, 1)$.
- Compute $x_2+y_4$ and $\cos(x_3) + \sin(x_2) e^{-y_2}$. (Answers: `104`, `-0.9899925`)
- Set $x_{2}=0$ and $y_{2}=-1$. Recompute the previous expressions. (Answers: `97`, `2.785875`)
- Index $y$ by $x+1$ and store it as `z`. What is the output? (Answer: `z` is `c(-1, 100, 97, 96)`)

```

### Some functions

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# Functions take arguments between parenthesis and transform them into an output
sum(myData)
prod(myData)

# Summary of an object
summary(myData)

# Length of the vector
length(myData)

# Mean, standard deviation, variance, covariance, correlation
mean(myData)
var(myData)
cov(myData, myData^2)
cor(myData, myData * 2)
quantile(myData)

# Maximum and minimum of vectors
min(myData)
which.min(myData)

# Usually the functions have several arguments, which are set by "argument = value"
# In this case, the second argument is a logical flag to indicate the kind of sorting
sort(myData) # If nothing is specified, decreasing = FALSE is assumed
sort(myData, decreasing = TRUE)

# Don't know what are the arguments of a function? Use args and help!
args(sort)
?sort
```

```{block, type = 'rmdexercise'}
Do the following:

- Compute the mean, median and variance of $y$. (Answers: `49.5`, `49.5`, `843.6869`)
- Do the same for $y+1$. What are the differences?
- What is the maximum of $y$? Where is it placed?
- Sort $y$ increasingly and obtain the 5th and 76th positions. (Answer: `c(4,75)`)
- Compute the covariance between $y$ and $y$. Compute the variance of $y$. Why do you get the same result?
```

### Matrices, data frames and lists

```{r, collapse = TRUE, error = TRUE, cache = TRUE}
# A matrix is an array of vectors
A <- matrix(1:4, nrow = 2, ncol = 2)
A

# Another matrix
B <- matrix(1, nrow = 2, ncol = 2, byrow = TRUE)
B

# Binding by rows or columns
rbind(1:3, 4:6)
cbind(1:3, 4:6)

# Entry-wise operations
A + 1
A * B

# Accessing elements
A[2, 1] # Element (2, 1)
A[1, ] # First row
A[, 2] # First column

# A data frame is a matrix with column names
# Useful when you have multiple variables
myDf <- data.frame(var1 = 1:2, var2 = 3:4)
myDf

# You can change names
names(myDf) <- c("newname1", "newname2")
myDf

# The nice thing is that you can access variables by its name with the $ operator
myDf$newname1

# And create new variables also (it has to be of the same
# length as the rest of variables)
myDf$myNewVariable <- c(0, 1)
myDf

# A list is a collection of arbitrary variables
myList <- list(myData = myData, A = A, myDf = myDf)

# Access elements by names
myList$myData
myList$A
myList$myDf

# Reveal the structure of an object
str(myList)
str(myDf)

# A less lengthy output
names(myList)
```

```{block, type = 'rmdexercise'}
Do the following:

- Create a matrix called `M` with rows given by `y[3:5]`, `y[3:5]^2` and `log(y[3:5])`.
- Create a data frame called `myDataFrame` with column names "y", "y2" and "logy" containing the vectors `y[3:5]`, `y[3:5]^2` and `log(y[3:5])`, respectively.
- Create a list, called `l`, with entries for `x` and `M`. Access the elements by their names.
- Compute the squares of `myDataFrame` and save the result as `myDataFrame2`.
- Compute the log of the sum of `myDataFrame` and `myDataFrame2`. Answer:

          ##         y       y2     logy
          ## 1 9.180087 18.33997 3.242862
          ## 2 9.159678 18.29895 3.238784
          ## 3 9.139059 18.25750 3.234656

```

## Model formulation and estimation by least squares {#modelsimp}

The simple linear model is a statistical tool for describing the relation between two random variables, $X$ and $Y$. For example, in the `pisa` dataset, $X$ could be `ReadingMean` and $Y=$ `MathMean`. The simple linear model is *constructed by assuming* that the linear relation
\begin{align}
Y = \beta_0 + \beta_1 X + \varepsilon (\#eq:1)
\end{align}
holds between $X$ and $Y$. In \@ref(eq:1), $\beta_0$ and $\beta_1$ are known as the *intercept* and *slope*, respectively. $\varepsilon$ is a random variable with mean zero and independent from $X$. It describes the *error* around the mean, or the effect of other variables that we do not model. Another way of looking at \@ref(eq:1) is
\begin{align}
\mathbb{E}[Y|X=x]=\beta_0+\beta_1x, (\#eq:2)
\end{align}
since $\mathbb{E}[\varepsilon|X=x]=0$.

The Left Hand Side (LHS) of \@ref(eq:2) is the *conditional expectation* of $Y$ given $X$. It represents how the mean of the random variable $Y$ is changing according to a particular value, denoted by $x$, of the random variable $X$. With the RHS, what we are saying is that the mean of $Y$ is changing in a *linear* fashion with respect to the value of $X$. Hence the interpretation of the coefficients:

- $\beta_0$: is the mean of $Y$ when $X=0$.
- $\beta_1$: is the increment in mean of $Y$ for an increment of one unit in $X=x$.

If we have a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ for our random variables $X$ and $Y$, we can estimate the unknown coefficients $\beta_0$ and $\beta_1$. In the `pisa` dataset, the sample are the observations for `ReadingMean` and `MathMean`. A possible way of estimating $(\beta_0,\beta_1)$ is by minimizing the *Residual Sum of Squares* (RSS):
\begin{align*}
\text{RSS}(\beta_0,\beta_1)=\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2.
\end{align*}
In other words, we look for the estimators $(\hat\beta_0,\hat\beta_1)$ such that
\begin{align*}
(\hat\beta_0,\hat\beta_1)=\arg\min_{(\beta_0,\beta_1)\in\mathbb{R}^2} \text{RSS}(\beta_0,\beta_1).
\end{align*}

It can be seen that *the minimizers of the RSS*^[They are unique and always exist. They can be obtained by solving $\frac{\partial}{\partial \beta_0}\text{RSS}(\beta_0,\beta_1)=0$ and $\frac{\partial}{\partial \beta_1}\text{RSS}(\beta_0,\beta_1)=0$.] are
\begin{align}
\hat\beta_0=\bar{Y}-\hat\beta_1\bar{X},\quad \hat\beta_1=\frac{s_{xy}}{s_x^2},(\#eq:3)
\end{align}
where:

- $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ is the *sample mean*.
- $s_x^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2$ is the *sample variance*. The sample standard deviation is $s_x=\sqrt{s_x^2}$.
- $s_{xy}=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})$ is the *sample covariance*. It measures the degree of linear association between $X_1,\ldots,X_n$ and $Y_1,\ldots,Y_n$. Once scaled by $s_xs_y$, it gives the *sample correlation coefficient*, $r_{xy}=\frac{s_{xy}}{s_xs_y}$.

```{block, type = 'rmdinsight'}
As a consequence of $\hat\beta_1=r_{xy}\frac{s_y}{s_x}$, $\hat\beta_1$ has aways the same sign as $r_{xy}$.
```


There are some important points hidden behind the election of RSS as the error criterion for obtaining $(\hat\beta_0,\hat\beta_1)$:

- *Why the vertical distances and not horizontal or perpendicular?* Because we want to minimize the error in the *prediction* of $Y$! Note that the treatment of the variables is *not symmetrical*^[In Chapter \@ref(pca) we will consider perpendicular distances.].
- *Why the squares in the distances and not the absolute value?* Due to mathematical convenience. Squares are nice to differentiate and are closely related with the Normal distribution.

Figure \@ref(fig:leastsquares) illustrates the influence of the distance employed in the sum of squares. Try to minimize the sum of squares for the different datasets. Is the best choice of intercept and slope independent of the type of distance?  

```{r, leastsquares, echo = FALSE, fig.cap = 'The effect of the kind of distance in the error criterion. The choices of intercept and slope that minimize the sum of squared distances for a kind of distance are not the optimal for a different kind of distance.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares/', height = '900px')
```

The data of the figure has been generated with the following code:
```{r, cache = TRUE}
# Generates 50 points from a N(0, 1): predictor and error
set.seed(34567) # Fixes the seed for the random generator
x <- rnorm(n = 50)
eps <- rnorm(n = 50)

# Responses
yLin <- -0.5 + 1.5 * x + eps
yQua <- -0.5 + 1.5 * x^2 + eps
yExp <- -0.5 + 1.5 * 2^x + eps

# Data
leastSquares <- data.frame(x = x, yLin = yLin, yQua = yQua, yExp = yExp)
```

```{block, type = 'rmdexercise'}
The minimizers of the error in the above illustration are indeed the coefficients given by the `lm` function. Check this for the three types of responses: `yLin`, `yQua` and `yExp`.
```

```{block, type = 'rmdinsight'}
The *population regression coefficients*, $(\beta_0,\beta_1)$, **are not the same** as the *estimated regression coefficients*, $(\hat\beta_0,\hat\beta_1)$:

- $(\beta_0,\beta_1)$ are the theoretical and **always** unknown quantities (except under controlled scenarios).
- $(\hat\beta_0,\hat\beta_1)$ are the estimates computed from the data. In particular, they are the output of `lm`. They are *random variables*, since they are computed from the random sample $(X_1,Y_1),\ldots,(X_n,Y_n)$.

In an abuse of notation, the term *regression line* is often used to denote both the *theoretical* ($y=\beta_0+\beta_1x$) and the *estimated* ($y=\hat\beta_0+\hat\beta_1x$) regression lines.
```

Once we have the least squares estimates $(\hat\beta_0,\hat\beta_1)$, we can define the next two concepts:

- The *fitted values* $\hat Y_1,\ldots,\hat Y_n$, where
\begin{align*}
\hat Y_i=\hat\beta_0+\hat\beta_1X_i,\quad i=1,\ldots,n.
\end{align*}
They are the vertical projections of $Y_1,\ldots,Y_n$ into the fitted line (see Figure \@ref(fig:leastsquares)).

- The *estimated residuals* $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$, where
\begin{align*}
\hat\varepsilon_i=Y_i-\hat Y_i,\quad i=1,\ldots,n.
\end{align*}
They are the vertical distances between actual data $(X_i,Y_i)$ and fitted data $(X_i,\hat Y_i)$. Hence, another way of writing the minimum RSS is $\sum_{i=1}^n(Y_i-\hat\beta_0-\hat\beta_1X_i)^2=\sum_{i=1}^n\hat\varepsilon_i^2$.

To conclude this section, we check that the regression coefficients given by `lm` are indeed the ones given in \@ref(eq:3).

```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Covariance
Sxy <- cov(x, yLin)

# Variance
Sx2 <- var(x)

# Coefficients
beta1 <- Sxy / Sx2
beta0 <- mean(yLin) - beta1 * mean(x)
c(beta0, beta1)

# Output from lm
mod <- lm(yLin ~ x, data = leastSquares)
mod$coefficients
```

```{block, type = 'rmdexercise'}
Adapt the code conveniently for doing the same checking with

- $X=$`ReadingMean` and $Y=$`MathMean` from the `pisa` dataset.
- $X=$`logGDPp` and $Y=$`MathMean`.
- $X=$`Population2010` and $Y=$`Seats2013.2023` from the `US` dataset.
- $X=$`Population2010` and $Y=$`Seats2011` from the `EU` dataset.
```

## Assumptions of the model {#assumptions}

Why do we need assumptions? To make **inference** on the model parameters. In other words, to infer properties about the *unknown* population coefficients $\beta_0$ and $\beta_1$ from the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$.

```{r, linearmodel, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The key concepts of the simple linear model. The yellow band denotes where the $95\\%$ of the data is, according to the model.', cache = TRUE}
knitr::include_graphics("images/R/linearmodel.png")
```

The assumptions of the linear model are:

i. **Linearity**: $\mathbb{E}[Y|X=x]=\beta_0+\beta_1x$.
ii. **Homoscedasticity**: $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$, with $\sigma^2$ constant for $i=1,\ldots,n$.
iii. **Normality**: $\varepsilon_i\sim\mathcal{N}(0,\sigma^2)$ for $i=1,\ldots,n$.
iv. **Independence of the errors**: $\varepsilon_1,\ldots,\varepsilon_n$ are independent (or uncorrelated, $\mathbb{E}[\varepsilon_i\varepsilon_j]=0$, $i\neq j$, since they are assumed to be Normal).

A good one-line summary of the linear model is (independence is assumed)
\begin{align*}
Y|X=x\sim \mathcal{N}(\beta_0+\beta_1x,\sigma^2)
\end{align*}

```{block, type = 'rmdinsight'}
Recall:

- Nothing is said about the distribution of $X$. Indeed, $X$ could be deterministic (called *fixed design*) or random (*random design*).

- The linear model **assumes that $Y$ is continuous** due to the normality of the errors. However, **$X$ can be discrete**!
```

Figures \@ref(fig:linearmodelgood) and \@ref(fig:linearmodelbad) represent situations where the assumptions of the model are respected and violated, respectively. For the moment, we will focus on building the intuition for checking the assumptions visually. In Chapter \@ref(mult) we will see more sophisticated methods for checking the assumptions. We will see also what are the possible fixes to the failure of assumptions.

```{r, linearmodelgood, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Perfectly valid simple linear models (all the assumptions are verified).', cache = TRUE}
knitr::include_graphics("images/R/linearmodelgood.png")
```

```{r, linearmodelbad, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Problematic simple linear models (a single assumption does not hold).', cache = TRUE}
knitr::include_graphics("images/R/linearmodelbad.png")
```

```{block, type = 'rmdexercise'}
The dataset `assumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/assumptions.RData)) contains the variables `x1`, ..., `x9` and `y1`, ..., `y9`. For each regression `y1 ~ x1`, ..., `y9 ~ x9`:

- Check whether the assumptions of the linear model are being satisfied (make a scatterplot with a regression line).
- State which assumption(s) are violated and justify your answer.
```

## Inference for the model coefficients {#inference}

The assumptions introduced in the previous section allow to specify what is the distribution of the *random variables* $\hat\beta_0$ and $\hat\beta_1$. As we will see, this is a key point for making inference on $\beta_0$ and $\beta_1$.

The distributions are derived conditionally on the sample predictors $X_1,\ldots,X_n$. In other words, we assume that the randomness of $Y_i=\beta_0+\beta_1X_i+\varepsilon_i$, $i=1,\ldots,n$, comes only from the error terms and not from the predictors. To denote this, we employ lowercase for the sample predictors $x_1,\ldots,x_n$.

### Distributions of the fitted coefficients

The distributions of $\hat\beta_0$ and $\hat\beta_1$ are:
\begin{align}
\hat\beta_0\sim\mathcal{N}\left(\beta_0,\mathrm{SE}(\hat\beta_0)^2\right),\quad\hat\beta_1\sim\mathcal{N}\left(\beta_1,\mathrm{SE}(\hat\beta_1)^2\right)(\#eq:norm1)
\end{align}
where
\begin{align}
\mathrm{SE}(\hat\beta_0)^2=\frac{\sigma^2}{n}\left[1+\frac{\bar X^2}{s_x^2}\right],\quad \mathrm{SE}(\hat\beta_1)^2=\frac{\sigma^2}{ns_x^2}.(\#eq:se1)
\end{align}
Recall that an equivalent form for \@ref(eq:norm1) is (why?)
\begin{align*}
\frac{\hat\beta_0-\beta_0}{\mathrm{SE}(\hat\beta_0)}\sim\mathcal{N}(0,1),\quad\frac{\hat\beta_1-\beta_1}{\mathrm{SE}(\hat\beta_1)}\sim\mathcal{N}(0,1).
\end{align*}

Some important remarks on \@ref(eq:norm1) and \@ref(eq:se1) are

- **Bias**. Both estimates are unbiased. That means that their expectations are the true coefficients.
- **Variance**. The variances $\mathrm{SE}(\hat\beta_0)^2$ and $\mathrm{SE}(\hat\beta_1)^2$ have an interesting interpretation in terms of its components:

    - *Sample size $n$*. As the sample size grows, the precision of the estimators increases, since both variances decrease.
    - *Error variance $\sigma^2$*. The more disperse the error is, the less precise the estimates are, since the more vertical variability is present.
    - *Predictor variance $s_x^2$*. If the predictor is spread out (large $s_x^2$), then it is easier to fit a regression line: we have information about the data trend over a long interval. If $s_x^2$ is small, then all the data is concentrated on a narrow vertical band, so we have a much more limited view of the trend.

    - *Mean $\bar X$*. It has influence only on the precision of $\hat\beta_0$. The larger $\bar X$ is, the less precise $\hat\beta_0$ is.


```{r, randomcoefs, echo = FALSE, fig.cap = 'Illustration of the randomness of the fitted coefficients $(\\hat\\beta_0,\\hat\\beta_1)$ and the influence of $n$, $\\sigma^2$ and $s_x^2$. The sample predictors $x_1,\\ldots,x_n$ are fixed and new responses $Y_1,\\ldots,Y_n$ are generated each time from a linear model $Y=\\beta_0+\\beta_1X+\\varepsilon$.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/lm-random/', height = '1000px')
```

The problem with \@ref(eq:norm1) and \@ref(eq:se1) is that *$\sigma^2$ is unknown* in practice, so we need to estimate $\sigma^2$ from the data. We do so by computing the *sample variance of the fitted residuals* $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$. First note that *the fitted residuals have zero mean*. This can be easily seen by replacing $\hat\beta_0=\bar Y-\hat\beta_1\bar X$:
\begin{align}
\bar\varepsilon =\frac{1}{n}\sum_{i=1}^n(Y_i-\hat\beta_0-\hat\beta_1X_i)=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar Y+\hat\beta_1\bar X-\hat\beta_1X_i)=0.
\end{align}
Due to this, we can and we can do it by computing a *rescaled* sample variance of the fitted residuals:
\begin{align*}
\hat\sigma^2=\frac{\sum_{i=1}^n\hat\varepsilon_i^2}{n-2}.
\end{align*}
Note the $n-2$ in the denominator, instead of $n$! $n-2$ are the *degrees of freedom* and is the number of data points minus the number of already fitted parameters. The interpretation is that "we have consumed $2$ degrees of freedom of the sample on fitting $\hat\beta_0$ and $\hat\beta_1$".

If we use the estimate $\hat\sigma^2$ instead of $\sigma^2$, we get different -- and more useful -- distributions for $\beta_0$ and $\beta_1$:
\begin{align}
\frac{\hat\beta_0-\beta_0}{\hat{\mathrm{SE}}(\hat\beta_0)}\sim t_{n-2},\quad\frac{\hat\beta_1-\beta_1}{\hat{\mathrm{SE}}(\hat\beta_1)}\sim t_{n-2}(\#eq:norm3)
\end{align}
where $t_{n-2}$ represents the *Student's $t$ distribution*^[The Student's $t$ distribution has *heavier tails* than the normal, which means that large observations in absolute value are more likely. $t_n$ converges to a $\mathcal{N}(0,1)$ when $n$ is large. For example, with $n$ larger than $30$, the normal is a good approximation.] with $n-2$ degrees of freedom and
\begin{align}
\hat{\mathrm{SE}}(\hat\beta_0)^2=\frac{\hat\sigma^2}{n}\left[1+\frac{\bar X^2}{s_x^2}\right],\quad \hat{\mathrm{SE}}(\hat\beta_1)^2=\frac{\hat\sigma^2}{ns_x^2}(\#eq:se2)
\end{align}
are the estimates of $\mathrm{SE}(\hat\beta_0)^2$ and $\mathrm{SE}(\hat\beta_1)^2$. The LHS of \@ref(eq:norm3) is called the *$t$-statistic* because of its distribution. The interpretation of \@ref(eq:se2) is analogous to the one of \@ref(eq:se1).

### Confidence intervals for the coefficients

Due to \@ref(eq:norm3) and \@ref(eq:se2), we can have the $100(1-\alpha)\%$ Confidence Intervals (CI) for the coefficients:
\begin{align}
\left(\hat\beta_j\pm\hat{\mathrm{SE}}(\hat\beta_j)t_{n-2;\alpha/2}\right),\quad j=0,1,(\#eq:ci)
\end{align}
where $t_{n-2;\alpha/2}$ is the *$\alpha/2$-upper quantile of the $t_{n-2}$* (see Figure \@ref(fig:ttest)). Usually, $\alpha=0.10,0.05,0.01$ are considered.

```{r, ttest, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The Student\'s $t$ distribution for the $t$-statistics associated to null intercept and slope, for the `y1 ~ x1` regression of the `assumptions` dataset.', cache = TRUE}
knitr::include_graphics("images/R/ttest.png")
```

```{block, type = 'rmdtip'}
Do you need to remember the above equations? **No, althought you need to fully understand them.** `R` + `R Commander` will compute everything for you through the functions `lm`, `summary` and `confint`.
```

This *random* CI *contains the unknown coefficient $\beta_j$ with a probability of $1-\alpha$*. Note also that the CI is symmetric around $\hat\beta_j$. A simple way of understanding this concept is as follows. Suppose you have 100 samples generated according to a linear model. If you compute the CI for a coefficient, then in approximately $100(1-\alpha)$ of the samples the true coefficient would be actually inside the random CI. This is illustrated in Figure \@ref(fig:ci).

```{r, ci, echo = FALSE, fig.cap = 'Illustration of the randomness of the CI for $\\beta_0$ at $100(1-\\alpha)\\%$ confidence. The plot shows 100 random CIs for $\\beta_0$, computed from 100 random datasets generated by the same linear model, with intercept $\\beta_0$. The illustration for $\\beta_1$ is completely analogous.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/ci-random/', height = '900px')
```

Let's see how we can compute the CIs for $\beta_0$ and $\beta_1$ in practice. We do it in the first regression of the `assumptions` dataset. Assuming you have loaded the dataset, in `R` we can simply type:
```{r, echo = FALSE, cache = TRUE}
load("datasets/assumptions.RData")
```
```{r, collapse = TRUE, cache = TRUE}
mod1 <- lm(y1 ~ x1, data = assumptions)
confint(mod1)
```
In this example, the 95% confidence interval for $\beta_0$ is $(-0.2257, 0.1572)$. For $\beta_1$ is $(-0.5587, -0.2881)$. Therefore, we can say that **with a 95% confidence `x1` has a negative effect on `y1`**. If the CI for $\beta_1$ was $(-0.2256901, 0.1572392)$, we could not arrive to the same conclusion, since the CI contains both positive and negative numbers.

By default, the confidence interval is computed for $\alpha=0.05$. You can change this on the `level` argument, for example:
```{r, collapse = TRUE, cache = TRUE}
confint(mod1, level = 0.90) # alpha = 0.10
confint(mod1, level = 0.95) # alpha = 0.05
confint(mod1, level = 0.99) # alpha = 0.01
```

```{block, type = 'rmdinsight'}
Note that the larger the confidence of the interval, the longer -- thus less useful -- it is. For example, the interval $(-\infty,\infty)$ contains any coefficient with a 100% confidence, but is completely useless.
```

If you want to make the CIs through the help of `R Commander` (assuming the dataset has been loaded and is the active one), then do the following:

1. Fit the linear model (`'Statistics' -> 'Fit models' -> 'Linear regression...'`).
2. Go to `'Models' -> 'Confidence intervals...'` and then input the `'Condifence Level'`.

```{block, type = 'rmdexercise'}
Compute the CIs (95%) for the coefficients of the regressions:

- `y2 ~ x2`
- `y6 ~ x6`
- `y7 ~ x7`

Do you think all of them are meaningful? Which ones are and why? (Recall: inference on the model makes sense if assumptions of the model are verified)
```

```{block, type = 'rmdexercise'}
Compute the CIs for the coefficients of the following regressions:

- `MathMean ~ ScienceMean` (`pisa`)
- `MathMean ~ ReadingMean` (`pisa`)
- `Seats2013.2023 ~ Population2010` (`US`)
- `CamCom2011 ~ Population2010` (`EU`)

For the above regressions, can we conclude with a 95% confidence that the effect of the predictor is positive in the response?
```

A CI for $\sigma^2$ can be also computed, but is less important in practice. The formula is:
\begin{align*}
\left(\frac{n-2}{\chi^2_{n-2;\alpha/2}}\hat\sigma^2,\frac{n-2}{\chi^2_{n-2;1-\alpha/2}}\hat\sigma^2\right)
\end{align*}
where $\chi^2_{n-2;q}$ is the $q$-upper quantile of the *$\chi^2$ distribution*^[$\chi_n^2$ is the distribution of the sum of the squares of $n$ random variables $\mathcal{N}(0,1)$.] with $n-2$ degrees of freedom, $\chi^2_{n-2}$. Note that the CI is *not symmetric* around $\hat\sigma^2$.

```{block, type = 'rmdexercise'}
Compute the CI for $\sigma^2$ for the regression of `MathMean` on `logGDPp` in the `pisa` dataset. Do it for $\alpha=0.10,0.05,0.01$.

- To compute $\chi^2_{n-2;q}$, you can do:

  - In `R` by `qchisq(p = q, df = n - 2, lower.tail = FALSE)`.
  - In `R Commander`, go to `'Distributions' -> 'Continuous distributions' -> 'Chi-squared distribution' -> 'Chi-squared quantiles'` and then select `'Upper tail'`. Input $q$ as the `'Probabilities'` and $n-2$ as the `'Degrees of freedom'`.

- To compute $\hat\sigma^2$, use `summary(lm(MathMean ~ logGDPp, data = pisa))$sigma^2`. Remember that there are 65 countries in the study.

Answers: `c(1720.669, 3104.512)`, `c(1635.441, 3306.257)` and `c(1484.639, 3752.946)`.
```

### Testing on the coefficients

The distributions in \@ref(eq:norm3) also allow to conduct a formal *hypothesis test* on the coefficients $\beta_j$, $j=0,1$. For example the test for *significance* (shortcut for *significantly difference from zero*) is specially important, that is, the test of the hypotheses
\begin{align*}
H_0:\beta_j=0
\end{align*}
for $j=0,1$. The test of $H_0:\beta_1=0$ is specially interesting, since it allows to answer whether *the variable $X$ has a significant linear effect on $Y$*. The statistic used for testing for significance is the $t$-statistic
\begin{align*}
\frac{\hat\beta_j-0}{\hat{\mathrm{SE}}(\hat\beta_j)},
\end{align*}
which is distributed as a $t_{n-2}$ *under the (veracity of) the null hypothesis*.

```{block, type = 'rmdinsight'}
Remember the analogy of hypothesis testing vs a trial, as given in the table below.
```

|         Hypothesis testing       |                      Trial                        |
|:---------------------------------|:--------------------------------------------------|
| Null hypothesis $H_0$ | **Accused of comitting a crime**. It has the "presumption of innocence", which means that it is *not guilty* until there is enough evidence to supporting its guilt |
| Sample $X_1,\ldots,X_n$ | Collection of small **evidences supporting innocence and guilt**  |
| Statistic $T_n$ | **Summary of the evicences presented** by the prosecutor and defense lawyer   |
| Distribution of $T_n$ under $H_0$ | The **judge** conducting the trial. Evaluates the evidence presented by both sides and presents a verdict for $H_0$  |
| Significance level $\alpha$ | $1-\alpha$ is the **strength of evidences required by the judge for condemning $H_0$**. The judge allows evidences that on average condemn $100\alpha\%$ of the innocents! $\alpha=0.05$ is considered a reasonable level |
| $p$-value | **Decision** of the judge. If $p\text{-value}<\alpha$, $H_0$ is declared guilty. Otherwise, is declared not guilty |
| $H_0$ is rejected | $H_0$ is declared guilty: there are **strong evidences supporting its guilt** |
| $H_0$ is not rejected | $H_0$ is declared not guilty: either is **innocent or there are no enough evidences supporting its guilt** |

More formally, the $p$-value is defined as:

> The $p$-value is the probability of obtaining a statistic more unfavourable to $H_0$ than the observed, assuming that $H_0$ is true.

Therefore, **if the $p$-value is small** (smaller than the chosen level $\alpha$), **it is unlikely that the evidence against $H_0$ is due to randomness. As a consequence, $H_0$ is rejected**. If the $p$-value is large (larger than $\alpha$), then it is more possible that the evidences against $H_0$ are merely due to the randomness of the data. In this case, we do not reject $H_0$.

The null hypothesis $H_0$ is tested *against* the *alternative hypothesis*, $H_1$. If $H_0$ is rejected, it is *rejected in favor* of $H_1$. The alternative hypothesis can be *bilateral*, such as
\begin{align*}
H_0:\beta_j= 0\quad\text{vs}\quad H_1:\beta_j\neq 0
\end{align*}
or *unilateral*, such as
\begin{align*}
H_0:\beta_j\geq (\leq)0\quad\text{vs}\quad H_1:\beta_j<(>)0
\end{align*}
For the moment, we will focus only on the bilateral case.

```{block, type = 'rmdinsight'}
The connection of a $t$-test for $H_0:\beta_j=0$ and the CI for $\beta_j$, both at level $\alpha$, is the following.

**Is $0$ inside the CI for $\beta_j$?**

- **Yes $\leftrightarrow$ do not reject $H_0$**.
- **No $\leftrightarrow$ reject $H_0$**.
```

The tests for significance are built-in in the `summary` function, as we glimpsed in Section \@ref(euus). For `mod1`, we have:
```{r, collapse = TRUE, cache = TRUE}
summary(mod1)
```
The `Coefficients` block of the output of `summary` contains the next elements regarding the test $H_0:\beta_j=0$ vs $H_1:\beta_j\neq0$:

- `Estimate`: least squares estimate $\hat\beta_j$.
- `Std. Error`: estimated standard error $\hat{\mathrm{SE}}(\hat\beta_j)$.
- `t value`: $t$-statistic $\frac{\hat\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)}$.
- `Pr(>|t|)`: $p$-value of the $t$-test.
- `Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1`: codes indicating the size of the $p$-value. The more stars, the more evidence supporting that $H_0$ does not hold.

In the above output for `summary(mod1)`, $H_0:\beta_0=0$ is not rejected at any reasonable level for $\alpha$ (that is, $0.10$,$0.05$ and $0.01$). Hence $\hat\beta_0$ is not significantly different from zero and $\beta_0$ is not significant for the regression. On the other hand, $H_0:\beta_1=0$ is rejected at any level $\alpha$ larger than the $p$-value, `3.77e-09`. Therefore, $\beta_1$ is significant for the regression (and $\hat\beta_1$ is not significantly different from zero).

```{block, type = 'rmdexercise'}
For the `assumptions` dataset, do the next:

- Regression `y7 ~ x7`. Check that:
    - The intercept of  is not significant for the regression at any reasonable level $\alpha$.
    - The slope is significant for any $\alpha>10^{-7}$.

- Regression `y6 ~ x6`. Assume the linear model assumptions are verified.
    - Check that $\hat\beta_0$ is significantly different from zero at any level
$\alpha$.
    - For which $\alpha=0.10,0.05,0.01$ is $\hat\beta_1$ significantly different from zero?
```  

```{block, type = 'rmdexercise'}
Re-analyse the significance of the coefficients in `Seats2013.2023 ~ Population2010` and `Seats2011 ~ Population2010` for the `US` and `EU` datasets, respectively.
```

## Prediction {#prediction}

The forecast of $Y$ from $X=x$ in the linear model is approached by two different ways:

1. Inference on the **conditional mean** of $Y$ given $X=x$, $\mathbb{E}[Y|X=x]$. This is a deterministic quantity, which equals $\beta_0+\beta_1x$ in the linear model.
2. Prediction of the **conditional response** $Y|X=x$. This is a random variable, which in the linear model is distributed as $\mathcal{N}(\beta_0+\beta_1x,\sigma^2)$.

Let's study first the inference on the conditional mean. $\beta_0+\beta_1x$ is estimated by $\hat y=\hat\beta_0+\hat\beta_1x$. Then, is a *deterministic quantity estimated by a random variable*. Moreover, it can be shown that the $100(1-\alpha)\%$ CI for $\beta_0+\beta_1x$ is
\begin{align}
\left(\hat y \pm t_{n-2:\alpha/2}\sqrt{\frac{\hat\sigma^2}{n}\left(1+\frac{(x-\bar x)^2}{s_x^2}\right)}\right).(\#eq:ci1)
\end{align}
Some important remarks on \@ref(eq:ci1) are:

- **Bias**. The CI is centered around $\hat y=\hat\beta_0+\hat\beta_1x$, which obviously depends on $x$.

- **Variance**. The variance that determines the length of the CI is $\frac{\hat\sigma^2}{n}\left(1+\frac{(x-\bar x)^2}{s_x^2}\right)$. Its interpretation is very similar to the one given for $\hat\beta_0$ and $\hat\beta_1$ in Section \@ref(inference):

    - *Sample size $n$*. As the sample size grows, the length of the CI decreases, since theestimates $\hat\beta_0$ and $\hat\beta_1$ become more precise.
    - *Error variance $\sigma^2$*. The more disperse $Y$ is, the less precise $\hat\beta_0$ and $\hat\beta_1$ are, hence the more variance on estimating $\beta_0+\beta_1x$.
    - *Predictor variance $s_x^2$*. If the predictor is spread out (large $s_x^2$), then it is easier to "anchor" the regression line. This helps on reducing the variance, but up to a **certain limit**: there is a variance component purely dependent on the error!
    - *Centrality $(x-\bar x)^2$*. The more extreme $x$ is, the wider the CI becomes. This is due to the "leverage" of the slope estimate $\hat\beta_1$: a small deviation from the true $\beta_1$ is magnified when $x$ is far away from $\bar x$, hence the more variability in these points. The minimum is achieved with $x=\bar x$, but it does not correspond to zero variance.

Figure \@ref(fig:cipred) helps visualizing these concepts interactively.

```{r, cipred, echo = FALSE, fig.cap = 'Illustration of the CIs for the conditional mean and response. Note how the length of the CIs is influenced by $x$, specially for the conditional mean.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/ci-prediction/', height = '1000px')
```

The prediction and computation of CIs can be done with the `R` function `predict`. The objects required for `predict` are: first, the output of `lm`; second, a `data.frame` containing the locations $x$ where we want to predict $\beta_0+\beta_1x$. To illustrate the use of `predict`, we are going to use the `pisa` dataset. In case you do not have it loaded, you can download it [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/pisa.RData) as an `.RData` file.

```{r, echo = FALSE, warning = FALSE, cache = TRUE}
pisa <- read.csv(file = "datasets/pisa.csv", header = TRUE)
library(car)
```

```{r, collapse = TRUE, cache = TRUE}
# Plot the data and the regression line (alternatively, using R Commander)
scatterplot(MathMean ~ ReadingMean, data = pisa, smooth = FALSE)

# Fit a linear model (alternatively, using R Commander)
model <- lm(MathMean ~ ReadingMean, data = pisa)
summary(model)

# Data for which we want a prediction for the mean
# Important! You have to name the column with the predictor name!
newData <- data.frame(ReadingMean = 400)

# Prediction
predict(model, newdata = newData)

# Prediction with 95% confidence interval (the default)
# CI: (lwr, upr)
predict(model, newdata = newData, interval = "confidence")
predict(model, newdata = newData, interval = "confidence", level = 0.95)

# Other levels
predict(model, newdata = newData, interval = "confidence", level = 0.90)
predict(model, newdata = newData, interval = "confidence", level = 0.99)

# Predictions for several values
summary(pisa$ReadingMean)
newData2 <- data.frame(ReadingMean = c(400, 450, 500, 550))
predict(model, newdata = newData2, interval = "confidence")
```

```{block, type = 'rmdexercise'}
For the `pisa` dataset, do the following:

- Regress `MathMean` on `logGDPp` excluding Shanghai-China, Vietnam and Qatar (use `subset = -c(1, 16, 62)`). Name the fitted model `modExercise`.
- Show the scatterplot with regression line (use `subset = -c(1, 16, 62)`).
- Compute the estimate for the conditional mean of `MathMean` for a `logGDPp` of $10.263$ (Spain's). What is the CI at $\alpha=0.05$?
- Do the same with the `logGDPp` of Sweden, Denmark, Italy and United States.
- Check that `modExercise\$fitted.values` is the same as `predict(modExercise, newdata = data.frame(logGDPp = pisa$logGDPp))` (except for the three countries omitted). Why is so?
```

Let's study now the prediction of the conditional response $Y|X$. $Y|X$ is predicted by $\hat y=\hat\beta_0+\hat\beta_1x$ (the estimated conditional mean). So we estimate the unknown response $Y|X$ simply by its conditional mean. This is clearly a different situation: now we **estimate a random variable by another random variable**. As a consequence, there is a price to pay in terms of extra variability, and this is reflected in the $100(1-\alpha)\%$ CI for $Y|X$:
\begin{align}
\left(\hat y \pm t_{n-2:\alpha/2}\sqrt{\hat\sigma^2+\frac{\hat\sigma^2}{n}\left(1+\frac{(x-\bar x)^2}{s_x^2}\right)}\right)(\#eq:ci2)
\end{align}
The CI \@ref(eq:ci2) is very similar to \@ref(eq:ci1), but there is a key difference: it is always longer due to the extra term $\hat\sigma^2$. In Figure \@ref(fig:cipred) you can visualize the differences between both CIs.

```{block, type = 'rmdinsight'}
Similarities and differences in the prediction of the conditional mean $\mathbb{E}[Y|X=x]$ and conditional response $Y|X=x$:

- *Similarities*. The estimate is the same, $\hat y=\hat\beta_0+\hat\beta_1x$. Both CI are centered in $\hat y$ and share the term $\frac{\hat\sigma^2}{n}\left(1+\frac{(x-\bar x)^2}{s_x^2}\right)$ in the variance.
- *Differences*. $\mathbb{E}[Y|X=x]$ is deterministic and $Y|X=x$ is random. Therefore, the variance is larger for the prediction of $Y|X=x$: there is an extra $\hat\sigma^2$ term in the variance of its prediction.
```

The prediction and computation of CIs can be done with the `R` function predict. The objects required for predict are: first, the output of `lm`; second, a `data.frame` containing the locations $x$ where we want to predict $\beta_0+\beta_1 x$. To illustrate the use of predict, we are going to use the `pisa` dataset. In case you do not have it loaded, you can download it [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/pisa.RData) as an `.RData` file.

The prediction of a new observation can be done via the function `predict`, which also provides confidence intervals.
```{r, collapse = TRUE, cache = TRUE}
# Prediction with 95% confidence interval (the default) CI: (lwr, upr)
predict(model, newdata = newData, interval = "prediction")

# Other levels
predict(model, newdata = newData, interval = "prediction", level = 0.90)
predict(model, newdata = newData, interval = "prediction", level = 0.99)

# Predictions for several values
predict(model, newdata = newData2, interval = "prediction")

# Comparison with the mean CI
predict(model, newdata = newData2, interval = "confidence")
predict(model, newdata = newData2, interval = "prediction")
```

```{block, type = 'rmdexercise'}
Redo the third and fourth points of the previous exercise with CIs for the conditional response. In addition, check if the `MathMean` scores of Sweden, Denmark, Vietnam and Qatar are inside or outside the prediction CIs.
```

## ANOVA and model fit {#fit}

### ANOVA

As we have seen in Sections \@ref(inference) and \@ref(prediction), the variance of the error, $\sigma^2$, plays a fundamental role in the inference for the model coefficients and prediction. In this section we will see how the variance of $Y$ is decomposed into two parts, each one corresponding to the regression and to the error, respectively. This decomposition is called the *ANalysis Of VAriance* (ANOVA).

Before explaining ANOVA, it is important to recall an interesting result: *the mean of the fitted values $\hat Y_1,\ldots,\hat Y_n$ is the mean of $Y_1,\ldots, Y_n$*. This is easily seen if we plug-in the expression of $\hat\beta_0$:
\begin{align*}
\frac{1}{n}\sum_{i=1}^n \hat Y_i=\frac{1}{n}\sum_{i=1}^n \left(\hat \beta_0+\hat\beta_1X_i\right)=\hat \beta_0+\hat\beta_1\bar X=\left(\bar Y - \hat\beta_1\bar X \right) + \hat\beta_1\bar X=\bar Y.
\end{align*}
The ANOVA decomposition considers the following measures of variation related with the response:

- $\text{SST}=\sum_{i=1}^n\left(Y_i-\bar Y\right)^2$, the **total sum of squares**. This is the *total variation* of $Y_1,\ldots,Y_n$, since $\text{SST}=ns_y^2$, where $s_y^2$ is the sample variance of $Y_1,\ldots,Y_n$.
- $\text{SSR}=\sum_{i=1}^n\left(\hat Y_i-\bar Y\right)^2$, the **regression sum of squares**^[Recall that SSR is different from RSS (Residual Sum of Squares, Section \@ref(modelsimp)).]. This is the variation explained by the regression line, that is, *the variation from $\bar Y$ that is explained by the estimated conditional mean $\hat Y_i=\hat\beta_0+\hat\beta_1X_i$*. $\text{SSR}=ns_{\hat y}^2$, where $s_{\hat y}^2$ is the sample variance of $\hat Y_1,\ldots,\hat Y_n$.
- $\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2$, the **sum of squared errors**^[Recall that SSE and RSS (for $(\hat \beta_0,\hat \beta_1)$) are just different names for referring to the same quantity: $\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2=\sum_{i=1}^n\left(Y_i-\hat \beta_0-\hat \beta_1X_i\right)^2=\mathrm{RSS}\left(\hat \beta_0,\hat \beta_1\right)$.]. Is the variation around the conditional mean. Recall that $\text{SSE}=\sum_{i=1}^n \hat\varepsilon_i^2=(n-2)\hat\sigma^2$, where $\hat\sigma^2$ is the sample variance of $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$.

The ANOVA decomposition is
\begin{align}
\underbrace{\text{SST}}_{\text{Variation of }Y_i's} = \underbrace{\text{SSR}}_{\text{Variation of }\hat Y_i's} + \underbrace{\text{SSE}}_{\text{Variation of }\hat \varepsilon_i's} (\#eq:anova)
\end{align}
or, equivalently (dividing by $n$ in \@ref(eq:anova)),
\begin{align*}
\underbrace{s_y^2}_{\text{Variance of $Y_i$'s}} = \underbrace{s_{\hat y}^2}_{\text{Variance of $\hat Y_i$'s}} + \underbrace{(n-2)/n\times\hat\sigma^2}_{\text{Variance of $\hat\varepsilon_i$'s}}.
\end{align*}
The graphical interpretation of \@ref(eq:anova) is shown in Figures \@ref(fig:anova) and \@ref(fig:anovaillus).

```{r, anova, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Visualization of the ANOVA decomposition. SST measures the variation of $Y_1,\\ldots,Y_n$ with respect to $\\bar Y$. SST measures the variation with respect to the conditional means, $\\hat \\beta_0+\\hat\\beta_1X_i$. SSE collects the variation of the residuals.', fig.show = 'hold'}
knitr::include_graphics("images/R/anova.png")
```

```{r, anovaillus, echo = FALSE, fig.cap = 'Illustration of the ANOVA decomposition and its dependence on $\\sigma^2$ and $\\hat\\sigma^2$.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/anova/', height = '800px')
```

The ANOVA table summarizes the decomposition of the variance. Here is given in the layout employed by `R`.

|  | Degrees of freedom | Sum Squares | Mean Squares | $F$-value | $p$-value |
|--|------------|--------|---------|----------------|----------------|
| Predictor | $1$ | SSR | $\frac{\text{SSR}}{1}$ | $\frac{\text{SSR}/1}{\text{SSE}/(n-2)}$ | $p$ |
| Residuals | $n - 2$ | SSE | $\frac{\text{SSE}}{n-2}$ | | |

The `anova` function in `R` takes a model as an input and returns the ANOVA table. In `R Commander`, the ANOVA table can be computed by going to `'Models' -> 'Hypothesis tests' -> 'ANOVA table...'`. In the `'Type of Tests'` option, select `'Sequential ("Type I")'`. (This types `anova()` for you...)

```{r, collapse = TRUE, cache = TRUE}
# Fit a linear model (alternatively, using R Commander)
model <- lm(MathMean ~ ReadingMean, data = pisa)
summary(model)

# ANOVA table
anova(model)
```

The "$F$-value" of the ANOVA table represents the value of the $F$-statistic $\frac{\text{SSR}/1}{\text{SSE}/(n-2)}$. This statistic is employed to test
\begin{align*}
H_0:\beta_1=0\quad\text{vs.}\quad H_1:\beta_1\neq 0,
\end{align*}
that is, the hypothesis of no linear dependence of $Y$ on $X$. The result of this test is completely equivalent to the $t$-test for $\beta_1$ that we saw in Section \@ref(inference) (this is something *specific for simple linear regression* -- the $F$-test will not be equivalent to the $t$-test for $\beta_1$ in Chapter \@ref(mult)). I It happens that
\begin{align*}
F=\frac{\text{SSR}/1}{\text{SSE}/(n-2)}\stackrel{H_0}{\sim} F_{1,n-2},
\end{align*}
where $F_{1,n-2}$ is the *Snedecor's $F$ distribution*^[The $F_{n,m}$ distribution arises as the quotient of two independent random variables $\chi^2_n$ and $\chi^2_m$, $\frac{\chi^2_n/n}{\chi^2_m/m}$.] with $1$ and $n-2$ degrees of freedom. If $H_0$ is true, then $F$ is expected to be *small* since SSR will be close to zero. The $p$-value of this test is the same as the $p$-value of the $t$-test for $H_0:\beta_1=0$.

Recall that the $F$-statistic, its $p$-value and the degrees of freedom are also given in the output of `summary`.

```{block, type = 'rmdexercise'}
For the `y6 ~ x6` and `y7 ~ x7` in the `assumptions` dataset, compute their ANOVA tables. Check that the $p$-values of the $t$-test for $\beta_1$ and the $F$-test are the same.
```

### The $R^2$

The *coefficient of determination* $R^2$ is closely related with the ANOVA decomposition. $R^2$ is defined as
\begin{align*}
R^2=\frac{\text{SSR}}{\text{SST}}=\frac{\text{SSR}}{\text{SSR}+\text{SSE}}=\frac{\text{SSR}}{\text{SSR}+(n-2)\hat\sigma^2}.
\end{align*}
$R^2$ measures the **proportion of variation** of the response variable $Y$ that is **explained** by the predictor $X$ through the regression. The proportion of total variation of $Y$ that is not explained is $1-R^2=\frac{\text{SSE}}{\text{SST}}$. Intuitively, $R^2$ measures the **tightness of the data cloud around the regression line**, since is related directly with $\hat\sigma^2$. Check in Figure \@ref(fig:anovaillus) how changing the value of $\sigma^2$ (not $\hat\sigma^2$, but $\hat\sigma^2$ is obviously dependent on $\sigma^2$) affects the $R^2$.

The $R^2$ is related with the sample *correlation coefficient*
\begin{align*}
r_{xy}=\frac{s_{xy}}{s_xs_y}=\frac{\sum_{i=1}^n \left(X_i-\bar X \right)\left(Y_i-\bar Y \right)}{\sqrt{\sum_{i=1}^n \left(X_i-\bar X \right)^2}\sqrt{\sum_{i=1}^n \left(Y_i-\bar Y \right)^2}}
\end{align*}
and it can be seen that $R^2=r_{xy}^2$. Interestingly, it also holds that $R^2=r^2_{y\hat y}$, that is, *the square of the sample correlation coefficient between $Y_1,\ldots,Y_n$ and $\hat Y_1,\ldots,\hat Y_n$ is $R^2$*, a fact that is not immediately evident. This can be easily by first noting that
\begin{align}
\hat Y_i=\hat\beta_0+\hat\beta_1X_i=(\bar Y-\hat\beta_1X_i)+\hat\beta_1X_i=\bar Y+\hat\beta_1(X_i-\bar X) (\#eq:yhat)
\end{align}
and then replacing \@ref(eq:yhat) into
\begin{align*}
r^2_{y\hat y}=\frac{s_{y\hat y}^2}{s_y^2s_{\hat y}^2}=\frac{\left(\sum_{i=1}^n \left(Y_i-\bar Y \right)\left(\hat Y_i-\bar Y \right)\right)^2}{\sum_{i=1}^n \left(Y_i-\bar Y \right)^2\sum_{i=1}^n \left(\hat Y_i-\bar Y \right)^2}=\frac{\left(\sum_{i=1}^n \left(Y_i-\bar Y \right)\left(\bar Y+\hat\beta_1(X_i-\bar X)-\bar Y \right)\right)^2}{\sum_{i=1}^n \left(Y_i-\bar Y \right)^2\sum_{i=1}^n \left(\bar Y+\hat\beta_1(X_i-\bar X)-\bar Y \right)^2}=r^2_{xy}.
\end{align*}

```{block, type = 'rmdinsight'}
The equality $R^2=r^2_{y \hat y}$ is still true for the **multiple** linear regression, e.g. $Y=\beta_0+\beta_1X_1+\beta_2X_2+\varepsilon$. On the contrary, there is no coefficient of correlation between three or more variables, so $r_{x_1x_2y}$ does not exist. Hence, $R^2=r^2_{x y}$ is a specific fact for **simple** linear regression.
```

The result $R^2=r^2_{xy}=r^2_{y\hat y}$ can be checked numerically and graphically with the next code.
```{r, fig.show = 'hold', collapse = TRUE, cache = TRUE}
# Responses generated following a linear model
set.seed(343567) # Fixes seed, allows to generate the same random data
x <- rnorm(50)
eps <- rnorm(50)
y <- -1 + 2 * x + eps

# Regression model
reg <- lm(y ~ x)
yHat <- reg$fitted.values

# Summary
summary(reg)

# Square of the correlation coefficient
cor(y, x)^2
cor(y, yHat)^2

# Plots
scatterplot(y ~ x, smooth = FALSE)
scatterplot(y ~ yHat, smooth = FALSE)
```

We conclude this section by pointing out two common sins regarding the use of $R^2$. First, recall two important concepts regarding the application of *any* regression model in practice, in particular the linear model:

1. **Correctness**. The linear model is built on certain assumptions, such as the ones we saw in Section \@ref(assumptions). **All the inferential results are based on these assumptions being true!**^[If the assumptions are not satisfied (mismatch between what is assumed to happen in theory and what the data is), then the inference results may be misleading.]. A model is formally *correct* whenever the assumptions on which is based are not violated in the data.

2. **Usefulness**. The usefulness of the model is a more subjective concept, but is usually measured by the accuracy on the prediction and explanation of the response $Y$ by the predictor $X$. For example, $Y=0X+\varepsilon$ is a valid linear model, but is completely useless for predicting $Y$ from $X$.

Figure \@ref(fig:anovaillus) show a fitted regression line to a small dataset, for various levels of $\sigma^2$. All the linear models are *correct* by construction, but the ones with a larger $R^2$ are more *useful* for predicting/explaining $Y$ from $X$, since this is done in a more precise way.

```{block, type = 'rmdinsight'}
$R^2$ does not measure the correctness of a linear model but its **usefulness** (for prediction, for *explaining the variance* of $Y$), assuming the model is correct.
```

Trusting blindly the $R^2$ can lead to catastrophic conclusions, since the model may not be correct. Here is a counterexample of a linear regression performed in a data that clearly does not satisfy the assumptions discussed in Section \@ref(assumptions), but despite so it has a large $R^2$. Recall how biased will be the predictions for $x=0.35$ and $x=0.65$!

```{r, collapse = TRUE, cache = TRUE}
# Create data that:
# 1) does not follow a linear model
# 2) the error is heteroskedastic
x <- seq(0.15, 1, l = 100)
set.seed(123456)
eps <- rnorm(n = 100, sd = 0.25 * x^2)
y <- 1 - 2 * x * (1 + 0.25 * sin(4 * pi * x)) + eps

# Great R^2!?
reg <- lm(y ~ x)
summary(reg)

# But prediction is obviously problematic
scatterplot(y ~ x, smooth = FALSE)
```

So remember:
```{block, type = 'rmdinsight'}
A large $R^2$ means *nothing* if the **assumptions of the model do not hold**. $R^2$ is the proportion of variance of $Y$ explained by $X$, but, of course, *only when the linear model is correct*.
```

## Nonlinear relationships {#nonlin}

The linear model is termed *linear* not because the regression curve is a line, but because **the effects of the parameters $\beta_0$ and $\beta_1$ are linear**. Indeed, the predictor $X$ may exhibit a nonlinear effect on the response $Y$ and still be a linear model! For example, the following models can be transformed into simple linear models:

  1. $Y=\beta_0+\beta_1X^2+\varepsilon$
  2. $Y=\beta_0+\beta_1\log(X)+\varepsilon$
  3. $Y=\beta_0+\beta_1(X^3-\log(|X|) + 2^X)+\varepsilon$

The trick is to work with the transformed predictor ($X^2$, $\log(X)$, ...), instead of with the original variable $X$. Then, rather than working with the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$, we consider the transformed sample $(\tilde X_1,Y_1),\ldots,(\tilde X_n,Y_n)$ with (for the above examples):

  1. $\tilde X_i=X_i^2$, $i=1,\ldots,n$.
  2. $\tilde X_i=\log(X_i)$, $i=1,\ldots,n$.
  3. $\tilde X_i=X_i^3-\log(|X_i|) + 2^{X_i}$, $i=1,\ldots,n$.

```{r, nonlineartransf, echo = FALSE, warning = FALSE, results = 'hide', out.width = '45%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Some common nonlinear transformations and their negative counterparts. Recall the domain of definition of each transformation.', cache = TRUE}
x <- seq(-2, 5, l = 200)
plot(x, x, xlab = "x", ylab = "y", type = "l", col = 1, lwd = 2)
lines(x, x^2, col = 2, lwd = 2)
lines(x, x^3, col = 3, lwd = 2)
lines(x, sqrt(x), col = 4, lwd = 2)
lines(x, exp(x), col = 5, lwd = 2)
lines(x, exp(-x), col = 6, lwd = 2)
lines(x, log(x), col = 7, lwd = 2)
legend("bottomright", legend = expression(y == x, y == x^2, y == x^3, y == sqrt(x), y == exp(x), y == exp(-x), y == log(x)), lwd = 2, col = 1:7)
plot(x, -x, xlab = "x", ylab = "y", type = "l", col = 1, lwd = 2)
lines(x, -x^2, col = 2, lwd = 2)
lines(x, -x^3, col = 3, lwd = 2)
lines(x, -sqrt(x), col = 4, lwd = 2)
lines(x, -exp(x), col = 5, lwd = 2)
lines(x, -exp(-x), col = 6, lwd = 2)
lines(x, -log(x), col = 7, lwd = 2)
legend("topright", legend = expression(y == -x, y == -x^2, y == -x^3, y == -sqrt(x), y == -exp(-x), y == -exp(x), y == -log(x)), lwd = 2, col = 1:7)
```

An example of this simple but powerful trick is given as follows. The left panel of Figure \@ref(fig:quadratic) shows the scatterplot for some data `y` and  `x`, together with its fitted regression line. Clearly, the data does not follow a linear pattern, but a nonlinear one. In order to identify which one might be, we compare it against the set of mathematical functions displayed in Figure \@ref(fig:nonlineartransf). We see that the shape of the point cloud is similar to $y=x^2$. Hence, `y` might be better explained by the *square* of `x`, `x^2`, rather than by `x`. Indeed, if we plot `y` against `x^2` in the right panel of Figure \@ref(fig:quadratic), we can see that the fit of the regression line is much better.

In conclusion, with a simple trick we have increased drastically the explanation of the response. However, there is a catch: knowing which transformation is required in order to linearise the relation between response and the predictor is a kind of art which requires some good eye. This is partially alleviated by the extension of this technique to deal with *polynomials* rather than *monomials*, as we will see in Chapter \@ref(mult). For the moment, we will consider only the transformations displayed in Figure \@ref(fig:nonlineartransf). Figure \@ref(fig:transf) shows different transformations linearising nonlinear data patterns.

```{r, quadratic, echo = FALSE, out.width = '45%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Left: quadratic pattern when plotting $Y$ against $X$. Right: linearized pattern when plotting $Y$ against $X^2$.', cache = TRUE}
set.seed(345607)
x <- round(seq(-2, 5, l = 50), 1)
y <- round(0.5 * x^2 + rnorm(50), 1)
mod1 <- lm(y ~ x)
plot(x, y, pch = 16)
abline(mod1$coefficients, col = 2, lwd = 2)
mod2 <- lm(y ~ I(x^2))
plot(x^2, y, xlab = "x^2", pch = 16)
abline(mod2$coefficients, col = 2, lwd = 2)
```

```{block2, type = 'rmdinsight'}
If you apply a nonlinear transformation, namely $f$, and fit the linear model $Y=\beta_0+\beta_1 f(X)+\varepsilon$, then there is no point in fit also the model resulting from the negative transformation $-f$. The model with $-f$ is exactly the same as the one with $f$ but with the sign of $\beta_1$ flipped!

As a rule of thumb, use Figure \@ref(fig:nonlineartransf) with the transformations to compare it with the data pattern, then choose the most similar curve, and finally apply the corresponding function with **positive sign**.
```

```{r, transf, echo = FALSE, fig.cap = 'Illustration of the choice of the nonlinear transformation.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/non-linear/', height = '550px')
```

```{block2, type = 'rmdinsight'}
As you might have realized, applying nonlinear transformations to the predictors is a **simple trick that extends enormously the functionality of the linear model**. This is particular useful in real applications, where linearity is hardly verified (For example, in the PISA case study of Section \@ref(pisa), we employed
`logGDPp` instead of `GDPp` due to its higher linearity with `MathMean`.)
```

Let's see how we can compute transformations of our predictors and perform a linear regression with them. The data for the above example is the following:
```{r, cache = TRUE}
# Data
x <- c(-2, -1.9, -1.7, -1.6, -1.4, -1.3, -1.1, -1, -0.9, -0.7, -0.6,
       -0.4, -0.3, -0.1, 0, 0.1, 0.3, 0.4, 0.6, 0.7, 0.9, 1, 1.1, 1.3,
       1.4, 1.6, 1.7, 1.9, 2, 2.1, 2.3, 2.4, 2.6, 2.7, 2.9, 3, 3.1,
       3.3, 3.4, 3.6, 3.7, 3.9, 4, 4.1, 4.3, 4.4, 4.6, 4.7, 4.9, 5)
y <- c(1.4, 0.4, 2.4, 1.7, 2.4, 0, 0.3, -1, 1.3, 0.2, -0.7, 1.2, -0.1,
       -1.2, -0.1, 1, -1.1, -0.9, 0.1, 0.8, 0, 1.7, 0.3, 0.8, 1.2, 1.1,
       2.5, 1.5, 2, 3.8, 2.4, 2.9, 2.7, 4.2, 5.8, 4.7, 5.3, 4.9, 5.1,
       6.3, 8.6, 8.1, 7.1, 7.9, 8.4, 9.2, 12, 10.5, 8.7, 13.5)

# Data frame (a matrix with column names)
nonLinear <- data.frame(x = x, y = y)
```
In order to perform a simple linear regression in `x^2`, and not in `x`, we need to compute a new variable in our dataset that contains the square of `x`. We can do it in two equivalent ways:

1. Through `R Commander`. In Section \@ref(euus) we saw how to create a new variable in our active dataset (remember Figure \@ref(fig:newvar)). Go to `'Data' -> 'Manage variables in active dataset...' -> 'Compute new variable...'`. Set the `'New variable name'` to `x2` and the `'Expression to compute'` to `x^2`.

2. Through `R`. Just type:

    ```{r, collapse = TRUE, cache = TRUE}
    # We create a new column inside nonLinear, called x2, that contains
    # nonLinear$x^2
    nonLinear$x2 <- nonLinear$x^2

    # Check the variables
    names(nonLinear)
    ```

With either the two previous points you will have a new variable called `x2`. If you wish to remove it, you can do it by either typing
```{r, eval = FALSE, cache = TRUE}
# Empties the column named x2
nonLinear$x2 <- NULL
```
or, in `R Commander`, by going to `'Data' -> 'Manage variables in active data set' -> 'Delete variables from data set...'` and select to remove  `x2`.

Now we are ready to perform the regression. If you do it directly through `R`, you will obtain:
```{r, collapse = TRUE, cache = TRUE}
mod1 <- lm(y ~ x, data = nonLinear)
summary(mod1)
mod2 <- lm(y ~ x2, data = nonLinear)
summary(mod2)
```

```{block, type = 'rmdtip'}
A fast way of performing and summarizing the quadratic fit is

    summary(lm(y ~ I(x^2), data = nonLinear))

Some remarks about this expression:

  - The `I()` function wrapping `x^2` is fundamental when applying arithmetic operations in the predictor. The symbols `+`, `\*`, `^`, ... have **different meaning** when inputted in a formula, so is required to use `I()` to indicate that they must be interpreted in their arithmetic meaning and that the result of the expression denotes a new predictor variable. For example, use `I((x - 1)^3 - log(3 \* x))` if you want to apply the transformation `(x - 1)^3 - log(3 * x)`.

  - We are computing the `summary` of `lm` directly, without using an intermediate variable for storing the output of `lm`. This is perfectly valid and very handy, but note that you will not be able to access the information outputted by `lm`, only the one from `summary`.

```

```{block2, type = 'rmdexercise'}
Load the dataset `assumptions.RData`. We are going to work with the regressions `y2 ~ x2`, `y3 ~ x3`, `y8 ~ x8` and `y9 ~ x9`, in order to identify which transformation of Figure \@ref(fig:nonlineartransf) gives the best fit. (For the purpose of illustration, we do not care if the assumptions are respected.) For these, do the following:

- Find the transformation that yields the largest $R^2$.
- Compare the original and the transformed linear models.

Some hints:

- `y2 ~ x2` has a negative dependence, so look at the right panel of the transformations figure.
- `y3 ~ x3` seems to have just a subtle nonlinearity... Will it be worth to attempt a transformation?
- For `y9 ~ x9`, try with also with `exp(-abs(x9))`, `log(abs(x9))` and `2^abs(x9)`. (`abs` computes the absolute value.)

```

## Exercises and case studies

### Data importation

```{block, type = 'rmdexercise'}
Import the following datasets into `R Commander` indicating the right formatting options:

- [`iris.txt`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/iris.txt)
- [`ads.csv`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/ads.csv)
- [`auto.txt`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/auto.txt)
- [`Boston.xlsx`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/Boston.xlsx)
- [`anscombe.RData`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/anscombe.RData)
- [`airquality.txt`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/airquality.txt)
- [`wine.csv`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/wine.csv)
- [`world-population.RData`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/world-population.RData)
- [`la-liga-2015-2016.xlsx`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/la-liga-2015-2016.xlsx)
- [`wdi-countries.txt`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/wdi-countries.txt)

Create the file `datasets.RData` for saving all the datasets together.
```

```{block, type = 'rmdtip'}
There are a good number of datasets directly available in `R`:

- To **inspect them**, go to `'Data' -> 'Data in packages' -> 'List datasets in packages'` and you will get a long list of the available datasets, the package where they are stored and a small description about them. Or, in `R`, simply type `data()`.

- To **load a dataset**, go to `'Data' -> 'Data in packages' -> 'Read dataset from an attached package...'` and select the package and dataset. Or, in `R`, simply type `data(nameDataset, package = "namePackage")`.

- To **get help** on the dataset, go to `'Help' -> 'Help on active dataset (if available)'` or simply type `help(nameDataset)`.

As you can see, this is a handy way of accessing a good number of datasets directly from `R`.
```

```{block, type = 'rmdexercise'}
Import the datasets `Titanic` (`datasets` package), `PublicSchools` (`sandwich` package) and `USArrests` (`datasets` package). Describe briefly the characteristics of each dataset (dimensions, variables, context).
```


### Simple data management

```{block, type = 'rmdexercise'}
Perform the following data management operations. Remember to select the adequate dataset as the active one:

- Load `datasets.RData`.
- Establish the case names in `la-liga-2015-2016` as the variable `Team` (if they were not set).
- Establish the case names in `wdi-countries` as the variable `Country`.
- In `la-liga-2015-2016`, create a new variable named `Goals.wrt.mean`, defined as `Goals - mean(Goals)`.
- In `wdi-countries`, create a new variable that standardizes^[The standardization of $X_1,\ldots,X_n$ is $Z_1,\ldots,Z_n$, with $Z_i=\frac{X_i-\bar X}{s_x}$, where $s_x$ is the standard deviation of $X_1,\ldots,X_n$] the `GDP.growth`. Call it `GDP.growth`.
- Delete the variable `Species` from `iris`.
- For `la-liga-2015-2016`, go to `'Edit dataset'` and change the `Points` of `Getafe` to `40`. To do so, click on the cell, change the content and **click OK** or select a new cell to **save changes**. Do not hit `'Enter`' or you will add a new column!.
- Explore the menu options of `'Edit dataset'`for adding and removing rows/columns. Is a useful feature for simple edits.
- Create a `newDatasets.RData` file saving all the modified datasets.
- Restart `R Commander` and then load `newDatasets.RData` (ignore the error `'ERROR: There is more than one object in the file...'` and check that all the datasets are indeed available).
```


### Computing simple linear regressions

```{block, type = 'rmdexercise'}
Import the `iris` dataset, either from [`iris.txt`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/iris.txt) or `datasets.RData`. This dataset contains measurements for 150 iris flowers. The purpose of this exercise is to do the following analyses through `R Commander` *while inspecting and understanding the outputed code* to identify what parts are changing, how and why.

- Fit the regression line for `Petal.Width` (response) on `Petal.Length` (predictor) and summarize it.
- Make the scatterplot of `Petal.Width` (y) vs `Petal.Length` (x) with a regression line.
- Set the `'Graph title'` to "iris dataset: petal width vs petal length", the `'x-axis label'` to "petal length" and `'y-axis label'` to "sepal length".
- Identify the 5 most outlier points `'Automatically'`.
- Redo the linear regression and scatterplot excluding the points labelled as outliers (exclude them in `'Subset expression'` with a `-c(...)`).
- Check that the summary for the fitted line and the scatterplot displayed are coherent.
- Make the matrix scatterplot for the four variables and including `'Least-squares lines'`.
- Set the `'On Diagonal'` plots to `'Histograms'` and `'Boxplots'`.
- Set the `'Graph title'` to "iris matrix scatterplot".
- Identify the 5 most outlier points `'Automatically'`.
- Modify the *code* to identify 15 points.
- Compute the regression line for the plot in the third row and the fourth column and create the scatterplot for it.
- Redo the scatterplot by selecting the option `'Plot by groups...'` and then selecting `'Species'`.
```

```{block, type = 'rmdinsight'}
The last scatterplots are an illustration of the **Simpson's paradox**. The paradox surges when there are two or more well-defined groups in the data, they all have *positive (negative) correlation*, but taken as a *whole dataset*, the **correlation is the opposite**.
```

### `R` basics

```{block, type = 'rmdexercise'}
Answer briefly in your own words:

- What is the operator `<-` doing? What does it mean, for example, that `a <- 1:5`?
- What is the difference between a matrix and a data frame? Why is the latter useful?
- What are the differences between a vector and a matrix?
- What is `c` employed for?
- Consider the expression `lm(a ~ b, data = myData)`.
    - What is `lm` standing for?
    - What does it mean `a ~ b`? What are the roles of `a` and `b`?
    - Is `myData` a matrix or a data frame?
    - What must be the relation between `myData` and `a` and `b`?
    - Explain the differences with `lm(b ~ a, data = myData)`.
- What are the differences between running `a <- 1; a`, `a <- 1` and `1`.
- What are the differences between a list and a data frame? What are their common parts?
- Why is `\$` employed? How can you know in which variables you can use `$`?
- If you have a vector `x`, what are `x^2` and `x + 1` doing to its elements?
```

```{block2, type = 'rmdexercise'}
Do the following:

- Create the vectors $x = (1.17, 0.41, 0.34, 1.11, 1.02, 0.22, -0.24, -0.27, -0.40, -1.38)$ and $y = (3.63, 1.69, 0.27, 5.83, 2.64, 1.33, 1.22 -0.62, 1.29, -0.43)$.
- Set the positions 3, 4 and 8 of $x$ to 0. Set the positions 1, 4, 9 of $y$ to 0.5, -0.75 and 0.3, respectively.
- Create a new vector $z$ containing $\log(x^2) - y^3\sqrt{\exp(x)}$.
- Create the vector $t=(1, 4, 9, 16, 25, \ldots, 100)$.
- Access all the elements of $t$ except the third and fifth.
- Create the matrix $A=\begin{pmatrix}1 & -3\\0 & 2\end{pmatrix}$. Hint: use `rbind` or `cbind`.
- Using $A$, what is a short way (less amount of code) of computing $B=\begin{pmatrix}1+\sqrt{2}\sin(3) & -3+\sqrt{2}\sin(3)\\0+\sqrt{2}\sin(3) & 2+\sqrt{2}\sin(3)\end{pmatrix}$?
- Compute `A*B`. Check that it makes sense with the results of `A[1, 1] * B[1, 1]`, `A[1, 2] * B[1, 2]`, `A[2, 1] * B[2, 1]` and `A[2, 2] * B[2, 2]`. Why?
- Create a data frame named `worldPopulation` such that:
    - the first variable is called `Year` and contains the values `c(1915, 1925, 1935, 1945, 1955, 1965, 1975, 1985, 1995, 2005, 2015)`.
    - the second variable is called `Population` and contains the values `c(1798.0, 1952.3, 2197.3, 2366.6, 2758.3, 3322.5, 4061.4, 4852.5, 5735.1, 6519.6, 7349.5)`.
- Write `names(worldPopulation)`. Access to the two variables.
- Create a new variable in `worldPopulation` called `logPopulation` that contains `log(Population)`.
- Compute the standard deviation, mean and median of the variables in  `worldPopulation`.
- Regress `logPopulation` into `Year`. Save the result as `mod`.
- Compute the summary of the model and save it as `sumMod`.
- Do a `str` on `A`, `worldPopulation`, `mod` and `sumMod`.
- Access the $R^2$ and $\hat\sigma$ in `sumMod`.
- Check that $R^2$ is the same as the squared correlation between predictor and response, and also the squared correlation between response and `mod$fitted.values`.

```

### Model formulation and estimation

```{block, type = 'rmdexercise'}
Answer the following conceptual questions in your own words:

- What is the difference between $(\beta_0,\beta_1)$ and $(\hat\beta_0,\hat\beta_1)$.
- Is $\hat\beta_0$ a random variable? What about $\hat\beta_1$? Justify your answer.
- What function are the least squares estimates minimizing? Is important the choice of the kind distances (horizontal, vertical, perpendicular)?
- What is the justification for the use of a vertical distance in the RSS?
- Is $\sigma^2$ affecting the $R^2$ (indirectly or directly)? Why?
- What are the estimated residuals? What is their interpretation?
- What are the fitted values? What is their interpretation?
- What is the relation of $\hat \beta_1$ with $r_{xy}$?

Finally, check that the regression line goes through $(\bar X, \bar Y)$, in other words, that $\bar Y=\hat\beta_0+\hat\beta_1\bar X$.
```

### Assumptions of the linear model

```{block, type = 'rmdexercise'}
The dataset `moreAssumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/moreAssumptions.RData)) contains the variables `x1`, ..., `x9` and `y1`, ..., `y9`. For each regression `y1 ~ x1`, ..., `y9 ~ x9` describe whether the assumptions of the linear model are being satisfied or not. Justify your answer and state which assumption(s) you think are violated.
```

### Inference

```{block, type = 'rmdexercise'}
TBA
```

### Prediction

```{block, type = 'rmdexercise'}
TBA
```

### ANOVA and model fit

```{block, type = 'rmdexercise'}
TBA
```

### Nonlinear relations

```{block2, type = 'rmdexercise'}
Load `moreAssumptions`. For the regressions `y1 ~ x1`, `y2 ~ x2`, `y6 ~ x6` and `y9 ~ x9`, identify which nonlinear transformation yields the largest $R^2$. For that transformation, check wether the assumptions are verified.

Hints: use the transformations in Figure \@ref(fig:nonlineartransf) for the three first regressions. For `y9 ~ x9`, try with `(5 - x9)^2`, `abs(x9 - 5)` and `abs(x9 - 5)^3`.
```

### Case study: *Moore's law*

*Moore's law* [@Moore1965] is an empirical law that states that the power of a computer doubles approximately every two years. More precisely:

> Moore's law is the observation that the number of transistors in a dense integrated circuit [e.g. a CPU] doubles approximately every two years.
>
> --- Wikipedia article on [Moore's law](https://en.wikipedia.org/wiki/Moore%27s_law)

Translated into a mathematical formula, Moore's law is
\begin{align*}
\text{transistors}\approx 2^{\text{years}/2}.
\end{align*}
Applying logarithms to both sides gives (why?)
\begin{align*}
\log(\text{transistors})\approx \frac{\log(2)}{2}\text{years}.
\end{align*}
We can write the above formula more generally
\begin{align*}
\log(\text{transistors})=\beta_0+\beta_1 \text{years}+\varepsilon,
\end{align*}
where $\varepsilon$ is a random error. This is a linear model!
```{block, type = 'rmdexercise'}
The dataset `cpus.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/cpus.txt)) contains the transistor counts for the CPUs appeared in the time range 1971--2015. For this data, do the following:

- Import conveniently the data and name it as `cpus`.
- Show a scatterplot of `Transistor.count` vs `Date.of.introduction` with a linear regression.
- Are the assumptions verified in `Transistor.count ~ Date.of.introduction`? Which ones are which are more "problematic"?
- Create a new variable, named `Log.Transistor.count`, containing the logarithm of `Transistor.count`.
- Show a scatterplot of `Log.Transistor.count` vs `Date.of.introduction`  with a linear regression.
- Are the assumptions verified in `Log.Transistor.count ~ Date.of.introduction`? Which ones are which are more "problematic"?
- Regress `Log.Transistor.count ~ Date.of.introduction`.
- Summarize the fit. What are the estimates $\hat\beta_0$ and $\hat\beta_1$? Is $\hat\beta_1$ close to $\frac{\log(2)}{2}$?
- Compute the CI for $\beta_1$ at $\alpha=0.05$. Is $\frac{\log(2)}{2}$ inside it? What happens at levels $\alpha=0.10,0.01$?
- We want to forecast the average log-number of transistors for the CPUs to be released in 2017. Compute the adequate prediction and CI.
- A new CPU design is expected for 2017. What is the range of log-number of transistors expected for it, at a 95% level of confidence?
- Compute the ANOVA table for `Log.Transistor.count ~ Date.of.introduction`. Is $\beta_1$ significative?
```

```{block, type = 'rmdexercise'}
The dataset `gpus.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/gpus.txt)) contains the transistor counts for the GPUs appeared in the period 1997--2016. Repeat the previous analysis for this dataset.
```

### Case study: *Growth in a time of debt*

In the aftermath of the 2007-2008 financial crisis, the paper *Growth in a time of debt* [@Reinhart2010a], from Carmen M. Reinhart and Kenneth Rogoff (both at Harvard), provided an important economical support for pro-austerity policies. The paper claimed that for levels of external debt in excess of 90% of the GDP, the GDP growth of a country was dramatically different than for lower levels of external debt. Therefore, it concludes the existence of a *magical threshold* -- 90% -- for which the level of external debt must be kept below in order to have a growing economy. Figure \@ref(fig:reinhart), extracted from @Reinhart2010a, illustrates the main finding.

```{r, reinhart, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'The *magical threshold* of $90\\%$ external debt.', cache = TRUE}
knitr::include_graphics("images/figures/reinhart.png")
```

@Herndon2013 replicated the analysis of @Reinhart2010a and found that "selective exclusion of available data, coding errors and inappropriate weighting of summary statistics lead to serious miscalculations that inaccurately represent the relationship between public debt and GDP growth among 20 advanced economies". The authors concluded that "both mean and median GDP growth when public debt levels exceed 90% of GDP are not dramatically different from when the public debt/GDP ratios are lower". As a consequence, @Reinhart2010a led to an unjustified support for the adoption of austerity policies for countries with various levels of public debt.

You can read the full story at [BBC](http://www.bbc.com/news/magazine-22223190),
[The New York Times](http://www.nytimes.com/2013/04/19/opinion/krugman-the-excel-depression.html) and
[The Economist](http://www.economist.com/news/finance-and-economics/21576362-seminal-analysis-relationship-between-debt-and-growth-comes-under). Also, the video in Figure \@ref(fig:video) contains a quick summary of the story by Nobel Prize laureate Paul Krugman.

```{r, video, echo = FALSE, fig.align = 'center', fig.cap = 'CNN interview to Paul Krugman (key point from 1:16 to 1:40), broadcasted in 2013.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE}
knitr::include_url("https://www.youtube.com/embed/3xj338ACri8")
```

@Herndon2013 made the data of the study publicly available. You can download it [here](http://www.peri.umass.edu/236/hash/31e2ff374b6377b2ddec04deaa6388b1/publication/566/).

```{block, type = 'rmdexercise'}
The dataset `hap.txt` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/hap.txt)) contains data for 20 advanced economies in the time period 1946–2009, and is the data source for the papers aforementioned. The variable `dRGDP` represents the real GDP growth (as a percentage) and `debtgdp` represents the percentage of public debt with respect to the GDP.

- Import the data and save it as `hap`.
- Set the case names of `hap` as `Country.Year`.
- Summarize `dRGDP` and `debtgdp`. What are their minimum and maximum values?
- What is the correlation between `dRGDP` and `debtgdp`? What is the standard deviation of each variable?
- Show the scatterplot of `dRGDP` vs `debtgdp` with the regression line. Is this coherent with what was stated in the video at 1:30?
- Do you see any gap on the data around 90%? Is there any substantial change for `dRGDP` around there?
- Compute the linear regression of `dRGDP` on `debtgdp` and summarize the fit.
- What are the fitted coefficients? What are their standard errors? What is the $R^2$?
- Compute the ANOVA table. How many degrees of freedom are? What is the SSR? What is SSE? What is the $p$-value for $H_0:\beta_1=0$?
- Is SSR larger than SSE? Is this coherent with the resulting $R^2$?
- Are $\beta_0$ and $\beta_1$ significant for the regression at level $\alpha=0.05$? And at level $\alpha=0.10,0.01$?
- Compute the CIs for the coefficients. Can we conclude that the effect of `debtgdp` on `dRGDP` is positive at $\alpha=0.05$? And negative?
- Predict the average growth for levels of debt of 60%, 70%, 80%, 90%, 100% and 110%. Compute the 95% CIs for all of them.
- Predict the growth for the previous levels of debt. Compute also the CI for them. Is there a marked difference on the CIs for debt levels below and above 90%?
- Which assumptions of the linear model you think are satisfied? Should we trust blindly the inferential results obtained assuming that the assumptions were satisfied?
```

<!--chapter:end:01-simp-lin-reg.Rmd-->


# Multiple linear regression {#mult}

The multiple linear regression is an *extension* of the simple linear regression saw in Chapter \@ref(simp). If the simple linear regression employed a *single* predictor $X$ to explain the response $Y$, the multiple linear regression employs *multiple* predictors $X_1,\ldots,X_k$ for explaining a single response $Y$:
\[
Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\beta_kX_k+\varepsilon
\]
To convince you why is useful, let's begin by seeing what it can deliver in real-case scenarios!

## Examples and applications

### Case study I: *The Bordeaux equation* {#wine}

> Calculate the winter rain and the harvest rain (in millimeters). Add summer heat in the vineyard (in degrees centigrade). Subtract 12.145. And what do you have? A very, very passionate argument over wine.
>
> --- "Wine Equation Puts Some Noses Out of Joint", [The New York Times](http://www.nytimes.com/1990/03/04/us/wine-equation-puts-some-noses-out-of-joint.html), 04/03/1990

This case study is motivated by the study of Princeton professor Orley Ashenfelter [@Ashenfelter1995] on the quality of red Bordeaux vintages. The study became mainstream after disputes with the wine press, especially with Robert Parker, Jr., one of the most influential wine critic in America. See a short review of the story at the [Financial Times](http://www.ft.com/cms/s/0/1e9cb152-5824-11dc-8c65-0000779fd2ac.html) ([Google's cache](https://webcache.googleusercontent.com/search?q=cache:1mRF68v_Uz4J:https://www.ft.com/content/1e9cb152-5824-11dc-8c65-0000779fd2ac)) and at the video in Figure \@ref(fig:video2).

Red Bordeaux wines have been produced in Bordeaux, one of most famous and prolific wine regions in the world, in a very similar way for hundreds of years. However, *the quality of vintages is largely variable* from one season to another due to a long list of random factors, such as the weather conditions. Because Bordeaux wines taste better when they are older (young wines are astringent, when the wines age they lose their astringency),  there is an incentive to store the young wines until they are mature. Due to the important difference in taste, it is hard to determine the quality of the wine when it is so young just by tasting it, because it is going to change substantially when the aged wine is in the market. Therefore, being able to *predict the quality of a vintage* is a valuable information for investing resources, for determining a fair price for vintages and for understanding what factors are affecting the wine quality. The purpose of this case study is to answer:

- Q1. *Can we predict the quality of a vintage effectively?*
- Q2. *What is the interpretation of such prediction?*

The `wine.csv` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/wine.csv)) contains 27 red Bordeaux vintages. The data is the originally employed by @Ashenfelter1995, except for the inclusion of the variable `Year`, the exclusion of NAs and the reference price used for the wine. The original source is [here](http://www.liquidasset.com/winedata.html). Each row has the following variables:

- `Year`: year in which grapes were harvested to make wine.
- `Price`: *logarithm* of the average market price for Bordeaux vintages according to 1990--1991 auctions.^[In Ashenfelter, Ashmore, and Lalonde (1995), this variable is expressed relative to the price of the 1961 vintage, regarded as the best one ever recorded. In other words, they consider `Price - 8.4937` as the price variable.] This is a nonlinear transformation of the *response* (hence different to what we did in Section \@ref(nonlin)) made to *linearize* the response.
- `WinterRain`: winter rainfall (in mm).
- `AGST`: Average Growing Season Temperature (in Celsius degrees).
- `HarvestRain`: harvest rainfall (in mm).
- `Age`: age of the wine measured as the number of years stored in a cask.
- `FrancePop`: population of France at `Year` (in thousands).

The *quality* of the wine is quantified as the `Price`, a clever way of quantifying a qualitative measure. The data is shown in Table \@ref(tab:winetable).

```{r, winetable, echo = FALSE, out.width = '90%', fig.align = 'center', cache = TRUE}
wine <- read.csv(file = "datasets/wine.csv", header = TRUE)
knitr::kable(
  head(wine, 15),
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'First 15 rows of the `wine` dataset.'
)
row.names(wine) <- wine$Year
wine$Year <- NULL
```

Let's begin by summarizing the information in Table \@ref(tab:winetable). First, import correctly the dataset into `R Commander` and `'Set case names...'` as the variable `Year`. Let's summarize and inspect the data in two ways:

1. **Numerically**. Go to `'Statistics' -> 'Summaries' -> 'Active data set'`.

    ```{r, collapse = TRUE, cache = TRUE}
    summary(wine)
    ```
    Additionally, other summary statistics are available in `'Statistics' -> 'Summaries' -> 'Numerical summaries...'`.

2. **Graphically**. Make a scatterplot matrix with all the variables. Add the `'Least-squares lines'`, `'Histograms'` on the diagonals and choose to identify 2 points.

    ```{r, scat, collapse = TRUE, out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = 'Scatterplot matrix for `wine`.', cache = TRUE}
    scatterplotMatrix(~ Age + AGST + FrancePop + HarvestRain + Price + WinterRain,
                      reg.line = lm, smooth = FALSE, spread = FALSE, span = 0.5,
                      ellipse = FALSE, levels = c(.5, .9), id.n = 2,
                      diagonal = 'histogram', data = wine)
    ```

Recall that the objective is to **predict** `Price`. Based on the above matrix scatterplot the best we can predict `Price` by a simple linear regression seems to be with `AGST` or `HarvestRain`. Let's see which one yields the larger $R^2$.
```{r, collapse = TRUE, cache = TRUE}
modAGST <- lm(Price ~ AGST, data = wine)
summary(modAGST)

modHarvestRain <- lm(Price ~ HarvestRain, data = wine)
summary(modHarvestRain)
```
In `Price ~ AGST`, the intercept is not significant for the regression but the slope is, and  `AGST` has a positive effect on the `Price`. For `Price ~ HarvestRain`, both intercept and slope are significant and the effect is negative.

```{block, type = 'rmdexercise'}
Complete the analysis by computing the linear models `Price ~ FrancePop`, `Price ~ Age` and `Price ~ WinterRain`. Name them as `modFrancePop`, `modAge` and `modWinterRain`. Check if the intercepts and slopes are significant for the regression.
```
```{r, echo = FALSE, cache = TRUE}
modFrancePop <- lm(Price ~ FrancePop, data = wine)
modAge <- lm(Price ~ Age, data = wine)
modWinterRain <- lm(Price ~ WinterRain, data = wine)
```

If we do the simple regressions of `Price` on the remaining predictors, we obtain a table like this for the $R^2$:

| Predictor | $R^2$ |
|:----------|:------|
|`AGST`| $0.4456$ |
|`HarvestRain`| $0.2572$ |
|`FrancePop`| $0.2314$ |
|`Age`| $0.2120$ |
|`WinterRain`| $0.0181$ |

A natural question to ask is:

> Can we *combine* these simple regressions to increase both the $R^2$ and the prediction accuracy for `Price`?

The answer is yes, by means of the **multiple linear regression**. In order to make our first one, go to `'Statistics' -> 'Fit models' -> 'Linear model...'`. A window like Figure \@ref(fig:lmod) will pop-up.

```{r, lmod, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Window for performing multiple linear regression.', cache = TRUE}
knitr::include_graphics("images/screenshots/lmm.png")
```

Set the response as `Price` and add the rest of variables as predictors, in the form `Age + AGST + FrancePop + HarvestRain + WinterRain`. Note the **use of `+` for including all the predictors**. This does *not* mean that they are all summed and then the regression is done on the sum!^[If you wanted to do so, you will need the function `I()` for indicating that `+` is not including predictors in the model, but is acting as a sum operator: `Price ~ I(Age + AGST + FrancePop + HarvestRain + WinterRain)`.]. Instead of, this notation is designed to **resemble the multiple linear model**:
\begin{align*}
Y=\beta_0+\beta_1X_1+\beta_2X_2+\ldots+\beta_kX_k+\varepsilon
\end{align*}
If the model is named `modWine1`, we get the following summary when clicking in `'OK'`:
```{r, collapse = TRUE, cache = TRUE}
modWine1 <- lm(Price ~ Age + AGST + FrancePop + HarvestRain + WinterRain, data = wine)
summary(modWine1)
```
The main difference with simple linear regressions is that we have more rows on the `'Coefficients'` section, since these correspond to each of the predictors. The fitted regression is `Price` $= -2.343 + 0.013\,\times$ `Age` $+ 0.614\,\times$ `AGST` $- 0.000\,\times$ `FrancePop` $- 0.003\,\times$ `HarvestRain` $+ 0.001\,\times$ `WinterRain`
. Recall that the `'Multiple R-squared'` has almost doubled with respect to the best simple linear regression!^[The $R^2$ for the multiple linear regression $Y=\beta_0+\beta_1X_1+\ldots+\beta_kX_k+\varepsilon$ is not the sum of the $R^2$'s for the simple linear regressions $Y=\beta_0+\beta_jX_j+\varepsilon$, $j=1,\ldots,k$.] This tells us that we can explain up to $82.75\%$ of the `Price` variability by the predictors.

Note however that **many predictors are not significant** for the regression: `FrancePop`, `Age` and the intercept are not significant. This is an indication of an **excess of predictors** adding little information to the response. Note the almost perfect correlation between `FrancePop` and `Age` shown in Figure \@ref(fig:scat): one of them is not adding any extra information to explain `Price`. This complicates the model unnecessarily and, more importantly, it has the undesirable effect of making the **coefficient estimates less precise**. We opt to remove the predictor `FrancePop` from the model since it is exogenous to the wine context.

```{block, type = 'rmdtip'}
Two useful tips  about `lm`'s syntax for including/excluding predictors faster:

- `Price ~ .` -> **includes all the variables in the dataset as predictors**. It is equivalent to `Price ~ Age + AGST + FrancePop + HarvestRain + WinterRain`.
- `Price ~ . - FrancePop` -> **includes all the variables except the ones with `-` as predictors**. It is equivalent to It is equivalent to `Price ~ Age + AGST + HarvestRain + WinterRain`.
```

Then, the model without `FrancePop` is
```{r, collapse = TRUE, cache = TRUE}
modWine2 <- lm(Price ~ . - FrancePop, data = wine)
summary(modWine2)
```
All the coefficients are significant at level $\alpha=0.05$. Therefore, there is no clear redundant information. In addition, the $R^2$ is very similar to the full model, but the `'Adjusted R-squared'`, a weighting of the $R^2$ to account for the number of predictors used by the model, is slightly larger. Hence, this means that, comparatively to the number of predictors used, `modWine2` explains more variability of `Price` than `modWine1`. Later in this chapter we will see the precise meaning of the $R^2$ adjusted.

The comparison of the coefficients of both models can be done with `'Models -> Compare model coefficients...'`:
```{r, collapse = TRUE, cache = TRUE}
compareCoefs(modWine1, modWine2)
```
Note how **the coefficients for `modWine2` have smaller errors than `modWine1`**.

As a conclusion, `modWine2` is a model that explains the $82.75\%$ of the variability in a non-redundant way and with all their coefficients significant. Therefore, we have a formula for effectively explaining and predicting the quality of a vintage (answers Q1).

The interpretation of `modWine2` agrees with well-known facts in viticulture that make perfect sense (Q2):

- Higher temperatures are associated with better quality (higher priced) wine.
- Rain before the growing season is good for the wine quality, but during harvest is bad.
- The quality of the wine improves with the age.

Although these were known facts, keep in mind that the model allows to *quantify the effect of each variable on the wine quality* and provides us with a precise way of *predicting the quality of future vintages*.

```{block, type = 'rmdexercise'}
Create a new variable in `wine` named `PriceOrley`, defined as `Price - 8.4937`. Check that the model `PriceOrley ~ . - FrancePop - Price` *kind of* coincides with the formula given in the second paragraph of the [Financial Times article](http://www.ft.com/cms/s/0/1e9cb152-5824-11dc-8c65-0000779fd2ac.html) ([Google's cache](https://webcache.googleusercontent.com/search?q=cache:1mRF68v_Uz4J:https://www.ft.com/content/1e9cb152-5824-11dc-8c65-0000779fd2ac)). (There are a couple of typos in the article's formula: the `Age` term is missing and the `ACGS` coefficient has an extra zero. Emailed the author, his answer: "Thanks for the heads up on this. Ian Ayres.".)

```{r, video2, echo = FALSE, fig.align = 'center', fig.cap = 'ABC interview to Orley Ashenfelter, broadcasted in 1992.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE}
knitr::include_url("https://www.youtube.com/embed/Ec8hPHLMyzY")
```

### Case study II: Housing values in Boston {#boston}

The second case study is motivated by @Harrison1978, who proposed an *hedonic model* for determining the willingness of house buyers to pay for clean air. An hedonic model is a model that decomposes the price of an item into separate components that determine its price. For example, an hedonic model for the price of a house may decompose its price into the house characteristics, the kind of neighborhood, and the location. The study of @Harrison1978 employed data from the Boston metropolitan area, containing 560 suburbs and 14 variables. The `Boston` dataset is available through the file `Boston.xlsx` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/Boston.xlsx)) and through the dataset `Boston` in the `MASS` package (load `MASS` by `'Tools' -> 'Load package(s)...'`).

The description of the related variables can be found in `?Boston` and @Harrison1978^[But be aware of the changes in units for `medv`, `black`, `lstat` and `nox`.], but we summarize here the most important ones as they appear in `Boston`. They are aggregated into five topics:

- *Dependent* variable: `medv`, the median value of owner-occupied homes (in thousands of dollars).
- *Structural* variables indicating the house characteristics: `rm` (average number of rooms "in owner units") and `age` (proportion of owner-occupied units built prior to 1940).
- *Neighborhood* variables: `crim` (crime rate), `zn` (proportion of residential areas), `indus` (proportion of non-retail business area), `chas` (river limitation), `tax` (cost of public services in each community), `ptratio` (pupil-teacher ratio), `black` (variable $1000(B - 0.63)^2$, where $B$ is the black proportion of population -- low and high values of $B$ increase housing prices) and `lstat` (percent of lower status of the population).
- *Accesibility* variables: `dis` (distances to five Boston employment centers) and `rad` (accessibility to radial highways -- larger index denotes better accessibility).
- *Air pollution* variable: `nox`, the annual concentration of nitrogen oxide (in parts per ten million).

A summary of the data is shown below:
```{r, echo = FALSE, warning = FALSE, cache = TRUE}
library(RcmdrMisc)
Boston <- readXL("datasets/Boston.xlsx", rownames = FALSE, header = TRUE, na = "",
                 sheet = "Hoja1", stringsAsFactors = TRUE)
```
```{r, collapse = TRUE, cache = TRUE}
summary(Boston)
```

The two goals of this case study are:

- Q1. *Quantify the influence of the predictor variables in the housing prices.*
- Q2. *Obtain the "best possible" model for decomposing the housing variables and interpret it.*

We begin by making an exploratory analysis of the data with a matrix scatterplot. Since the number of variables is high, we opt to plot only five variables: `crim`, `dis`, `medv`, `nox` and `rm`. Each of them represents the five topics in which variables were classified.

```{r, scat2, collapse = TRUE, out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = 'Scatterplot matrix for `crim`, `dis`, `medv`, `nox` and `rm` from the `Boston` dataset.', cache = TRUE}
scatterplotMatrix(~ crim + dis + medv + nox + rm, reg.line = lm, smooth = FALSE,
                  spread = FALSE, span = 0.5, ellipse = FALSE, levels = c(.5, .9),
                  id.n = 0, diagonal = 'density', data = Boston)
```
The diagonal panels are showing an estimate of the unknown density of each variable. Note the peculiar distribution of `crim`, very concentrated at zero, and the asymmetry in `medv`, with a second mode associated to the most expensive properties. Inspecting the individual panels, it is clear that some nonlinearity exists in the data. For simplicity, we disregard that analysis for the moment (but see the final exercise).

Let's fit a multiple linear regression for explaining `medv`. There are a good number of variables now, and some of them might be of little use for predicting `medv`. However, there is no clear intuition of which predictors will yield better explanations of `medv` with the information at hand. Therefore, we can start by doing a linear model on *all* the predictors:
```{r, collapse = TRUE, cache = TRUE}
modHouse <- lm(medv ~ ., data = Boston)
summary(modHouse)
```
There are a couple of non-significant variables, but so far the model has an $R^2=0.74$ and the fitted coefficients are sensible with what it would be expected. For example, `crim`, `tax`, `ptratio` and `nox` have negative effects on `medv`, while `rm`, `rad` and `chas` have positive. However, the non-significant coefficients are not improving significantly the model, but only adding artificial noise and decreasing the overall accuracy of the coefficient estimates!

Let's polish a little bit the previous model. Instead of removing manually each non-significant variable to reduce the complexity, we employ an automatic tool in `R` called **stepwise model selection**. It has different flavors, that we will see in detail in Section \@ref(selection), but essentially this powerful tool *usually* ends up selecting "a" *best model*: **a model that delivers the maximum fit with the minimum number of variables**.

The stepwise model selection is located at `'Models' -> 'Stepwise model selection...'` and is always applied on the active model. Apply it with the default options to `modBest`:

```{r, collapse = TRUE, cache = TRUE}
modBest <- stepwise(modHouse, direction = 'backward/forward', criterion = 'BIC')
```

Note the different steps: it starts with the full model and, when `+` is shown, it means that the variable is *excluded* at that step. The procedure seeks to minimize an **information criterion** (BIC or AIC)^[Although note that the printed messages always display `'AIC'` even if you choose `'BIC'`.]. An information criterion balances the fitness of a model with the number of predictors employed. Hence, it determines objectively the *best model*: *the one that minimizes the information criterion*. Remember to save the output to a variable if you want to have the final model (you need to do this in `R`)!

The summary of the final model is:
```{r, collapse = TRUE, cache = TRUE}
summary(modBest)
```
Let's compute the confidence intervals at level $\alpha=0.05$:
```{r, collapse = TRUE, cache = TRUE}
confint(modBest)
```
We have quantified the influence of the predictor variables in the housing prices (Q1) and we can conclude that, in the final model and with confidence level $\alpha=0.05$:

- `chas`, `age`, `rad` and `black` have a **significantly positive** influence on `medv`.
- `nox`, `dis`, `tax`, `pratio` and `lstat` have a **significantly negative** influence on `medv`.

```{block2, type = 'rmdexercise'}
The model employed in @Harrison1978 is different from `modBest`. In the paper, several nonlinear transformations of the predictors (remember Section \@ref(nonlin)) and the response are done to improve the linear fit. Also, different units are used for `medv`, `black`, `lstat` and `nox`. The authors considered these variables:

- *Response*: `log(1000 * medv)`
- *Linear predictors*: `age`, `black / 1000` (this variable corresponds to their $(B-0.63)^2$), `tax`, `ptratio`, `crim`, `zn`, `indus` and `chas`.
- *Nonlinear predictors*: `rm^2`, `log(dis)`, `log(rad)`, `log(lstat / 100)` and `(10 * nox)^2`.

Do the following:

1. Check if the model with such predictors corresponds to the one in the first column, Table VII, page 100 of @Harrison1978
(open-access paper available [here](https://deepblue.lib.umich.edu/bitstream/handle/2027.42/22636/0000186.pdf)). To do so, Save this model as `modelHarrison` and summarize it. **Hint**: the formula should be something like `I(log(1000 * medv)) ~ age + I(black / 1000) + ... + I(log(lstat / 100)) + I((10 * nox)^2)`.

2. Make a `stepwise` selection of the variables in `modelHarrison` (use defaults) and save it as `modelHarrisonSel`. Summarize it.

3. Which model has a larger $R^2$? And adjusted $R^2$? Which is simpler and has more significant coefficients?

```
<!--
lm(I(log(medv*1000)) ~ I(rm^2) + age + log(dis) + log(rad) + tax + ptratio + I(black/1000) + I(log(lstat/100)) + crim + zn + indus + chas + I((10*nox)^2), data = Boston)
-->


## Model formulation and estimation by least squares {#modelmult}

The multiple linear model extends the simple linear model by describing the relation between the random variables $X_1,\ldots,X_k$ and $Y$. For example, in the last model for the `wine` dataset, we had $k=4$ variables $X_1=$`WinterRain`, $X_2=$`AGST`, $X_3=$`HarvestRain` and $X_4=$`Age`, and $Y=$ `Price`. Therefore, as in Section \@ref(modelsimp), the multiple linear model is *constructed by assuming* that the linear relation
\begin{align}
Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k + \varepsilon (\#eq:k1)
\end{align}
holds between the predictors $X_1,\ldots,X_k$ and the response $Y$. In \@ref(eq:k1), $\beta_0$ is the *intercept* and $\beta_1,\ldots,\beta_k$ are the *slopes*, respectively. $\varepsilon$ is a random variable with mean zero and independent from $X_1,\ldots,X_n$. Another way of looking at \@ref(eq:k1) is
\begin{align}
\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k]=\beta_0+\beta_1x_1+\ldots+\beta_kx_k, (\#eq:k2)
\end{align}
since $\mathbb{E}[\varepsilon|X_1=x_1,\ldots,X_k=x_k]=0$.

The LHS of \@ref(eq:k2) is the conditional expectation of $Y$ given $X_1,\ldots,X_k$. It represents how the mean of the random variable $Y$ is changing according to particular values, denoted by $x_1,\ldots,x_k$, of the random variables $X_1,\ldots,X_k$. With the RHS, what we are saying is that the mean of $Y$ is changing in a *linear* fashion with respect to the value of $X$. Hence the interpretation of the coefficients:

- $\beta_0$: is the mean of $Y$ when $X_1=\ldots=X_k=0$.
- $\beta_j$, $1\leq j\leq k$: is the increment in mean of $Y$ for an increment of one unit in $X_j=x_j$, provided that the remaining variables $X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_k$ *do not change*.

Figure \@ref(fig:leastsquares2) illustrates the geometrical interpretation of a multiple linear model: a plane in the $(k+1)$-dimensional space. If $k=1$, the plane is the regression line for simple linear regression. If $k=2$, then the plane can be visualized in a three-dimensional plot

```{r, leastsquares2, echo = FALSE, fig.cap = 'The least squares regression plane $y=\\hat\\beta_0+\\hat\\beta_1x_1+\\hat\\beta_2x_2$ and its dependence on the kind of squared distance considered.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/least-squares-3D/', height = '700px')
```

The estimation of $\beta_0,\beta_1,\ldots,\beta_k$ is done as in simple linear regression, by minimizing the Residual Sum of Squares (RSS). First we need to introduce some helpful *matrix notation*. In the following, **bold face** are used for distinguishing vectors and matrices from scalars:

- A sample of $(X_1,\ldots,X_k,Y)$ is $(X_{11},\ldots,X_{1k},Y_1),\ldots,(X_{n1},\ldots,X_{nk},Y_n)$, where $X_{ij}$ denotes the $i$-th observation of the $j$-th predictor $X_j$. We denote with $\mathbf{X}_i=(X_{i1},\ldots,X_{ik})$ to the $i$-th observation of $(X_1,\ldots,X_k)$, so the sample simplifies to $(\mathbf{X}_{1},Y_1),\ldots,(\mathbf{X}_{n},Y_n)$

- The *design matrix* contains all the information of the predictors and a column of ones
\[
\mathbf{X}=\begin{pmatrix}
1 & X_{11} & \cdots & X_{1k}\\
\vdots & \vdots & \ddots & \vdots\\
1 & X_{n1} & \cdots & X_{nk}
\end{pmatrix}_{n\times(k+1)}
\]

- The *vector of responses* $\mathbf{Y}$, the *vector of coefficients* $\boldsymbol\beta$ and the *vector of errors* are, respectively^[The vectors are regarded as column matrices.],
\[
\mathbf{Y}=\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix}_{n\times 1},\quad\boldsymbol\beta=\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{pmatrix}_{(k+1)\times 1}\text{ and }
\boldsymbol\varepsilon=\begin{pmatrix}
\varepsilon_1 \\
\vdots \\
\varepsilon_n
\end{pmatrix}_{n\times 1}.
\]
Thanks to the matrix notation, we can turn the sample version of the multiple linear model, namely
\begin{align*}
Y_i&=\beta_0 + \beta_1 X_{i1} + \ldots +\beta_k X_{ik} + \varepsilon_i,\quad i=1,\ldots,n
\end{align*}
into something as compact as
\begin{align*}
\mathbf{Y}=\mathbf{X}\boldsymbol\beta+\boldsymbol\varepsilon.
\end{align*}

```{block2, type = 'rmdinsight'}
Recall that if $k=1$ we have the simple linear model. In this case:
\[
\mathbf{X}=\begin{pmatrix}
1 & X_{11}\\
\vdots & \vdots\\
1 & X_{n1}
\end{pmatrix}_{n\times2}\text{ and } \beta=\begin{pmatrix}
\beta_0 \\
\beta_1
\end{pmatrix}_{2\times 1}
\]
```

The RSS for the multiple linear regression is
\begin{align}
\text{RSS}(\boldsymbol\beta)&=\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{i1}-\ldots-\beta_kX_{ik})^2\nonumber\\
&=(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^T(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta}).(\#eq:rss)
\end{align}
The RSS aggregates the *squared vertical distances* from the data to a regression plane given by $\boldsymbol\beta$. Remember that the *vertical distances* are considered because we want to minimize the error in the *prediction* of $Y$. The least squares estimators are *the minimizers of the RSS*^[They are unique and always exist.]:
\begin{align*}
\hat{\boldsymbol{\beta}}=\arg\min_{\boldsymbol{\beta}\in\mathbb{R}^{k+1}} \text{RSS}(\boldsymbol{\beta}).
\end{align*}
Luckily, thanks to the matrix form of \@ref(eq:rss), it is simple to compute a closed-form expression for the least squares estimates:
\begin{align}
\hat{\boldsymbol{\beta}}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y},(\#eq:k3)
\end{align}

```{block2, type = 'rmdinsight'}
There are some similarities between \@ref(eq:k3) and $\hat\beta_1=(s_x^2)^{-1}s_{xy}$ from the simple linear model: both are related to the covariance between $\mathbf{X}$ and $Y$ weighted by the variance of $\mathbf{X}$.
```

The data of the illustration has been generated with the following code:
```{r, cache = TRUE}
# Generates 50 points from a N(0, 1): predictors and error
set.seed(34567) # Fixes the seed for the random generator
x1 <- rnorm(50)
x2 <- rnorm(50)
x3 <- x1 + rnorm(50, sd = 0.05) # Make variables dependent
eps <- rnorm(50)

# Responses
yLin <- -0.5 + 0.5 * x1 + 0.5 * x2 + eps
yQua <- -0.5 + x1^2 + 0.5 * x2 + eps
yExp <- -0.5 + 0.5 * exp(x2) + x3 + eps

# Data
leastSquares3D <- data.frame(x1 = x1, x2 = x2, yLin = yLin,
                             yQua = yQua, yExp = yExp)
```

Let's check that indeed the coefficients given by `lm` are the ones given by equation \@ref(eq:k3) for the regression `yLin ~ x1 + x2`.
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
# Matrix X
X <- cbind(1, x1, x2)

# Vector Y
Y <- yLin

# Coefficients
beta <- solve(t(X) %*% X) %*% t(X) %*% Y
# %*% multiplies matrices
# solve() computes the inverse of a matrix
# t() transposes a matrix
beta

# Output from lm
mod <- lm(yLin ~ x1 + x2, data = leastSquares3D)
mod$coefficients
```

```{block2, type = 'rmdexercise'}
Compute $\boldsymbol{\beta}$ for the regressions `yLin ~ x1 + x2`, `yQua ~ x1 + x2` and `yExp ~ x2 + x3` using:

- equation \@ref(eq:k3) and
- the function `lm`.

Check that the fitted plane and the coefficient estimates are coherent.
```

Once we have the least squares estimates $\hat{\boldsymbol{\beta}}$, we can define the next two concepts:

- The *fitted values* $\hat Y_1,\ldots,\hat Y_n$, where
\begin{align*}
\hat Y_i=\hat\beta_0+\hat\beta_1X_{i1}+\cdots+\hat\beta_kX_{ik},\quad i=1,\ldots,n.
\end{align*}
They are the vertical projections of $Y_1,\ldots,Y_n$ into the fitted line (see Figure \@ref(fig:leastsquares2)). In a matrix form, inputting \@ref(eq:rss)
\[
\hat{\mathbf{Y}}=\mathbf{X}\hat{\boldsymbol{\beta}}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}=\mathbf{H}\mathbf{Y},
\]
where $\mathbf{H}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T$ is called the *hat matrix* because it "puts the hat into $\mathbf{Y}$". What it does is to project $\mathbf{Y}$ into the regression plane (see Figure \@ref(fig:leastsquares2)).

- The *estimated residuals* $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$, where
\begin{align*}
\hat\varepsilon_i=Y_i-\hat Y_i,\quad i=1,\ldots,n.
\end{align*}
They are the vertical distances between actual data and fitted data.

We conclude with an insight on the relation of multiple and simple linear regressions. It is illustrated in Figure \@ref(fig:multmarg).

```{block, type = 'rmdinsight'}
Consider the multiple linear model $Y=\beta_0+\beta_1X_1+\beta_2X_2+\varepsilon$ and its associated simple linear models $Y=\alpha_0+\alpha_1X_1+\varepsilon$ and $Y=\gamma_0+\gamma_1X_2+\varepsilon$. Assume that we have a sample $(X_{11},X_{12},Y_1),\ldots, (X_{n1},X_{n2},Y_n)$. Then, in general, $\hat\alpha_0\neq\hat\beta_0$, $\hat\alpha_1\neq\hat\beta_1$, $\hat\gamma_0\neq\hat\beta_0$ and $\hat\gamma_1\neq\hat\beta_1$. This is, in general, **the inclusion of a new predictor changes the coefficient estimates**.
```

```{r, multmarg, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The regression plane (blue) and its relation with the simple linear regressions (green lines). The red points represent the sample for $(X_1,X_2,Y)$ and the black points the subsamples for $(X_1,X_2)$ (bottom), $(X_1,Y)$ (left) and $(X_2,Y)$ (right).', cache = TRUE}
knitr::include_graphics("images/R/multmarg.png")
```

The data employed in Figure \@ref(fig:multmarg) is:
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
set.seed(212542)
n <- 100
x1 <- rnorm(n, sd = 2)
x2 <- rnorm(n, mean = x1, sd = 3)
y <- 1 + 2 * x1 - x2 + rnorm(n, sd = 1)
data <- data.frame(x1 = x1, x2 = x2, y = y)
```

```{block, type = 'rmdexercise'}
With the above `data`, cheek how the fitted coefficients change for `y ~ x1`, `y ~ x2` and `y ~ x1 + x2`.
```

## Assumptions of the model {#assumptionsmult}

Some probabilistic assumptions are required for performing inference on the model parameters. In other words, to infer properties about the *unknown* population coefficients $\boldsymbol{\beta}$ from the sample $(\mathbf{X}_1, Y_1),\ldots,(\mathbf{X}_n, Y_n)$.

```{r, linearmodel2, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The key concepts of the multiple linear model when $k=2$. The space between the yellow planes denotes where the $95\\%$ of the data is, according to the model.', cache = TRUE}
knitr::include_graphics("images/R/linearmodel2.png")
```

The assumptions of the multiple linear model are an extension of the simple linear model:

i. **Linearity**: $\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k]=\beta_0+\beta_1x_1+\ldots+\beta_kx_k$.
ii. **Homoscedasticity**: $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$, with $\sigma^2$ constant for $i=1,\ldots,n$.
iii. **Normality**: $\varepsilon_i\sim\mathcal{N}(0,\sigma^2)$ for $i=1,\ldots,n$.
iv. **Independence of the errors**: $\varepsilon_1,\ldots,\varepsilon_n$ are independent (or uncorrelated, $\mathbb{E}[\varepsilon_i\varepsilon_j]=0$, $i\neq j$, since they are assumed to be Normal).

A good one-line summary of the linear model is the following (independence is assumed)
\begin{align}
Y|(X_1=x_1,\ldots,X_k=x_k)\sim \mathcal{N}(\beta_0+\beta_1x_1+\ldots+\beta_kx_k,\sigma^2)(\#eq:condnorm)
\end{align}

```{block2, type = 'rmdinsight'}
Recall:

- Compared with simple liner regression, the only **different assumption is linearity**.

- Nothing is said about the distribution of $X_1,\ldots,X_k$. They could be deterministic or random. They could be discrete or continuous.

- $X_1,\ldots,X_k$ are **not required to be independent** between them.

- **$Y$ has to be continuous**, since the errors are normal -- recall \@ref(eq:1).

```

Figure \@ref(fig:linearmodelassump) represent situations where the assumptions of the model are respected and violated, for the situation with two predictors. Clearly, the inspection of the scatterplots for identifying strange patterns is more complicated than in simple linear regression -- and here we are dealing only with two predictors. In Section \@ref(diagnostics) we will see more sophisticated methods for checking whether the assumptions hold or not for an arbitrary number of predictors.

```{r, linearmodelassump, echo = FALSE, fig.cap = 'Valid (all the assumptions are verified) and problematic (a single assumption does not hold) multiple linear models, when there are two predictors.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/assump-lm-3D/', height = '700px')
```

To conclude this section, let's see how to make a 3D scatterplot with the regression plane, in order to evaluate visually how good the fit of the model is. We will do it with the `iris` dataset, that can be imported in `R` simply by running `data(iris)`. In `R Commander` go to `'Graphs' -> '3D Graphs' -> '3D scatterplot...'`. A window like Figures \@ref(fig:scatter3d1) and \@ref(fig:scatter3d2) will pop-up. The options are similar to the ones for `'Graphs' -> 'Scatterplot...'`.

```{r, scatter3d1, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = '3D scatterplot window, `\'Data\'` panel.', cache = TRUE}
knitr::include_graphics("images/screenshots/scatterplot3d1.png")
```
```{r, scatter3d2, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = '3D scatterplot window, `\'Options\'` panel. Remember to tick the `\'Linear least-squares fit\'` box in order to display the fitted regression plane', cache = TRUE}
knitr::include_graphics("images/screenshots/scatterplot3d2.png")
```

If you select the options as shown in Figures \@ref(fig:scatter3d1) and \@ref(fig:scatter3d2), you should get something like this:
```{r echo = FALSE, warning = FALSE, eval = TRUE, cache = TRUE}
# include this code chunk as-is to enable 3D graphs
library(rgl)
knitr::knit_hooks$set(webgl = hook_webgl)
```
```{r, webgl = TRUE, cache = TRUE}
data(iris)
scatter3d(Petal.Length ~ Petal.Width + Sepal.Length, data = iris, fit = "linear",
          residuals = TRUE, bg = "white", axis.scales = TRUE, grid = TRUE,
          ellipsoid = FALSE, id.method = 'mahal', id.n = 2)
```

## Inference for model parameters

The assumptions introduced in the previous section allow to specify what is the distribution of the *random vector* $\hat{\boldsymbol{\beta}}$. The distribution is derived conditionally on the sample predictors $\mathbf{X}_1,\ldots,\mathbf{X}_n$. In other words, we assume that the randomness of $\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol\varepsilon$ comes only from the error terms and not from the predictors. To denote this, we employ lowercase for the sample predictors $\mathbf{x}_1,\ldots,\mathbf{x}_n$.

### Distributions of the fitted coefficients

The distribution of $\hat{\boldsymbol{\beta}}$ is:
\begin{align}
\hat{\boldsymbol{\beta}}\sim\mathcal{N}_{k+1}\left(\boldsymbol\beta,\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\right)
(\#eq:normp)
\end{align}
where $\mathcal{N}_{m}$ is the $m$-dimensional Normal, this is, the extension of the usual Normal distribution to deal with $m$ random variables^[With $m=1$, the density of a $\mathcal{N}_{m}$ corresponds to a bell-shaped *curve* With $m=2$, the density is a *surface* similar to a bell.]. The interpretation of \@ref(eq:normp) is not so easy as in the simple linear case. Here are some broad remarks:

- **Bias**. The estimates are unbiased.
- **Variance**. Depending on:

    - *Sample size $n$*. Hidden inside $\mathbf{X}^T\mathbf{X}$. As $n$ grows, the precision of the estimators increases.
    - *Error variance $\sigma^2$*. The larger $\sigma^2$ is, the less precise $\hat{\boldsymbol{\beta}}$ is.
    - *Predictor sparsity $(\mathbf{X}^T\mathbf{X})^{-1}$*. The more *sparse* the predictor is (small $|(\mathbf{X}^T\mathbf{X})^{-1}|$), the more precise $\hat{\boldsymbol{\beta}}$ is.


The problem with \@ref(eq:normp) is that *$\sigma^2$ is unknown* in practice, so we need to estimate $\sigma^2$ from the data. We do so by computing a rescaled sample variance of the fitted residuals $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$:
\begin{align*}
\hat\sigma^2=\frac{\sum_{i=1}^n\hat\varepsilon_i^2}{n-k-1}.
\end{align*}
Note the $n-k-1$ in the denominator. Now $n-k-1$ are the *degrees of freedom*, the number of data points minus the number of already fitted parameters ($k$ slopes and $1$ intercept). As in simple linear regression, *the mean of the fitted residuals $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$ is zero*.

If we use the estimate $\hat\sigma^2$ instead of $\sigma^2$, we get more useful distributions, this time for the *individual* $\beta_j$'s:
\begin{align}
\frac{\hat\beta_j-\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)}\sim t_{n-k-1},\quad\hat{\mathrm{SE}}(\hat\beta_j)^2=\hat\sigma^2v_j^2(\#eq:normp2)
\end{align}
where $t_{n-k-1}$ represents the Student's $t$ distribution with $n-k-1$ degrees of freedom and
\[
v_j\text{ is the $j$-th element of the diagonal of }(\mathbf{X}^T\mathbf{X})^{-1}.
\]
The LHS of \@ref(eq:normp2) is the $t$-statistic for $\beta_j$, $j=0,\ldots,k$. They are employed for building confidence intervals and hypothesis tests.


### Confidence intervals for the coefficients

Thanks to \@ref(eq:normp2), we can have the $100(1-\alpha)\%$ CI for the coefficient $\beta_j$, $j=0,\ldots,k$:
\begin{align}
\left(\hat\beta_j\pm\hat{\mathrm{SE}}(\hat\beta_j)t_{n-k-1;\alpha/2}\right)(\#eq:cip)
\end{align}
where $t_{n-k-1;\alpha/2}$ is the *$\alpha/2$-upper quantile of the $t_{n-k-1}$*. Note that with $k=1$ we have same CI as in \@ref(eq:ci).

Let's see how we can compute the CIs. We return to the `wine` dataset, so in case you do not have it loaded, you can download it [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/wine.RData) as an `.RData` file. We analyse the CI for the coefficients of `Price ~ Age + WinterRain`.
```{r, collapse = TRUE, cache = TRUE}
# Fit model
mod <- lm(Price ~ Age + WinterRain, data = wine)

# Confidence intervals at 95%
confint(mod)

# Confidence intervals at other levels
confint(mod, level = 0.90)
confint(mod, level = 0.99)
```
In this example, the 95% confidence interval for $\beta_0$ is $(4.7460, 7.2201)$, for $\beta_1$ is $(0.0077, 0.0644)$ and for $\beta_2$ is $(-0.0010, 0.0026)$. Therefore, we can say with a 95% confidence that *the coefficient of `WinterRain` is non significant*. But in Section \@ref(wine) we saw that it *was significant* in the model `Price ~ Age + AGST + HarvestRain + WinterRain`! How is this possible? The answer is that the presence of extra predictors affects the coefficient estimate, as we saw in Figure \@ref(fig:multmarg). Therefore, the precise statement to make is: **in the model `Price ~ Age + WinterRain`**, with $\alpha=0.05$, the coefficient of `WinterRain` is non significant. Note that this **does not** mean that it will be always non significant: in `Price ~ Age + AGST + HarvestRain + WinterRain` it is.

```{block, type = 'rmdexercise'}
Compute and interpret the CIs for the coefficients, at levels $\alpha=0.10,0.05,0.01$, for the following regressions:

- `medv ~ . - lstat - chas - zn - crim` (`Boston`)
- `nox ~ chas + zn + indus + lstat + dis + rad` (`Boston`)
- `Price ~ WinterRain + HarvestRain + AGST` (`wine`)
- `AGST ~ Year + FrancePop` (`wine`)
```


### Testing on the coefficients

The distributions in \@ref(eq:normp2) also allow to conduct a formal hypothesis test on the coefficients $\beta_j$, $j=0,\ldots,k$. For example the test for significance is specially important:
\begin{align*}
H_0:\beta_j=0
\end{align*}
for $j=0,\ldots,k$. The test of $H_0:\beta_j=0$ with $1\leq j\leq k$ is specially interesting, since it allows to answer whether *the variable $X_j$ has a significant linear effect on $Y$*. The statistic used for testing for significance is the $t$-statistic
\begin{align*}
\frac{\hat\beta_j-0}{\hat{\mathrm{SE}}(\hat\beta_j)},
\end{align*}
which is distributed as a $t_{n-k-1}$ *under the (veracity of) the null hypothesis*. $H_0$ is tested *against* the *bilateral* alternative hypothesis $H_1:\beta_j\neq 0$.

Remember two important insights regarding hypothesis testing.

```{block, type = 'rmdinsight'}
In an hypothesis test, the *$p$-value measures the degree of veracity of $H_0$ according to the data*. The rule of thumb is the following:

**Is the $p$-value lower than $\alpha$?**

- **Yes $\rightarrow$ reject $H_0$**.
- **No $\rightarrow$ do not reject $H_0$**.
```


```{block, type = 'rmdinsight'}
The connection of a $t$-test for $H_0:\beta_j=0$ and the CI for $\beta_j$, both at level $\alpha$, is the following.

**Is $0$ inside the CI for $\beta_j$?**

- **Yes $\leftrightarrow$ do not reject $H_0$**.
- **No $\leftrightarrow$ reject $H_0$**.
```

The tests for significance are built-in in the `summary` function, as we saw in Section \@ref(mult). For `mod`, the regression of `Price ~ Age + WinterRain`, we have:
```{r, collapse = TRUE, cache = TRUE}
summary(mod)
```


```{block, type = 'rmdinsight'}
The unilateral test $H_0:\beta_j\geq 0$ (respectively, $H_0:\beta_j\leq 0$) vs $H_1:\beta_j<0$ ($H_1:\beta_j>0$) can be done by means of the CI for $\beta_j$. If $H_0$ is rejected, they allow to conclude that *$\hat\beta_j$ is significantly negative (positive)* and that *for the considered regression model, $X_j$ has a significant negative (positive) effect on $Y$*. We have been doing them using the following rule of thumb:

**Is the CI for $\beta_j$ below (above) $0$ at level $\alpha$?**

- **Yes $\rightarrow$ reject $H_0$ at level $\alpha$. Conclude $X_j$ has a significant negative (positive) effect on $Y$ at level $\alpha$**.
- **No $\rightarrow$ the criterion is not conclusive**.
```


## Prediction

As in the simple linear model, the forecast of $Y$ from $\mathbf{X}=\mathbf{x}$ (this is, $X_1=x_1,\ldots,X_k=x_k$) is approached by two different ways:

1. Inference on the **conditional mean** of $Y$ given $\mathbf{X}=\mathbf{x}$, $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$. This is a deterministic quantity, which equals $\beta_0+\beta_1x_1+\ldots+\beta_{k}x_k$.
2. Prediction of the **conditional response** $Y|\mathbf{X}=\mathbf{x}$. This is a random variable distributed as $\mathcal{N}(\beta_0+\beta_1x_1+\ldots+\beta_{k}x_k,\sigma^2)$.

The prediction and computation of CIs can be done with the `R` function `predict` (unfortunately, there is no `R Commander` shortcut for this one). The objects required for `predict` are: first, the output of `lm`; second, a `data.frame` containing the locations $\mathbf{x}=(x_1,\ldots,x_k)$ where we want to predict $\beta_0+\beta_1x_1+\ldots+\beta_{k}x_k$. The prediction is $\hat\beta_0+\hat\beta_1x_1+\ldots+\hat\beta_{k}x_k$

```{block, type = 'rmdcaution'}
It is mandatory to name the columns of the data frame with the same names of the predictors used in `lm`. Otherwise `predict` will generate an error, see below.
```

To illustrate the use of `predict`, we return to the `wine` dataset.

```{r, collapse = TRUE, cache = TRUE}
# Fit a linear model for the price on WinterRain, HarvestRain and AGST
modelW <- lm(Price ~ WinterRain + HarvestRain + AGST, data = wine)
summary(modelW)

# Data for which we want a prediction
# Important! You have to name the column with the predictor name!
weather <- data.frame(WinterRain = 500, HarvestRain = 123,
                      AGST = 18)

## Prediction of the mean

# Prediction of the mean at 95% - the defaults
predict(modelW, newdata = weather)

# Prediction of the with 95% confidence interval (the default)
# CI: (lwr, upr)
predict(modelW, newdata = weather, interval = "confidence")
predict(modelW, newdata = weather, interval = "confidence", level = 0.95)

# Other levels
predict(modelW, newdata = weather, interval = "confidence", level = 0.90)
predict(modelW, newdata = weather, interval = "confidence", level = 0.99)

## Prediction of the response

# Prediction of the mean at 95% - the defaults
predict(modelW, newdata = weather)

# Prediction of the with 95% confidence interval (the default)
# CI: (lwr, upr)
predict(modelW, newdata = weather, interval = "prediction")
predict(modelW, newdata = weather, interval = "prediction", level = 0.95)

# Other levels
predict(modelW, newdata = weather, interval = "prediction", level = 0.90)
predict(modelW, newdata = weather, interval = "prediction", level = 0.99)

# Predictions for several values
weather2 <- data.frame(WinterRain = c(500, 200), HarvestRain = c(123, 200),
                       AGST = c(17, 18))
predict(modelW, newdata = weather2, interval = "prediction")
```

```{block, type = 'rmdexercise'}
For the `wine` dataset, do the following:

- Regress `WinterRain` on `HarvestRain` and `AGST`. Name the fitted model `modExercise`.
- Compute the estimate for the conditional mean of `WinterRain` for `HarvestRain`$=123.0$ and `AGST`$=16.15$. What is the CI at $\alpha=0.01$?
- Compute the estimate for the conditional response for `HarvestRain`$=125.0$ and `AGST`$=15$. What is the CI at $\alpha=0.10$?
- Check that `modExercise\$fitted.values` is the same as `predict(modExercise, newdata = data.frame(WinterRain = wine\$HarvestRain, AGST = wine\$AGST))`. Why is so?
```

```{block, type = 'rmdinsight'}
Similarities and differences in the prediction of the conditional mean $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$ and conditional response $Y|\mathbf{X}=\mathbf{x}$:

- *Similarities*. The estimate is the same, $\hat y=\hat\beta_0+\hat\beta_1x_1+\ldots+\hat\beta_kx_k$. Both CI are centered in $\hat y$.
- *Differences*. $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$ is deterministic and $Y|\mathbf{X}=\mathbf{x}$ is random. Therefore, the variance is larger for the prediction of $Y|\mathbf{X}=\mathbf{x}$ than for the prediction of $\mathbb{E}[Y|\mathbf{X}=\mathbf{x}]$.
```


## ANOVA and model fit {#anovamult}

### ANOVA

The ANOVA decomposition for multiple linear regression is quite analogous to the one in simple linear regression. The ANOVA decomposes the variance of $Y$ into two parts, each one corresponding to the regression and to the error, respectively. Since the difference between simple and multiple linear regression is the number of predictors -- the response $Y$ is unique in both cases -- the ANOVA decompositions are highly similar, as we will see.

As in simple linear regression, *the mean of the fitted values $\hat Y_1,\ldots,\hat Y_n$ is the mean of $Y_1,\ldots, Y_n$*. This is an important result that can be checked if we use matrix notation. The ANOVA decomposition considers the following measures of variation related with the response:

- $\text{SST}=\sum_{i=1}^n\left(Y_i-\bar Y\right)^2$, the **total sum of squares**. This is the *total variation* of $Y_1,\ldots,Y_n$, since $\text{SST}=ns_y^2$, where $s_y^2$ is the sample variance of $Y_1,\ldots,Y_n$.
- $\text{SSR}=\sum_{i=1}^n\left(\hat Y_i-\bar Y\right)^2$, the **regression sum of squares**. This is the variation explained by the regression plane, that is, *the variation from $\bar Y$ that is explained by the estimated conditional mean $\hat Y_i=\hat\beta_0+\hat\beta_1X_{i1}+\ldots+\hat\beta_kX_{ik}$*. $\text{SSR}=ns_{\hat y}^2$, where $s_{\hat y}^2$ is the sample variance of $\hat Y_1,\ldots,\hat Y_n$.
- $\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2$, the **sum of squared errors**^[SSE and RSS are two names for the same quantity (that appears in different contexts): $\text{SSE}=\sum_{i=1}^n\left(Y_i-\hat Y_i\right)^2=\sum_{i=1}^n\left(Y_i-\hat \beta_0-\hat \beta_1X_{i1}-\ldots-\hat \beta_kX_{ik}\right)^2=\mathrm{RSS}(\hat{\boldsymbol{\beta}})$.]. Is the variation around the conditional mean. Recall that $\text{SSE}=\sum_{i=1}^n \hat\varepsilon_i^2=(n-k-1)\hat\sigma^2$, where $\hat\sigma^2$ is the sample variance of $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$.

The ANOVA decomposition is exactly the same as in simple linear regression:
\begin{align}
\underbrace{\text{SST}}_{\text{Variation of }Y_i's} = \underbrace{\text{SSR}}_{\text{Variation of }\hat Y_i's} + \underbrace{\text{SSE}}_{\text{Variation of }\hat \varepsilon_i's} (\#eq:anovamult)
\end{align}
or, equivalently (dividing by $n$ in \@ref(eq:anovamult)),
\begin{align*}
\underbrace{s_y^2}_{\text{Variance of $Y_i$'s}} = \underbrace{s_{\hat y}^2}_{\text{Variance of $\hat Y_i$'s}} + \underbrace{(n-k-1)/n\times\hat\sigma^2}_{\text{Variance of $\hat\varepsilon_i$'s}}.
\end{align*}
Notice the $n-k-1$ instead of simple linear regression's $n-2$, which is the main change. The graphical interpretation of \@ref(eq:anovamult) when $k=2$ is shown in Figures \@ref(fig:anova3D) and \@ref(fig:anovaillus3D).

```{r, anova3D, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Visualization of the ANOVA decomposition when $k=2$. SST measures the variation of $Y_1,\\ldots,Y_n$ with respect to $\\bar Y$. SST measures the variation with respect to the conditional means, $\\hat\\beta_0+\\hat\\beta_1X_{i1}+\\hat\\beta_2X_{i2}$. SSE collects the variation of the residuals.', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/R/anova3D.png")
```

```{r, anovaillus3D, echo = FALSE, fig.cap = 'Illustration of the ANOVA decomposition and its dependence on $\\sigma^2$ and $\\hat\\sigma^2$.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%', cache = TRUE}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/anova-3D/', height = '900px')
```

The ANOVA table summarizes the decomposition of the variance.

|     | Degrees of freedom | Sum Squares | Mean Squares | $F$-value | $p$-value |
|-----|------------|--------|---------|----------------|----------------|
| Predictors | $k$ | SSR | $\frac{\text{SSR}}{k}$ | $\frac{\text{SSR}/k}{\text{SSE}/(n-k-1)}$ | $p$ |
| Residuals | $n - k-1$ | SSE | $\frac{\text{SSE}}{n-k-1}$ | | |

The "$F$-value" of the ANOVA table represents the value of the $F$-statistic $\frac{\text{SSR}/k}{\text{SSE}/(n-k-1)}$. This statistic is employed to test
\begin{align*}
H_0:\beta_1=\ldots=\beta_k=0\quad\text{vs.}\quad H_1:\beta_j\neq 0\text{ for any $j$},
\end{align*}
that is, the hypothesis of *no linear dependence of $Y$ on $X_1,\ldots,X_k$* (the plane is completely flat, with no inclination). If $H_0$ is rejected, it means that *at least one $\beta_j$ is significantly different from zero*. It happens that
\begin{align*}
F=\frac{\text{SSR}/k}{\text{SSE}/(n-k-1)}\stackrel{H_0}{\sim} F_{k,n-k-1},
\end{align*}
where $F_{k,n-k-1}$ is the *Snedecor's $F$ distribution* with $k$ and $n-k-1$ degrees of freedom. If $H_0$ is true, then $F$ is expected to be *small* since SSR will be close to zero (little variation is explained by the regression model since $\hat{\boldsymbol{\beta}}\approx\mathbf{0}$). The $p$-value of this test is **not** the same as the $p$-value of the $t$-test for $H_0:\beta_1=0$, that only happens in simple linear regression because $k=1$!

```{block, type = 'rmdcaution'}
The "ANOVA table" is a broad concept in statistics, with different variants. Here we are only covering the basic ANOVA table from the relation $\text{SST} = \text{SSR} + \text{SSE}$. However, further sophistications are possible when $\text{SSR}$ is decomposed into the variations contributed by *each* predictor. In particular, for multiple linear regression `R`'s `anova` implements a *sequential (type I) ANOVA table*, which is **not** the previous table!
```

The `anova` function in `R` takes a model as an input and returns the following *sequential* ANOVA table^[More complex -- included here just for clarification of the `anova`'s output.]:

|              | Degrees of freedom | Sum Squares | Mean Squares | $F$-value |$p$-value|
|--------------|------------|--------|---------|----------------|-------------|
| Predictor 1| $1$ | SSR$_1$ | $\frac{\text{SSR}_1}{1}$ | $\frac{\text{SSR}_1/1}{\text{SSE}/(n-k-1)}$ | $p_1$ |
| Predictor 2| $1$ | SSR$_2$ | $\frac{\text{SSR}_2}{1}$ | $\frac{\text{SSR}_2/1}{\text{SSE}/(n-k-1)}$ | $p_2$ |
| $\vdots$| $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| Predictor $k$| $1$ | SSR$_k$ | $\frac{\text{SSR}_k}{1}$ | $\frac{\text{SSR}_k/1}{\text{SSE}/(n-k-1)}$ | $p_k$ |
| Residuals | $n - k - 1$ | SSE | $\frac{\text{SSE}}{n-k-1}$ | | |

Here the SSR$_j$ represents the regression sum of squares associated to the inclusion of $X_j$ in the model with predictors $X_1,\ldots,X_{j-1}$, this is:
\[
\text{SSR}_j=\text{SSR}(X_1,\ldots,X_j)-\text{SSR}(X_1,\ldots,X_{j-1}).
\]
The $p$-values $p_1,\ldots,p_k$ correspond to the testing of the hypotheses
\begin{align*}
H_0:\beta_j=0\quad\text{vs.}\quad H_1:\beta_j\neq 0,
\end{align*}
carried out **inside the linear model $Y=\beta_0+\beta_1X_1+\ldots+\beta_jX_j+\varepsilon$**. This is like the $t$-test for $\beta_j$ for the model with predictors $X_1,\ldots,X_j$.

Let's see how we can compute both ANOVA tables in `R`. The sequential table is simple: use `anova`. We illustrate it with the `Boston` dataset.

```{r, collapse = TRUE, message=FALSE, cache = TRUE}
# Load data
library(MASS)
data(Boston)

# Fit a linear model
model <- lm(medv ~ crim + lstat + zn + nox, data = Boston)
summary(model)

# ANOVA table with sequential test
anova(model)
# The last p-value is the one of the last t-test
```

In order to compute the simplified ANOVA table, we need to rely on an ad-hoc *function*^[You will need to run this piece of code whenever you want to call `simpleAnova`, since it is not part of `R` nor `R Commander`.]. The function takes as input a fitted `lm`:

```{r, cache = TRUE}
# This function computes the simplied anova from a linear model
simpleAnova <- function(object, ...) {

  # Compute anova table
  tab <- anova(object, ...)

  # Obtain number of predictors
  p <- nrow(tab) - 1

  # Add predictors row
  predictorsRow <- colSums(tab[1:p, 1:2])
  predictorsRow <- c(predictorsRow, predictorsRow[2] / predictorsRow[1])

  # F-quantities
  Fval <- predictorsRow[3] / tab[p + 1, 3]
  pval <- pf(Fval, df1 = p, df2 = tab$Df[p + 1], lower.tail = FALSE)
  predictorsRow <- c(predictorsRow, Fval, pval)

  # Simplified table
  tab <- rbind(predictorsRow, tab[p + 1, ])
  row.names(tab)[1] <- "Predictors"
  return(tab)

}

# Simplified ANOVA
simpleAnova(model)
```

Recall that the $F$-statistic, its $p$-value and the degrees of freedom are also given in the output of `summary`.

```{block, type = 'rmdexercise'}
Compute the ANOVA table for the regression `Price ~ WinterRain + AGST + HarvestRain + Age` in the `wine` dataset. Check that the $p$-value for the $F$-test given in `summary` and by `simpleAnova` are the same.
```

### The $R^2$

The *coefficient of determination* $R^2$ is defined as in simple linear regression:
\begin{align*}
R^2=\frac{\text{SSR}}{\text{SST}}=\frac{\text{SSR}}{\text{SSR}+\text{SSE}}=\frac{\text{SSR}}{\text{SSR}+(n-k-1)\hat\sigma^2}.
\end{align*}
$R^2$ measures the **proportion of variation** of the response variable $Y$ that is **explained** by the predictor $X$ through the regression. Intuitively, $R^2$ measures the **tightness of the data cloud around the regression plane**. Check in Figure \@ref(fig:anovaillus3D) how changing the value of $\sigma^2$ (not $\hat\sigma^2$, but $\hat\sigma^2$ is obviously dependent on $\sigma^2$) affects the $R^2$. Also, as we saw in Section \@ref(fit), $R^2=r^2_{y\hat y}$, that is, *the square of the sample correlation coefficient between $Y_1,\ldots,Y_n$ and $\hat Y_1,\ldots,\hat Y_n$ is $R^2$*.

Trusting blindly the $R^2$ can lead to catastrophic conclusions in model selection. Here is a counterexample of a multiple regression where the $R^2$ is apparently large but the assumptions discussed in Section \@ref(assumptionsmult) are clearly not satisfied.

```{r, collapse = TRUE, webgl = TRUE, cache = TRUE}
# Create data that:
# 1) does not follow a linear model
# 2) the error is heteroskedastic
x1 <- seq(0.15, 1, l = 100)
set.seed(123456)
x2 <- runif(100, -3, 3)
eps <- rnorm(n = 100, sd = 0.25 * x1^2)
y <- 1 - 3 * x1 * (1 + 0.25 * sin(4 * pi * x1)) + 0.25 * cos(x2) + eps

# Great R^2!?
reg <- lm(y ~ x1 + x2)
summary(reg)

# But prediction is obviously problematic
scatter3d(y ~ x1 + x2, fit = "linear")
```

```{block, type = 'rmdinsight'}
Remember that:

- $R^2$ does not measure the correctness of a linear model but its **usefulness**, assuming the model is correct.
- $R^2$ is the proportion of variance of $Y$ explained by $X_1,\ldots,X_k$, but, of course, *only when the linear model is correct*.
```

We finalize by pointing out a nice connection between the $R^2$, the ANOVA decomposition and the least squares estimator $\hat{\boldsymbol{\beta}}$:

```{block, type = 'rmdinsight'}
The ANOVA decomposition gives another interpretation of the least-squares estimates: **$\hat{\boldsymbol{\beta}}$ are the estimated coefficients that maximize the $R^2$** (among all the possible estimates we could think about). To see this, recall that
\[
R^2=\frac{\text{SSR}}{\text{SST}}=\frac{\text{SST} - \text{SSE}}{\text{SST}}=\frac{\text{SST} - \text{RSS}(\hat{\boldsymbol{\beta}})}{\text{SST}},
\]
so if $\text{RSS}(\hat{\boldsymbol{\beta}})=\min_{\boldsymbol{\beta}\in\mathbb{R}^{k+1}}\text{RSS}(\boldsymbol{\beta})$, then $R^2$ is maximal for $\hat{\boldsymbol{\beta}}$!
```

### The $R^2_{\text{Adj}}$

As we saw, these are equivalent forms for $R^2$:
\begin{align}
R^2&=\frac{\text{SSR}}{\text{SST}}=\frac{\text{SST}-\text{SSE}}{\text{SST}}=1-\frac{\text{SSE}}{\text{SST}}\nonumber\\
&=1-\frac{\hat\sigma^2}{\text{SST}}\times(n-k-1).(\#eq:R2)
\end{align}
The SSE on the numerator always decreases as more predictors are added to the model, even if these are no significant. As a consequence, the **$R^2$ always increases with $k$**. Why is so? Intuitively, because the complexity -- hence the flexibility -- of the model augments when we use more predictors to explain $Y$. Mathematically, because when $k$ approaches $n-1$ and $\hat\sigma^2$, then the second term in \@ref(eq:R2) is reduced and as a consequence $R^2$ grows.

The *adjusted $R^2$* is an important quantity specifically designed to cover this $R^2$'s flaw, which is ubiquitous in multiple linear regression. The purpose is to have a better tool for **comparing models without systematically favouring complexer models**. This alternative coefficient is defined as
\begin{align}
R^2_{\text{Adj}}&=1-\frac{\text{SSE}/(n-k-1)}{\text{SST}/(n-1)}=1-\frac{\text{SSE}}{\text{SST}}\times\frac{n-1}{n-k-1}\nonumber\\
&=1-\frac{\hat\sigma^2}{\text{SST}}\times (n-1).(\#eq:R2A)
\end{align}
The $R^2_{\text{Adj}}$ is independent of $k$, at least explicitly. If $k=1$ then $R^2_{\text{Adj}}$ is *almost* $R^2$ (practically identical if $n$ is large). Both \@ref(eq:R2) and \@ref(eq:R2A) are quite similar except for the last factor, which in the former does not depend on $k$. Therefore, \@ref(eq:R2A) will only increase if $\hat\sigma^2$ is reduced with $k$ -- in other words, if the new variables contribute in the reduction of variability around the regression plane.

The different behavior between $R^2$ and $R^2_\text{Adj}$ can be visualized by a small simulation. Suppose that we generate a random dataset, with $n=200$ observations of a response $Y$ and two predictors $X_1,X_2$. This is, the sample $\{(X_{i1},X_{i2},Y_i)\}_{i=1}^n$ with
\[
Y_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+\varepsilon_i,\quad \varepsilon_i\sim\mathcal{N}(0,1).
\]
Tho this data, we add $196$ *garbage* predictors that are completely independent from $Y$. Therefore, we end up with $k=198$ predictors. Now we compute the $R^2(j)$ and $R^2_\text{Adj}(j)$ for the models
\[
Y=\beta_0+\beta_1X_{1}+\ldots+\beta_jX_{j}+\varepsilon,
\]
with $j=1,\ldots,k$ and we plot them as the curves $(j,R^2(j))$ and $(j,R_\text{Adj}^2(j))$. Since **$R^2$ and $R^2_\text{Adj}$ are random variables**, we repeat the procedure $100$ times to have a measure of the variability.

```{r, R2, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Comparison of $R^2$ and $R^2_{\\text{Adj}}$ for $n=200$ and $k$ ranging from $1$ to $198$. $M=100$ datasets were simulated with **only the first two** predictors being significant. The thicker curves are the mean of each color\'s curves.', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/R/R2vsAdjR2.png")
```

Figure \@ref(fig:R2) contains the results of this experiment. As you can see $R^2$ increases linearly with the number of predictors considered, although only the first two ones were important! On the contrary, $R^2_\text{Adj}$ only increases in the first two variables and then is flat on average, but it has a huge variability when $k$ approaches $n-2$. This is a consequence of the explosive variance of $\hat\sigma^2$ in that *degenerate case* (as we will see in Section \@ref(selection)). The experiment evidences that **$R^2_\text{Adj}$ is more adequate than the $R^2$ for evaluating the fit of a multiple linear regression**.

An example of a simulated dataset considered in the experiment of Figure \@ref(fig:R2):
```{r, eval = FALSE, cache = TRUE}
# Generate data
k <- 198
n <- 200
set.seed(3456732)
beta <- c(0.5, -0.5, rep(0, k - 2))
X <- matrix(rnorm(n * k), nrow = n, ncol = k)
Y <- drop(X %*% beta + rnorm(n, sd = 3))
data <- data.frame(y = Y, x = X)

# Regression on the two meaningful predictors
summary(lm(y ~ x.1 + x.2, data = data))

# Adding 20 garbage variables
summary(lm(y ~ X[, 1:22], data = data))
```

```{block, type = 'rmdcaution'}
The $R^2_\text{Adj}$ no longer measures the proportion of variation of $Y$ explained by the regression, but the result of *correcting this proportion by the number of predictors exmployed*. As a consequence of this, $R^2_\text{Adj}\leq1$ but **it can be negative!**
```

The next code illustrates a situation where we have two predictors completely independent from the response. The fitted model has a negative $R^2_\text{Adj}$.
```{r, collapse = TRUE, cache = TRUE}
# Three independent variables
set.seed(234599)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- 1 + rnorm(100)

# Negative adjusted R^2
summary(lm(y ~ x1 + x2))
```

```{block, type = 'rmdexercise'}
Construct more predictors (`x3`, `x4`, ...) by sampling 100 points from a normal (`rnorm(100)`). Check that when the predictors are added to the model, the $R^2_\text{Adj}$ decreases and the $R^2$ increases.
```

## Model selection {#selection}

In Section \@ref(wine) we briefly saw that **the inclusion of more predictors is not for free**: there is a price to pay in terms more variability on the coefficients. Indeed, there is a **maximum number of predictors $k$** that can be considered in a linear model for a sample size $n$: **$k\leq n-2$**. Or equivalently, there is a **minimum sample size $n$** required for fitting a model with $k$ predictors: **$n\geq k + 2$**.

The interpretation of this fact is simple if we think on the geometry for $k=1$ and $k=2$:

- If $k=1$, we need at least $n=2$ points to fit uniquely a line. However, this line gives no information on the vertical variation around it and hence $\hat\sigma^2$ can not be estimated (applying its formula, we would have $\hat\sigma^2=\infty$). Therefore we need at least $n=3$ points, or in other words $n\geq k + 2=3$.
- If $k=2$, we need at least $n=3$ points to fit uniquely a plane. But this plane gives no information on the variation of the data around it and hence $\hat\sigma^2$ can not be estimated. Therefore we need $n\geq k + 2=4$.

Another interpretation is the following:

> The fitting of a linear model with $k$ predictors involves the estimation the $k+2$ parameters $(\boldsymbol{\beta},\sigma^2)$ from $n$ data points. The closer $k+2$ and $n$ are, the more variable the estimates $(\hat{\boldsymbol{\beta}},\hat\sigma^2)$ will be, since less information is availiable for computing each one. In the limit case $n=k+2$, each sample point determines a parameter estimate.

The *degrees of freedom* $n-k-1$ quantify the increasing on the variability of $(\hat{\boldsymbol{\beta}},\hat\sigma^2)$ when $n-k-1$ decreases. For example:

- $t_{n-k-1;\alpha/2}$ appears in \@ref(eq:normp2) and influences the length of the CIs for $\beta_j$, see \@ref(eq:cip). It also influences the length of the CIs for the prediction. As Figure \@ref(fig:dft) shows, when the degrees of freedom decrease, $t_{n-k-1;\alpha/2}$ increases, thus the intervals become wider.
- $\hat\sigma^2=\frac{1}{n-k-1}\sum_{i=1}^n\hat\varepsilon_i^2$ influences the $R^2$ and $R^2_\text{Adj}$. If no relevant variables are added to the model then $\sum_{i=1}^n\hat\varepsilon_i^2$ will not change substantially. However, the reducing factor $\frac{1}{n-k-1}$ will decrease as $k$ augments, inflating $\hat\sigma^2$ and its variance. This is exactly what happened in Figure \@ref(fig:R2).


```{r, dft, echo = FALSE, warning = FALSE, results = 'hide', out.width = '70%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Effect of $\\text{df}=n-k-1$ in $t_{\\text{df};\\alpha/2}$ for $\\alpha=0.10,0.05,0.01$.', cache = TRUE}
df <- 1:30
alpha <- 0.10
plot(df, qt(p = alpha/2, df = df, lower.tail = FALSE), type = "o",
     xlab = expression(df), ylab = expression(t[df * ";" * alpha/2]),
     ylim = c(0, 10), col = rainbow(3)[1], pch = 16)
alpha <- 0.05
lines(df, qt(p = alpha/2, df = df, lower.tail = FALSE), col = rainbow(3)[2],
      type = "o", pch = 16)
alpha <- 0.01
lines(df, qt(p = alpha/2, df = df, lower.tail = FALSE), col = rainbow(3)[3],
      type = "o", pch = 16)
legend("topright", lwd = 2, col = rainbow(3),
       legend = expression(alpha == 0.10, alpha == 0.05, alpha == 0.01))
```

Now that we have added more light into the problem of having an excess of predictors, we turn the focus into **selecting the most adequate predictors for a multiple regression model**. This is a challenging task without a unique solution, and what is worse, without a method that is guaranteed to work in all the cases. However, there is a well-established procedure that usually gives good results: the *stepwise regression*. Its principle is to compare multiple linear regression models with different predictors (and, of course, with the same responses).

Before introducing the method, we need to understand what is an **information criterion**. An information criterion balances the fitness of a model with the number of predictors employed. Hence, it determines objectively the best model as the one that *minimizes the information criterion*. Tho common criteria are the *Bayesian Information Criterion* (BIC) and the *Akaike Information Criterion* (AIC). Both are based on a **balance between the model fitness and its complexity**:
\begin{align}
\text{BIC}(\text{model}) = \underbrace{-2\log\text{lik(model)}}_{\text{Model fitness}} + \underbrace{\text{npar(model)}\times\log n}_{\text{Complexity}}, (\#eq:bic)
\end{align}
where $\text{lik(model)}$ is the *likelihood of the model* (how well the model fits the data) and $\text{npar(model)}$ is the number of parameters of the model, $k+2$ in the case of a multiple linear regression model with $k$ predictors. The **AIC** replaces $\log n$ by $2$ in \@ref(eq:bic), so it **penalizes less complexer models**. This is one of the reasons why BIC is preferred by some practitioners for model comparison. Also, because is *consistent* in selecting the true model: if enough data is provided, the BIC is guaranteed to select the data-generating model among a list of candidate models.

The BIC and AIC can be computed in `R` through the functions `BIC` and `AIC`. They take a model as the input.
```{r, collapse = TRUE, cache = TRUE}
# Load iris dataset
data(iris)

# Two models with different predictors
mod1 <- lm(Petal.Length ~ Sepal.Width, data = iris)
mod2 <- lm(Petal.Length ~ Sepal.Width + Petal.Width, data = iris)

# BICs
BIC(mod1)
BIC(mod2) # Smaller -> better

# Check the summaries
summary(mod1)
summary(mod2)
```

Let's go back to the selection of predictors. If we have $k$ predictors, a naive procedure would be to check *all the possible* models that can be constructed with them and then select the best one in terms of BIC/AIC. The problem is that there are $2^{k+1}$ possible models! Fortunately, the `stepwise` procedure helps us navigating this ocean of models. The function takes as input a *model employing all the available predictors*.

```{r, collapse = TRUE, cache = TRUE}
# Explain NOx in Boston dataset
mod <- lm(nox ~ ., data = Boston)

# With BIC
modBIC <- stepwise(mod, trace = 0)
summary(modBIC)

# Different search directions
stepwise(mod, trace = 0, direction = "forward")
stepwise(mod, trace = 0, direction = "backward")


# With AIC
modAIC <- stepwise(mod, trace = 0, criterion = "AIC")
summary(modAIC)
```

The model selected by `stepwise` is a good starting point for further additions or deletions of predictors. For example, in `modAIC` we could remove `crim`.

```{block, type='rmdtip'}
When applying `stepwise` for BIC/AIC, different final models might be selected depending on the choice of `direction`. This is the interpretation:

- `"backward"`: starts from the full model, *removes* predictors sequentially.
- `"forward"`: starts a simple model, *adds* predictors sequentially.
- `"backward/forward"` (default) and `"forward/backward"`: combination of the above.

The **advice** is to try several of these methods and retain the one with minimum BIC/AIC. Set `trace = 0` to omit lengthy outputs of information of the search procedure.
```

```{block, type='rmdcaution'}
`stepwise` assumes no `NA`'s (missing values) are present in the data. It is advised to remove the missing values in the data before since their presence might lead to errors. To do so, employ `data = na.omit(dataset)` in the call to `lm` (if your dataset is `dataset`).
```


We conclude highlighting a caveat on the use of the BIC and AIC: they are constructed assuming that the sample size $n$ is much larger than the number of parameters in the model ($k+2$). Therefore, they will work reasonably well if $n>>k+2$, but if this is not true they may favor unrealistic complex models. An illustration of this phenomena is Figure \@ref(fig:bic), which is the BIC/AIC version of Figure \@ref(fig:R2) for the experiment done in Section \@ref(anovamult). The BIC and AIC curves tend to have local minimums close to $k=2$ and then increase. But when $k+2$ gets close to $n$, they quickly drop down. Note also how the BIC penalizes more the complexity than the AIC, which is more flat.

```{r, bic, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Comparison of BIC and AIC for $n=200$ and $k$ ranging from $1$ to $198$. $M=100$ datasets were simulated with **only the first two** predictors being significant. The thicker curves are the mean of each color\'s curves.', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/R/BICandAIC.png")
```

## Model diagnostics and multicollinearity {#diagnostics}

As we saw in Section \@ref(assumptionsmult), checking the assumptions of the multiple linear model through the data scatterplots becomes tricky even when $k=2$. To solve this issue, a series of *diagnostic plots* have been designed in order to evaluate graphically and in a simple way the validity of the assumptions. For illustration, we retake the `wine` dataset ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/wine.csv)).

```{r, collapse = TRUE, cache = TRUE}
mod <- lm(Price ~ Age + AGST + HarvestRain + WinterRain, data = wine)
```

We will focus only in three plots:

1. **Residuals vs. fitted values plot**. This plot serves mainly to check the **linearity**, although lack of homoscedasticity or independence can also be detected. Here is an example:

    ```{r, collapse = TRUE, cache = TRUE}
    plot(mod, 1)
    ```

    **Under linearity, we expect the red line** (a nonlinear fit of the mean of the residuals) **to be almost flat**. This which means that the trend of $Y_1,\ldots,Y_n$ is linear with respect to the predictors. Heteroskedasticity can be detected also in the form of irregular vertical dispersion around the red line. The dependence between residuals can be detected (harder) in the form of non randomly spread residuals.

2. **QQ-plot**. Checks the **normality**:

    ```{r, collapse = TRUE, cache = TRUE}
    plot(mod, 2)
    ```

    **Under normality, we expect the points** (sample quantiles of the standardized residuals vs. theoretical quantiles of a $\mathcal{N}(0,1)$) **to align with the diagonal line**, which represents the ideal position of the points if those were sampled from a $\mathcal{N}(0,1)$. It is usual to have larger departures from the diagonal in the extremes than in the center, even under normality, although these departures are more clear if the data is non-normal.

3. **Scale-location plot**. Serves for checking the **homoscedasticity**. It is similar to the first diagnostic plot, but now with the residuals standardized and transformed by a square root (of the absolute value). This change transforms the task of spotting heteroskedasticity by looking into irregular vertical dispersion patterns into spotting for nonlinearities, which is somehow simpler.

    ```{r, collapse = TRUE, cache = TRUE}
    plot(mod, 3)
    ```

    **Under homoscedasticity, we expect the red line to be almost flat.** If there are consistent nonlinear patterns, then there is evidence of heteroskedasticity.

```{block, type ='rmdtip'}
If you type `plot(mod)`, several diagnostic plots will be shown sequentially. In order to advance them, hit `'Enter'` in the `R` console.
```

The next figures present datasets where the assumptions are satisfied and violated.

```{r, diagnostics1, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Residuals vs. fitted values plots for datasets respecting (left column) and violating (right column) the linearity assumption.', cache = TRUE}
knitr::include_graphics("images/R/diagnostics1.png")
```

```{r, diagnostics2, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'QQ-plots for datasets respecting (left column) and violating (right column) the normality assumption.', cache = TRUE}
knitr::include_graphics("images/R/diagnostics2.png")
```

```{r, diagnostics3, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Scale-location plots for datasets respecting (left column) and violating (right column) the homoscedasticity assumption.', cache = TRUE}
knitr::include_graphics("images/R/diagnostics3.png")
```

```{block, type = 'rmdexercise'}
Load the dataset `assumptions3D.RData`([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/assumptions3D.RData)) and compute the regressions `y.3 ~ x1.3 + x2.3`, `y.4 ~ x1.4 + x2.4`, `y.5 ~ x1.5 + x2.5` and `y.8 ~ x1.8 + x2.8`. Use the three diagnostic plots to test the assumptions of the linear model.
```

A common problem that arises in multiple linear regression is **multicollinearity**. This is the situation when two or more predictors are highly linearly related between them. Multicollinearitiy has important effects on the fit of the model:

- It **reduces the precision of the estimates**. As a consequence, sings of fitted coefficients may be reversed and valuable predictors may appear as non significant.
- It is **difficult to determine how each of the highly related predictors affects the response**, since one masks the other. This may result in numerical instabilities.

<!--
Intuitively, multicollinearity can be thought in a
plane spins around a straight line (think about holding a card on their opposite
corners and make it spinning on its axis of rotation)
Animation:
Insight into straight line in 3D
-->

An approach is to detect multicollinearity is to compute the correlation matrix between the predictors by `cor` (in `R Commander`: `'Statistics' -> 'Summaries' -> 'Correlation matrix...'`)
```{r, collapse = TRUE, cache = TRUE}
cor(wine)
```
Here we can see what we already knew from Section \@ref(wine), that `Age` and `Year` are perfectly linearly related and that `Age` and `FrancePop` are highly linearly related.

However, is not enough to inspect remove pair by pair correlations in order to get rid of multicollinearity. Here is a counterexample:
```{r, collapse = TRUE, cache = TRUE}
# Create predictors with multicollinearity: x4 depends on the rest
set.seed(45678)
x1 <- rnorm(100)
x2 <- 0.5 * x1 + rnorm(100)
x3 <- 0.5 * x2 + rnorm(100)
x4 <- -x1 + x2 + rnorm(100, sd = 0.25)

# Response
y <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4 + rnorm(100)
data <- data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Correlations - none seems suspicius
cor(data)
```

A better approach is to compute the **Variance Inflation Factor** (VIF) of each coefficient $\hat\beta_j$. This is a *measure of how linearly dependent is $X_j$ with the rest of predictors*:
\[
\text{VIF}(\hat\beta_j)=\frac{1}{1-R^2_{X_j|X_{-j}}}
\]
where $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ into the remaining predictors. The next rule of thumb gives direct insight into which predictors are multicollinear:

- VIF close to 1: absence of multicollinearity.
- **VIF larger than 5 or 10: problematic amount of multicollinearity**. Advised to remove the predictor with largest VIF.

VIF is called by `vif` and takes as argument a linear model (In `R Commander`: `'Models' -> 'Numerical diagnostics' -> 'Variance-inflation factors'`). We continue with the previous example.
```{r, collapse = TRUE, cache = TRUE}
# Variance inflation factors anormal: largest for x4, we remove it
modMultiCo <- lm(y ~ x1 + x2 + x3 + x4)
vif(modMultiCo)

# Without x4
modClean <- lm(y ~ x1 + x2 + x3)

# Comparison
summary(modMultiCo)
summary(modClean)

# Variance inflation factors normal
vif(modClean)
```

<!--

## Polynomial fitting {#mult-poly}


```{r, mult-transf, echo = FALSE, fig.cap = 'Illustration of the choice of the polynomial transformation.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/mult-non-lin/', height = '1000px')
```

-->

<!--
## Exercises and case studies

### Computing multiple linear regressions

```{block, type = 'rmdexercise'}
TBA
```

College
Wage
USArrests
Boston

### Case study I: *a marketing plan*

(Case study from )

### Case study II: * *


-->

<!--chapter:end:02-mult-lin-reg.Rmd-->


# Logistic regression {#log-reg}

As we saw in Chapters \@ref(simp) and \@ref(mult), linear regression assumes that the response variable $Y$ is *continuous*. In this chapter we will see how *logistic regression* can deal with a *discrete* response $Y$. The simplest case is with $Y$ being a *binary* response, that is, a variable encoding two categories. In general, we assume that we have $X_1,\ldots,X_k$ predictors for explaining $Y$ (*multiple* logistic regression) and cover the peculiarities for $k=1$ as particular cases.

## More `R` basics

In order to implement some of the contents of this chapter we need to cover more `R` basics, mostly related with flexible plotting that is not implemented directly in `R Commander`. The `R` functions we will are also very useful for simplifying some `R Commander` approaches.

In the following sections, **type** -- not copy and paste systematically -- the code in the `'R Script'` panel and send it to the output panel. Remember that you should get the same outputs (which are preceded by `## [1]`).

### Data frames revisited

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# Let's begin importing the iris dataset
data(iris)

# names gives you the variables in the data frame
names(iris)

# The beginning of the data
head(iris)

# So we can access variables by $ or as in a matrix
iris$Sepal.Length[1:10]
iris[1:10, 1]
iris[3, 1]

# Information on the dimension of the data frame
dim(iris)

# str gives the structure of any object in R
str(iris)

# Recall the species variable: it is a categorical variable (or factor),
# not a numeric variable
iris$Species[1:10]

# Factors can only take certain values
levels(iris$Species)

# If a file contains a variable with character strings as observations (either
# encapsulated by quotation marks or not), the variable will become a factor
# when imported into R

```

```{block, type = 'rmdexercise'}
Do the following:

- Import [`auto.txt`](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/auto.txt) into `R` as the data frame `auto`. Check how the character strings in the file give rise to factor variables.
- Get the dimensions of `auto` and show beginning of the data.
- Retrieve the fifth observation of `horsepower` in two different ways.
- Compute the levels of `name`.
```

### Vector-related functions

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# The function seq creates sequences of numbers equally separated
seq(0, 1, by = 0.1)
seq(0, 1, length.out = 5)

# You can short the latter argument
seq(0, 1, l = 5)

# Repeat  number
rep(0, 5)

# Reverse a vector
myVec <- c(1:5, -1:3)
rev(myVec)

# Another way
myVec[length(myVec):1]

# Count repetitions in your data
table(iris$Sepal.Length)
table(iris$Species)
```

```{block, type = 'rmdexercise'}
Do the following:

- Create the vector $x=(0.3, 0.6, 0.9, 1.2)$.
- Create a vector of length 100 ranging from $0$ to $1$ with entries equally separated.
- Compute the amount of zeros and ones in `x <- c(0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0)`. Check that they are the same as in `rev(x)`.
- Compute the vector $(0.1, 1.1, 2.1, ..., 100.1)$ in four different ways using `seq` and `rev`. Do the same but using `:` instead of `seq`. (Hint: add `0.1`)

```

### Logical conditions and subsetting

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# Relational operators: x < y, x > y, x <= y, x >= y, x == y, x!= y
# They return TRUE or FALSE

# Smaller than
0 < 1

# Greater than
1 > 1

# Greater or equal to
1 >= 1 # Remember: ">="" and not "=>"" !

# Smaller or equal to
2 <= 1 # Remember: "<="" and not "=<"" !

# Equal
1 == 1 # Tests equality. Remember: "=="" and not "="" !

# Unequal
1 != 0 # Tests iequality

# TRUE is encoded as 1 and FALSE as 0
TRUE + 1
FALSE + 1

# In a vector-like fashion
x <- 1:5
y <- c(0, 3, 1, 5, 2)
x < y
x == y
x != y

# Subsetting of vectors
x
x[x >= 2]
x[x < 3]

# Easy way of work with parts of the data
data <- data.frame(x = c(0, 1, 3, 3, 0), y = 1:5)
data

# Data such that x is zero
data0 <- data[data$x == 0, ]
data0

# Data such that x is larger than 2
data2 <- data[data$x > 2, ]
data2

# In an example
iris$Sepal.Width[iris$Sepal.Width > 3]

# Problem - what happened?
data[x > 2, ]

# In an example
summary(iris)
summary(iris[iris$Sepal.Width > 3, ])

# On the factor variable only makes sense == and !=
summary(iris[iris$Species == "setosa", ])

# Subset argument in lm
lm(Sepal.Width ~ Petal.Length, data = iris, subset = Sepal.Width > 3)
lm(Sepal.Width ~ Petal.Length, data = iris, subset = iris$Sepal.Width > 3)
# Both iris$Sepal.Width and Sepal.Width in subset are fine: data = iris
# tells R to look for Sepal.Width in the iris dataset

# Same thing for the subset field in R Commander's menus

# AND operator &
TRUE & TRUE
TRUE & FALSE
FALSE & FALSE

# OR operator |
TRUE | TRUE
TRUE | FALSE
FALSE | FALSE

# Both operators are useful for checking for ranges of data
y
index1 <- (y <= 3) & (y > 0)
y[index1]
index2 <- (y < 2) | (y > 4)
y[index2]

# In an example
summary(iris[iris$Sepal.Width > 3 & iris$Sepal.Width < 3.5, ])
```
```{block, type = 'rmdexercise'}
Do the following for the `iris` dataset:

- Compute the subset corresponding to `Petal.Length` either smaller than `1.5` or larger than `2`. Save this dataset as `irisPetal`.
- Compute and summarize a linear regression of `Sepal.Width` into `Petal.Width + Petal.Length` for the dataset `irisPetal`. What is the $R^2$? (Solution: `0.101`)
- Check that the previous model is the same as regressing `Sepal.Width` into `Petal.Width + Petal.Length` for the dataset `iris` with the appropriate `subset` expression.
- Compute the variance for `Petal.Width` when `Petal.Width` is smaller or equal that `1.5` and larger than `0.3`. (Solution: `0.1266541`)

```

### Plotting functions

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# plot is the main function for plotting in R
# It has a different behaviour depending on the kind of object that it receives

# For example, for a regression model, it produces diagnostic plots
mod <- lm(Sepal.Width ~ Sepal.Length, data = iris)
plot(mod, 1)

# How to plot some data
plot(iris$Sepal.Length, iris$Sepal.Width, main = "Sepal.Length vs Sepal.Width")

# Change the axis limits
plot(iris$Sepal.Length, iris$Sepal.Width, xlim = c(0, 10), ylim = c(0, 10))

# How to plot a curve (a parabola)
x <- seq(-1, 1, l = 50)
y <- x^2
plot(x, y)
plot(x, y, main = "A dotted parabola")
plot(x, y, main = "A parabola", type = "l")
plot(x, y, main = "A red and thick parabola", type = "l", col = "red", lwd = 3)

# Plotting a more complicated curve between -pi and pi
x <- seq(-pi, pi, l = 50)
y <- (2 + sin(10 * x)) * x^2
plot(x, y, type = "l") # Kind of rough...

# More detailed plot
x <- seq(-pi, pi, l = 500)
y <- (2 + sin(10 * x)) * x^2
plot(x, y, type = "l")

# Remember that we are joining points for creating a curve!

# For more options in the plot customization see
?plot
?par

# plot is a first level plotting function. That means that whenever is called,
# it creates a new plot. If we want to add information to an existing plot, we
# have to use a second level plotting function such as points, lines or abline

plot(x, y) # Create a plot
lines(x, x^2, col = "red") # Add lines
points(x, y + 10, col = "blue") # Add points
abline(a = 5, b = 1, col = "orange", lwd = 2) # Add a straight line y = a + b * x
```

### Distributions

The operations on distributions described here are implemented in `R Commander` through the menu `'Distributions'`, but is convenient for you to grasp how are they working.

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# R allows to sample [r], compute density/probability mass [d],
# compute distribution function [p] and compute quantiles [q] for several
# continuous and discrete distributions. The format employed is [rdpq]name,
# where name stands for:
# - norm -> Normal
# - unif -> Uniform
# - exp -> Exponential
# - t -> Student's t
# - f -> Snedecor's F
# - chisq -> Chi squared
# - pois -> Poisson
# - binom -> Binomial
# More distributions:
?Distributions

# Sampling from a Normal - 100 random points from a N(0, 1)
rnorm(n = 10, mean = 0, sd = 1)

# If you want to have always the same result, set the seed of the random number
# generator
set.seed(45678)
rnorm(n = 10, mean = 0, sd = 1)

# Plotting the density of a N(0, 1) - the Gauss bell
x <- seq(-4, 4, l = 100)
y <- dnorm(x = x, mean = 0, sd = 1)
plot(x, y, type = "l")

# Plotting the distribution function of a N(0, 1)
x <- seq(-4, 4, l = 100)
y <- pnorm(q = x, mean = 0, sd = 1)
plot(x, y, type = "l")

# Computing the 95% quantile for a N(0, 1)
qnorm(p = 0.95, mean = 0, sd = 1)

# All distributions have the same syntax: rname(n,...), dname(x,...), dname(p,...)  
# and qname(p,...), but the parameters in ... change. Look them in ?Distributions
# For example, here is que same for the uniform distribution

# Sampling from a U(0, 1)
set.seed(45678)
runif(n = 10, min = 0, max = 1)

# Plotting the density of a U(0, 1)
x <- seq(-2, 2, l = 100)
y <- dunif(x = x, min = 0, max = 1)
plot(x, y, type = "l")

# Computing the 95% quantile for a U(0, 1)
qunif(p = 0.95, min = 0, max = 1)

# Sampling from a Bi(10, 0.5)
set.seed(45678)
samp <- rbinom(n = 200, size = 10, prob = 0.5)
table(samp) / 200

# Plotting the probability mass of a Bi(10, 0.5)
x <- 0:10
y <- dbinom(x = x, size = 10, prob = 0.5)
plot(x, y, type = "h") # Vertical bars

# Plotting the distribution function of a Bi(10, 0.5)
x <- 0:10
y <- pbinom(q = x, size = 10, prob = 0.5)
plot(x, y, type = "h")

```
```{block, type = 'rmdexercise'}
Do the following:

- Compute the 90%, 95% and 99% quantiles of a $F$ distribution with `df1 = 1` and `df2 = 5`. (Answer: `c(4.060420, 6.607891, 16.258177)`)
- Plot the distribution function of a $U(0,1)$. Does it make sense with its density function?
- Sample 100 points from a Poisson with `lambda = 5`.
- Sample 100 points from a $U(-1,1)$ and compute its mean.
- Plot the density of a $t$ distribution with `df = 1` (use a sequence spanning from `-4` to `4`). Add lines of different colors with the densities for `df = 5`, `df = 10`, `df = 50` and `df = 100`. Do you see any pattern?
```

### Defining functions

```{r, echo = TRUE, collapse = TRUE, error = TRUE, cache = TRUE}
# A function is a way of encapsulating a block of code so it can be reused easily
# They are useful for simplifying repetitive tasks and organize the analysis
# For example, in Setion 3.7 we had to make use of simpleAnova for computing
# the simple ANOVA table in multiple regression.

# This is a silly function that takes x and y and returns its sum
add <- function(x, y) {
  x + y
}

# Calling add - you need to run the definition of the function first!
add(1, 1)
add(x = 1, y = 2)

# A more complex function: computes a linear model and its posterior summary.
# Saves us a few keystrokes when computing a lm and a summary
lmSummary <- function(formula, data) {
  model <- lm(formula = formula, data = data)
  summary(model)
}

# Usage
lmSummary(Sepal.Length ~ Petal.Width, iris)

# Recall: there is no variable called model in the workspace.
# The function works on its own workspace!
model

# Add a line to a plot
addLine <- function(x, beta0, beta1) {
  lines(x, beta0 + beta1 * x, lwd = 2, col = 2)
}

# Usage
plot(x, y)
addLine(x, beta0 = 0.1, beta1 = 0)
```


## Examples and applications {#log-reg-applications}

### Case study: *The Challenger disaster* {#log-reg-challenger}

The *Challenger* disaster occurred on the 28th January of 1986, when the NASA Space Shuttle orbiter *Challenger* broke apart and disintegrated at 73 seconds into its flight, leading to the deaths of its seven crew members. The accident deeply shocked the US society, in part due to the attention the mission had received because of the presence of Christa McAuliffe, who would have been the first astronaut-teacher. Because of this, NASA TV broadcasted live the launch to US public schools, which resulted in millions of school children witnessing the accident. The accident had serious consequences for the NASA credibility and resulted in an interruption of 32 months in the shuttle program. The Presidential *Rogers Commission* (formed by astronaut Neil A. Armstrong and Nobel laureate Richard P. Feynman, among others) was created to investigate the disaster.

```{r, video3, echo = FALSE, fig.align = 'center', fig.cap = 'Challenger launch and posterior explosion, as broadcasted live by NBC in 28/01/1986.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE}
knitr::include_url("https://www.youtube.com/embed/fSTrmJtHLFU")
```

The Rogers Commission elaborated a report [@Roberts1986] with all the findings. The commission determined that the disintegration began with the **failure of an O-ring seal in the solid rocket motor due to the unusual cold temperatures (-0.6 Celsius degrees)** during the launch. This failure produced a breach of burning gas through the solid rocket motor that compromised the whole shuttle structure, resulting in its disintegration due to the extreme aerodynamic forces. The **problematic with O-rings was something known**: the night before the launch, there was a three-hour teleconference between motor engineers and NASA management, discussing the effect of low temperature forecasted for the launch on the O-ring performance. The conclusion, influenced by Figure \@ref(fig:rogerts)a, was:

> "Temperature data [are] not conclusive on predicting primary O-ring blowby."

```{r, rogerts, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Number of incidents in the O-rings (fied joints) versus temperatures. Panel *a* includes only flights with incidents. Panel *b* contains all flights (with and without incidents).', fig.show = 'hold', cache = TRUE}
knitr::include_graphics("images/figures/challenger.png")
```

The Rogers Commission noted a major flaw in Figure \@ref(fig:rogerts)a: the **flights with zero incidents were excluded** from the plot because *it was felt* that **these flights did not contribute any information about the temperature effect** (Figure \@ref(fig:rogerts)b). The Rogers Commission concluded:

> "A careful analysis of the flight history of O-ring performance would have revealed the correlation of O-ring damage in low temperature".

The purpose of this case study, inspired by @Dalal1989, is to quantify what was the influence of the temperature in the probability of having at least one incident related with the O-rings. Specifically, we want to address the following questions:

- Q1. *Is the temperature associated with O-ring incidents?*
- Q2. *In which way was the temperature affecting the probability of O-ring incidents?*
- Q3. *What was the predicted probability of an incidient in an O-ring for the temperature of the launch day?*

To try to answer these questions we have the `challenger` dataset ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/challenger.txt)). The dataset contains (shown in Table \@ref(tab:challengertable)) information regarding the state of the solid rocket boosters after launch^[After the shuttle exits the atmosphere, the solid rocket boosters separate and descend to land using a parachute where they are carefully analysed.] for 23 flights. Each row has, among others, the following variables:

- `fail.field`, `fail.nozzle`: binary variables indicating whether there was an incident with the O-rings in the field joints or in the nozzles of the solid rocket boosters. `1` codifies an incident and `0` its absence. On the analysis, we focus on the O-rings of the field joint as being the most determinants for the accident.
- `temp`: temperature in the day of launch. Measured in Celsius degrees.
- `pres.field`,	`pres.nozzle`: leak-check pressure tests of the O-rings. These tests assured that the rings would seal the joint.

```{r, challengertable, echo = FALSE, out.width = '90%', fig.align = 'center', cache = TRUE}
challenger <- read.table(file = "datasets/challenger.txt", header = TRUE, sep = "\t")
knitr::kable(
  challenger[, c(1, 2, 5:7)],
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'The `challenger` dataset.'
)
```

Let's begin the analysis by replicating Figures \@ref(fig:rogerts)a and \@ref(fig:rogerts)b and checking that linear regression is not the right tool for answering Q1--Q3. For that, we make two scatterplots of `nfails.field` (number of total incidents in the field joints) versus `temp`, the first one excluding the launches without incidents (`subset = nfails.field > 0`) and the second one for all the data. Doing it through `R Commander` as we saw in Chapter \@ref(simp), you should get something similar to:
```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
library(car)
```
```{r, collapse = TRUE, cache = TRUE}
scatterplot(nfails.field ~ temp, reg.line = lm, smooth = FALSE, spread = FALSE,
            boxplots = FALSE, data = challenger, subset = nfails.field > 0)
scatterplot(nfails.field ~ temp, reg.line = lm, smooth = FALSE, spread = FALSE,
            boxplots = FALSE, data = challenger)
```

There is a fundamental problem in using linear regression for this data: **the response is not continuous**. As a consequence, there is no linearity and the errors around the mean are not normal (indeed, they are strongly non normal). Let's check this with the corresponding diagnostic plots:
```{r, collapse = TRUE, cache = TRUE}
mod <- lm(nfails.field ~ temp, data = challenger)
par(mfrow = 1:2)
plot(mod, 1)
plot(mod, 2)
```

Albeit linear regression is not the adequate tool for this data, it is able to detect the obvious difference between the two plots:

1. **The trend for launches with incidents is flat, hence suggesting there is no dependence on the temperature** (Figure \@ref(fig:rogerts)a). This was one of the arguments behind NASA's decision of launching the rocket at a temperature of -0.6 degrees.
2. However, **the trend for *all* launches indicates a clear negative dependence between temperature and number of incidents!** (Figure \@ref(fig:rogerts)b). Think about it in this way: the minimum temperature for a launch without incidents ever recorded was above 18 degrees, and the Challenger was launched at -0.6 without clearly knowing the effects of such low temperatures.

Instead of trying to predict the number of incidents, we will concentrate on modelling the *probability of expecting at least one incident given the temperature*, a simpler but also revealing approach. In other words, we look to estimate the following curve:
$$
p(x)=\mathbb{P}(\text{incident}=1|\text{temperature}=x)
$$
from `fail.field` and `temp`. This probability can not be properly modeled as a linear function like $\beta_0+\beta_1x$, since inevitably will fall outside $[0,1]$ for some value of $x$ (some will have negative probabilities or probabilities larger than one). The technique that solves this problem is the **logistic regression**. The idea behind is quite simple: transform a linear model $\beta_0+\beta_1x$ -- which is aimed for a response in $\mathbb{R}$ -- so that it yields a value in $[0,1]$. This is achieved by the *logistic function*
\begin{align}
\text{logistic}(t)=\frac{e^t}{1+e^t}=\frac{1}{1+e^{-t}}.(\#eq:log)
\end{align}
The logistic model in this case is
$$
\mathbb{P}(\text{incident}=1|\text{temperature}=x)=\text{logistic}\left(\beta_0+\beta_1x\right)=\frac{1}{1+e^{-(\beta_0+\beta_1x)}},
$$
with $\beta_0$ and $\beta_1$ unknown. Let's fit the model to the data by estimating $\beta_0$ and $\beta_1$.

In order to fit a logistic regression to the data, go to `'Statistics' -> 'Fit models' -> 'Generalized linear model...'`. A window like Figure \@ref(fig:glm) will pop-up, which you should fill as indicated.

```{r, glm, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Window for performing logistic regression.', cache = TRUE}
knitr::include_graphics("images/screenshots/glm.png")
```
A code like this will be generated:
```{r, collapse = TRUE, cache = TRUE}
nasa <- glm(fail.field ~ temp, family = "binomial", data = challenger)
summary(nasa)
exp(coef(nasa)) # Exponentiated coefficients ("odds ratios")
```
The summary of the logistic model is notably different from the linear regression, as the methodology behind is quite different. Nevertheless, we have tests for the significance of each coefficient. Here we obtain that `temp` is significantly different from zero, at least at a level $\alpha=0.05$. Therefore we can conclude that **the temperature is indeed affecting the probability of an incident with the O-rings** (answers Q1).

The precise interpretation of the coefficients will be given in the next section. For now, the coefficient of `temp`, $\hat\beta_1$, can be regarded the "correlation between the temperature and the probability of having at least one incident". These correlation, as evidenced by the sign of $\hat\beta_1$, is negative. Let's plot the fitted logistic curve to see that indeed the probability of incident and temperature are negatively correlated:
```{r, collapse = TRUE, cache = TRUE}
# Plot data
plot(challenger$temp, challenger$fail.field, xlim = c(-1, 30), xlab = "Temperature",
     ylab = "Incident probability")

# Draw the fitted logistic curve
x <- seq(-1, 30, l = 200)
y <- exp(-(nasa$coefficients[1] + nasa$coefficients[2] * x))
y <- 1 / (1 + y)
lines(x, y, col = 2, lwd = 2)

# The Challenger
points(-0.6, 1, pch = 16)
text(-0.6, 1, labels = "Challenger", pos = 4)
```
At the sight of this curve and the summary of the model we can conclude that **the temperature was increasing the probability of an O-ring incident** (Q2). Indeed, the confidence intervals for the coefficients show a significative negative correlation at level $\alpha=0.05$:
```{r, collapse = TRUE, message = FALSE, cache = TRUE}
confint(nasa, level = 0.95)
```
Finally, **the probability of having at least one incident with the O-rings in the launch day was $0.9996$ according to the fitted logistic model** (Q3). This is easily obtained:
```{r, collapse = TRUE, cache = TRUE}
predict(nasa, newdata = data.frame(temp = -0.6), type = "response")
```
Be aware that `type = "response"` has a different meaning in logistic regression. As you can see it does not return a CI for the prediction as in linear models. Instead, `type = "response"` means that the *probability* should be returned, instead of the value of the link function, which is returned with `type = "link"` (the default).

Recall that there is a serious problem of **extrapolation** in the prediction, which makes it less precise (or more variable). But this extrapolation, together with the evidences raised by a simple analysis like we did, should have been strong arguments for postponing the launch.

To conclude this section, we refer to a funny and comprehensive exposition by Juan Cuesta (University of Cantabria) on the flawed statistical analysis that contributed to the Challenger disaster.
```{r, video4, echo = FALSE, fig.align = 'center', fig.cap = 'The Challenger disaster and other elegant applications of statistics in complex problems.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE}
knitr::include_url("https://www.youtube.com/embed/MOeDbQlF5Tw?start=552")
```


## Model formulation and estimation by maximum likelihood {#log-reg-model-estimation}

As saw in Section \@ref(modelmult), the multiple linear model described the relation between the random variables $X_1,\ldots,X_k$ and $Y$ by assuming the linear relation
\begin{align*}
Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_k X_k + \varepsilon.
\end{align*}
Since we assume $\mathbb{E}[\varepsilon|X_1=x_1,\ldots,X_k=x_k]=0$, the previous equation was equivalent to
\begin{align}
\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k]=\beta_0+\beta_1x_1+\ldots+\beta_kx_k,(\#eq:eq-lm)
\end{align}
where $\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k]$ is the mean of $Y$ for a particular value of the set of predictors. As remarked in Section \@ref(assumptionsmult), it was a *necessary condition that $Y$ was continuous in order to satisfy the normality of the errors*, hence the linear model assumptions. Or in other words, **the linear model is designed for a continuous response**.

The situation when $Y$ is *discrete* (naturally ordered values; e.g. number of fails, number of students) or *categorical* (non-ordered categories; e.g. territorial divisions, ethnic groups) requires a special treatment. The simplest situation is when $Y$ is *binary* (or *dichotomic*): it can only take two values, codified for convenience as $1$ (success) and $0$ (failure). For example, in the Challenger case study we used `fail.field` as an *indicator* of whether "there was at least an incident with the O-rings" (`1` = yes, `0` = no). For binary variables there is no fundamental distinction between the treatment of discrete and categorical variables.

More formally, a binary variable is known as a *Bernoulli variable*, which is the simplest non-trivial random variable. We say that $Y\sim\mathrm{Ber}(p)$, $0\leq p\leq1$, if
\[
Y=\left\{\begin{array}{ll}1,&\text{with probability }p,\\0,&\text{with probability }1-p,\end{array}\right.
\]
or, equivalently, if $\mathbb{P}[Y=1]=p$ and $\mathbb{P}[Y=0]=1-p$, which can be written compactly as
\begin{align}
\mathbb{P}[Y=y]=p^y(1-p)^{1-y},\quad y=0,1.(\#eq:ber)
\end{align}
Recall that a *binomial variable with size $n$ and probability $p$*, $\mathrm{Bi}(n,p)$, is obtained by summing $n$ independent $\mathrm{Ber}(p)$ (so $\mathrm{Ber}(p)$ is the same as $\mathrm{Bi}(1,p)$). This is why we need to use a `family = "binomial"` in `glm`, to indicate that the response is binomial.

```{block, type='rmdinsight'}
A Bernoulli variable $Y$ is completely determined by the probability $p$. **So do its mean and variance**:

- $\mathbb{E}[Y]=p\times1+(1-p)\times0=p$
- $\mathbb{V}\mathrm{ar}[Y]=p(1-p)$

In particular, recall that $\mathbb{P}[Y=1]=\mathbb{E}[Y]=p$.

This is something relatively uncommon (or a $\mathcal{N}(\mu,\sigma^2)$, $\mu$ determines the mean and $\sigma^2$ the variance) that has important consequences for the logistic model: we do not need a $\sigma^2$.
```

```{block, type='rmdexercise'}
Are these Bernoulli variables? If so, which is the value of $p$ and what could the codes $0$ and $1$ represent?

- The toss of a fair coin.
- A variable with mean $p$ and variance $p(1-p)$.
- The roll of a dice.
- A binary variable with mean $0.5$ and variance $0.45$.
- The winner of an election with two candidates.

```

Assume then that $Y$ is a binary/Bernoulli variable and that $X_1,\ldots,X_k$ are predictors associated to $Y$ (no particular assumptions on them). The purpose in *logistic regression* is to estimate
\[
p(x_1,\ldots,x_k)=\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k]=\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k],
\]
this is, how the probability of $Y=1$ is changing according to particular values, denoted by $x_1,\ldots,x_k$, of the random variables $X_1,\ldots,X_k$. $p(x_1,\ldots,x_k)=\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k]$ stands for the *conditional probability of $Y=1$ given $X_1,\ldots,X_k$*. At sight of \@ref(eq:eq-lm), a tempting possibility is to consider the model
\[
p(x_1,\ldots,x_k)=\beta_0+\beta_1x_1+\ldots+\beta_kx_k.
\]
However, such a model will run into serious problems inevitably: negative probabilities and probabilities larger than one. The solution is to consider a function to encapsulate the value of $z=\beta_0+\beta_1x_1+\ldots+\beta_kx_k$, in $\mathbb{R}$, and map it back to $[0,1]$. There are several alternatives to do so, based on distribution functions $F:\mathbb{R}\longrightarrow[0,1]$ that deliver $y=F(z)\in[0,1]$ (see Figure \@ref(fig:logitprobit)). Different choices of $F$ give rise to different models:

- **Uniform**. Truncate $z$ to $0$ and $1$ when $z<0$ and $z>1$, respectively.
- **Logit**. Consider the *logistic distribution function*:
\[
F(z)=\mathrm{logistic}(z)=\frac{e^z}{1+e^z}=\frac{1}{1+e^{-z}}.
\]
- **Probit**. Consider the *normal* distribution function, this is, $F=\Phi$.

```{r, logitprobit, echo = FALSE, results = 'hide', out.width = '70%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Different transformations mapping the response of a simple linear regression $z=\\beta_0+\\beta_1x$ to $[0,1]$.', cache = TRUE}
# Regression
x <- seq(-3, 3, l = 200)
z <- 2 * x

# Truncation
z1 <- z
z1[z1 > 1] <- 1
z1[z1 < 0] <- 0

# Logit
z2 <- 1 / (1 + exp(-z))

# Probit
z3 <- pnorm(z)

# Comparison
plot(x, z, type = "l", ylim = c(-0.1, 1.1), ylab = "Probability", lwd = 2)
lines(x, z1, col = "blue", lwd = 2)
lines(x, z2, col = "red", lwd = 2)
lines(x, z3, col = "green", lwd = 2)
legend("topleft", legend = c("Linear regression", "Uniform", "Logit", "Probit"), lwd = 2,
       col = c("black", "blue", "red", "green"))
```

The **logistic** transformation is the most employed due to its **tractability, interpretability and smoothness**. Its inverse, $F^{-1}:[0,1]\longrightarrow\mathbb{R}$, known as the *logit function*, is
\[
\mathrm{logit}(p)=\mathrm{logistic}^{-1}(p)=\log\frac{p}{1-p}.
\]
This is a *link function*, this is, a function that maps a given space (in this case $[0,1]$) into $\mathbb{R}$. The term link function is employed in *generalized linear models*, which follow exactly the same philosophy of the logistic regression -- mapping the domain of $Y$ to $\mathbb{R}$ in order to apply there a linear model. We will concentrate here exclusively on the logit as a link function. Therefore, the *logistic model* is
\begin{align}
p(x_1,\ldots,x_k)=\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)}}.(\#eq:eq-log)
\end{align}
The linear form inside the exponent has a clear interpretation:

- If $\beta_0+\beta_1x_1+\ldots+\beta_kx_k=0$, then $p(x_1,\ldots,x_k)=\frac{1}{2}$ ($Y=1$ and $Y=0$ are equally likely).
- If $\beta_0+\beta_1x_1+\ldots+\beta_kx_k<0$, then $p(x_1,\ldots,x_k)<\frac{1}{2}$ ($Y=1$ less likely).
- If $\beta_0+\beta_1x_1+\ldots+\beta_kx_k>0$, then $p(x_1,\ldots,x_k)>\frac{1}{2}$ ($Y=1$ more likely).

To be more precise on the interpretation of the coefficients $\beta_0,\ldots,\beta_k$ we need to introduce the *odds*. The **odds is an equivalent way of expressing the distribution of probabilities in a binary variable**. Since $\mathbb{P}[Y=1]=p$ and $\mathbb{P}[Y=0]=1-p$, both the success and failure probabilities can be inferred from $p$. Instead of using $p$ to characterize the distribution of $Y$, we can use
\begin{align}
\mathrm{odds}(Y)=\frac{p}{1-p}=\frac{\mathbb{P}[Y=1]}{\mathbb{P}[Y=0]}.(\#eq:eq-odds)
\end{align}
The odds is the *ratio between the probability of success and the probability of failure*. It is extensively used in betting^[Recall that the result of a bet is binary: you win or lose the bet.] due to its better interpretability. For example, if a horse $Y$ has a probability $p=2/3$ of winning a race ($Y=1$), then the odds of the horse is
\[
\text{odds}=\frac{p}{1-p}=\frac{2/3}{1/3}=2.
\]
This means that the horse has a *probability of winning that is twice larger than the probability of losing*. This is sometimes written as a $2:1$ or $2 \times 1$ (spelled "two-to-one"). Conversely, if the odds of $Y$ is given, we can easily know what is the probability of success $p$, using the inverse of \@ref(eq:eq-odds):
\[
p=\mathbb{P}[Y=1]=\frac{\text{odds}(Y)}{1+\text{odds}(Y)}.
\]
For example, if the odds of the horse were $5$, that would correspond to a probability of winning $p=5/6$.

```{block, type ='rmdinsight'}
Recall that the odds is a number in $[0,+\infty]$. The $0$ and $+\infty$ values are attained for $p=0$ and $p=1$, respectively. The log-odds (or logit) is a number in $[-\infty,+\infty]$.
```
We can rewrite \@ref(eq:eq-log) in terms of the odds \@ref(eq:eq-odds). If we do so, we have:
\begin{align}
\mathrm{odds}(Y|X_1=x_1,\ldots,X_k=x_k)&=\frac{p(x_1,\ldots,x_k)}{1-p(x_1,\ldots,x_k)}\nonumber\\
&=e^{\beta_0+\beta_1x_1+\ldots+\beta_kx_k}\nonumber\\
&=e^{\beta_0}e^{\beta_1x_1}\ldots e^{\beta_kx_k}.(\#eq:eq-odds-log1)
\end{align}
or, taking logarithms, the *log-odds* (or logit)
\begin{align}
\log(\mathrm{odds}(Y|X_1=x_1,\ldots,X_k=x_k))=\beta_0+\beta_1x_1+\ldots+\beta_kx_k.(\#eq:eq-odds-log2)
\end{align}
The conditional log-odds \@ref(eq:eq-odds-log2) plays here the role of the conditional mean for multiple linear regression. Therefore, we have an analogous interpretation for the coefficients:

- $\beta_0$: is the log-odds when $X_1=\ldots=X_k=0$.
- $\beta_j$, $1\leq j\leq k$: is the **additive** increment of the log-odds for an increment of one unit in $X_j=x_j$, provided that the remaining variables $X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_k$ *do not change*.

The log-odds is not so easy to interpret as the odds. For that reason, an equivalent way of interpreting the coefficients, this time based on \@ref(eq:eq-odds-log1), is:

- $e^{\beta_0}$: is the odds when $X_1=\ldots=X_k=0$.
- $e^{\beta_j}$, $1\leq j\leq k$: is the **multiplicative** increment of the odds for an increment of one unit in $X_j=x_j$, provided that the remaining variables $X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_k$ *do not change*. If the increment in $X_j$ is of $r$ units, then the multiplicative increment in the odds is $(e^{\beta_j})^r$.

As a consequence of this last interpretation, we have:
```{block2, type = 'rmdinsight'}
If $\beta_j>0$ (respectively, $\beta_j<0$) then $e^{\beta_j}>1$ ($e^{\beta_j}<1$) in \@ref(eq:eq-odds-log1). Therefore, an increment of one unit in $X_j$, provided that the remaining variables $X_1,\ldots,X_{j-1},X_{j+1},\ldots,X_k$ do not change, results in an increment (decrement) of the odds, this is, in an increment (decrement) of $\mathbb{P}[Y=1]$.
```

```{block, type='rmdcaution'}
Since the relationship between $p(X_1,\ldots,X_k)$ and $X_1,\ldots,X_k$ is not linear, **$\beta_j$ does not correspond to the change in $p(X_1,\ldots,X_k)$ associated with a one-unit increase in $X_j$**.
```

Let's visualize this concepts quickly with the output of the Challenger case study:
```{r, collapse = TRUE, cache = TRUE}
nasa <- glm(fail.field ~ temp, family = "binomial", data = challenger)
summary(nasa)
exp(coef(nasa)) # Exponentiated coefficients ("odds ratios")

# Plot data
plot(challenger$temp, challenger$fail.field, xlim = c(-1, 30), xlab = "Temperature",
     ylab = "Incident probability")

# Draw the fitted logistic curve
x <- seq(-1, 30, l = 200)
y <- exp(-(nasa$coefficients[1] + nasa$coefficients[2] * x))
y <- 1 / (1 + y)
lines(x, y, col = 2, lwd = 2)

# The Challenger
points(-0.6, 1, pch = 16)
text(-0.6, 1, labels = "Challenger", pos = 4)
```

The exponentials of the estimated coefficients are:

- $e^{\hat\beta_0}=1965.974$. This means that, *when the temperature is zero*, the fitted odds is $1965.974$, so the probability of having an incident ($Y=1$) is $1965.974$ times larger than the probability of not having an incident ($Y=0$). In other words, the probability of having an incident at temperature zero is $\frac{1965.974}{1965.974+1}=0.999$.
- $e^{\hat\beta_1}=0.659$. This means that each Celsius degree increment in the temperature multiplies the fitted odds by a factor of $0.659\approx\frac{2}{3}$, hence reducing it.

The estimation of $\boldsymbol{\beta}=(\beta_0,\beta_1,\ldots,\beta_k)$ from a sample $(\mathbf{X}_{1},Y_1),\ldots,(\mathbf{X}_{n},Y_n)$ is different than in linear regression. It is not based on minimizing the RSS but on the principle of *Maximum Likelihood Estimation* (MLE). MLE is based on the following *leitmotiv*: **what are the coefficients $\boldsymbol{\beta}$ that make the sample more likely?** Or in other words, **what coefficients make the model more probable, based on the sample**. Since $Y_i\sim \mathrm{Ber}(p(\mathbf{X}_i))$, $i=1,\ldots,n$, the likelihood of $\boldsymbol{\beta}$ is
\begin{align}
\text{lik}(\boldsymbol{\beta})=\prod_{i=1}^np(\mathbf{X}_i)^{Y_i}(1-p(\mathbf{X}_i))^{1-Y_i}.(\#eq:eq-lik)
\end{align}
$\text{lik}(\boldsymbol{\beta})$ is the **probability of the data based on the model**. Therefore, it is a number between $0$ and $1$. Its detailed interpretation is the following:

- $\prod_{i=1}^n$ appears because the sample elements are assumed to be independent and we are computing the probability of observing the whole sample $(\mathbf{X}_{1},Y_1),\ldots,(\mathbf{X}_{n},Y_n)$. This probability is equal to the product of the *probabilities of observing each $(\mathbf{X}_{i},Y_i)$*.
- $p(\mathbf{X}_i)^{Y_i}(1-p(\mathbf{X}_i))^{1-Y_i}$ is the probability of observing  $(\mathbf{X}_{i},Y_i)$, as given by \@ref(eq:ber). Remember that $p$ depends on $\boldsymbol{\beta}$ due to \@ref(eq:eq-log).

Usually, the log-likelihood is considered instead of the likelihood for stability reasons -- the estimates obtained are exactly the same and are
\[
\hat{\boldsymbol{\beta}}=\arg\max_{\boldsymbol{\beta}\in\mathbb{R}^{k+1}}\log \text{lik}(\boldsymbol{\beta}).
\]
Unfortunately, due to the non-linearity of the optimization problem there are no explicit expressions for $\hat{\boldsymbol{\beta}}$. These have to be obtained numerically by means of an iterative procedure (the number of iterations required is printed in the output of `summary`). In low sample situations with perfect classification, the iterative procedure may not converge.

Figure \@ref(fig:maximumlikelihood) shows how the log-likelihood changes with respect to the values for $(\beta_0,\beta_1)$ in three data patterns.
```{r, maximumlikelihood, echo = FALSE, fig.cap = 'The logistic regression fit and its dependence on $\\beta_0$ (horizontal displacement) and $\\beta_1$ (steepness of the curve). Recall the effect of the sign of $\\beta_1$ in the curve: if positive, the logistic curve has an \'s\' form; if negative, the form is a reflected \'s\'.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/log-maximum-likelihood/', height = '900px')
```

The data of the illustration has been generated with the following code:
```{r, cache = TRUE}
# Data
set.seed(34567)
x <- rnorm(50, sd = 1.5)
y1 <- -0.5 + 3 * x
y2 <- 0.5 - 2 * x
y3 <- -2 + 5 * x
y1 <- rbinom(50, size = 1, prob = 1 / (1 + exp(-y1)))
y2 <- rbinom(50, size = 1, prob = 1 / (1 + exp(-y2)))
y3 <- rbinom(50, size = 1, prob = 1 / (1 + exp(-y3)))

# Data
dataMle <- data.frame(x = x, y1 = y1, y2 = y2, y3 = y3)
```

Let's check that indeed the coefficients given by `glm` are the ones that maximize the likelihood given in the animation of Figure \@ref(fig:maximumlikelihood). We do so for `y ~ x1`.
```{r, echo = TRUE, collapse = TRUE, cache = TRUE}
mod <- glm(y1 ~ x, family = "binomial", data = dataMle)
mod$coefficients
```

```{block2, type = 'rmdexercise'}
For the regressions `y ~ x2` and `y ~ x3`, do the following:

- Check that $\boldsymbol{\beta}$ is indeed maximizing the likelihood as compared with Figure \@ref(fig:maximumlikelihood).
- plot the fitted logistic curve and compare it with the one in Figure \@ref(fig:maximumlikelihood).

```

```{block, type = 'rmdinsight'}
In linear regression we relied on least squares estimation, in other words, the minimization of the RSS. *Why do we need MLE in logistic regression and not least squares?* The answer is two-fold:

1. **MLE is asymptotically optimal** when estimating unknown parameters in a model. That means that when the sample size $n$ is large, it is **guaranteed to perform better than any other estimation method**. Therefore, considering a least squares approach for logistic regression will result in suboptimal estimates.
2. In **multiple linear regression**, due to the *normality* assumption, **MLE and least squares estimation coincide**. So MLE is hidden under the form of the least squares, which is a more intuitive estimation procedure. Indeed, the maximized likelihood $\text{lik}(\hat{\boldsymbol{\beta}})$ in the linear model and the RSS are intimately related.
```

```{block, type = 'rmdinsight'}
As in the linear model, **the inclusion of a new predictor changes the coefficient estimates of the logistic model**.  
```

## Assumptions of the model {#log-reg-assumptions}

Some probabilistic assumptions are required for performing inference on the model parameters $\boldsymbol\beta$ from the sample $(\mathbf{X}_1, Y_1),\ldots,(\mathbf{X}_n, Y_n)$. These assumptions are somehow simpler than the ones for linear regression.

```{r, logisticmodel, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The key concepts of the logistic model.', cache = TRUE}
knitr::include_graphics("images/R/logisticmodel.png")
```

The assumptions of the logistic model are the following:

i. **Linearity in the logit**^[An equivalent way of stating this assumption is $p(\mathbf{x})=\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)$.]: $\mathrm{logit}(p(\mathbf{x}))=\log\frac{
p(\mathbf{x})}{1-p(\mathbf{x})}=\beta_0+\beta_1x_1+\ldots+\beta_kx_k$.
ii. **Binariness**: $Y_1,\ldots,Y_n$ are binary variables.
iii. **Independence**: $Y_1,\ldots,Y_n$ are independent.

A good one-line summary of the logistic model is the following (independence is assumed)
\begin{align}
Y|(X_1=x_1,\ldots,X_k=x_k)&\sim\mathrm{Ber}\left(\mathrm{logistic}(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)\right)\nonumber\\
&=\mathrm{Ber}\left(\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)}}\right).(\#eq:condber)
\end{align}

There are three important points of the linear model assumptions  missing in the ones for the logistic model:

- *Why is homoscedasticity not required?* As seen in the previous section, Bernoulli variables are determined only by the probability of success, in this case $p(\mathbf{x})$. That determines also the variance, which is variable, so **there is heteroskedasticity**. In the linear model, we have to control $\sigma^2$ explicitly due to the higher flexibility of the normal.
- *Where are the errors? The errors played a fundamental role in the linear model assumptions, but are not employed in logistic regression.* The errors are not fundamental for building the linear model but just a helpful concept related to least squares. The linear model can be constructed without errors as \@ref(eq:condnorm), which has a logistic analogous in \@ref(eq:condber).
- *Why is normality not present?* A normal distribution is not adequate to replace the Bernoulli distribution in \@ref(eq:condber) since the response $Y$ *has to be binary* and the Normal or other continuous distribution would put yield illegal values for $Y$.


```{block, type='rmdinsight'}
Recall that:

- Nothing is said about the distribution of $X_1,\ldots,X_k$. They could be deterministic or random. They could be discrete or continuous.
- $X_1,\ldots,X_k$ are **not required to be independent** between them.
```

## Inference for model parameters {#log-reg-inference}

The assumptions on which the logistic model is constructed allow to specify what is the *asymptotic* distribution of the *random vector* $\hat{\boldsymbol{\beta}}$. Again, the distribution is derived conditionally on the sample predictors $\mathbf{X}_1,\ldots,\mathbf{X}_n$. In other words, we assume that the randomness of $Y$ comes only from $Y|(X_1=x_1,\ldots,X_k=x_k)\sim\mathrm{Ber}(p(\mathbf{x}))$ and not from the predictors. To denote this, we employ lowercase for the sample predictors $\mathbf{x}_1,\ldots,\mathbf{x}_n$.

There is an important difference between the inference results for the linear model and for logistic regression:

- **In linear regression the inference is exact**. This is due to the nice properties of the normal, least squares estimation and linearity. As a consequence, the distributions of the coefficients are perfectly known assuming that the assumptions hold.
- **In logistic regression the inference is asymptotic**. This means that the distributions of the coefficients are unknown except for large sample sizes $n$, for which we have **approximations**. The reason is the more complexity of the model in terms of non-linearity. This is the usual situation for the majority of the regression models.

### Distributions of the fitted coefficients {#log-reg-distributions}

The distribution of $\hat{\boldsymbol{\beta}}$ is given by the asymptotic theory of MLE:
\begin{align}
\hat{\boldsymbol{\beta}}\sim\mathcal{N}_{k+1}\left(\boldsymbol\beta,I(\hat{\boldsymbol{\beta}})^{-1}\right)
(\#eq:mle1)
\end{align}
where $\sim$ must be understood as *approximately distributed as [...] when $n\to\infty$* for the rest of this chapter. $I(\boldsymbol\beta)$ is known as the *Fisher information matrix*, and receives that name because *it measures the information available in the sample for estimating $\boldsymbol\beta$*. Therefore, the *larger* the matrix is, the more precise is the estimation of $\boldsymbol\beta$, because that results in smaller variances in \@ref(eq:mle1). The inverse of the Fisher information matrix is
\begin{align}
I(\hat{\boldsymbol{\beta}})^{-1}=(\mathbf{X}^T\mathbf{V}\mathbf{X})^{-1},
(\#eq:mle2)
\end{align}
where $\mathbf{V}$ is a diagonal matrix containing the *different* variances for each $Y_i$ (remember that $p(\mathbf{x})=1/(1+e^{-(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)})$):
\[
\mathbf{V}=\begin{pmatrix}
p(\mathbf{X}_1)(1-p(\mathbf{X}_1)) & & &\\
& p(\mathbf{X}_2)(1-p(\mathbf{X}_2)) & & \\
& & \ddots & \\
& & & p(\mathbf{X}_n)(1-p(\mathbf{X}_n))
\end{pmatrix}
\]
In the case of the multiple linear regression, $I(\hat{\boldsymbol{\beta}})^{-1}=\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$ (see \@ref(eq:normp)), so the presence of $\mathbf{V}$ here is revealing the heteroskedasticity of the model.

The interpretation of \@ref(eq:mle1) and \@ref(eq:mle2) give some useful insights on what concepts affect the quality of the estimation:

- **Bias**. The estimates are *asymptotically* unbiased.
- **Variance**. It depends on:

    - *Sample size $n$*. Hidden inside $\mathbf{X}^T\mathbf{V}\mathbf{X}$. As $n$ grows, the precision of the estimators increases.
    - *Weighted predictor sparsity $(\mathbf{X}^T\mathbf{V}\mathbf{X})^{-1}$*. The more *sparse* the predictor is (small $|(\mathbf{X}^T\mathbf{V}\mathbf{X})^{-1}|$), the more precise $\hat{\boldsymbol{\beta}}$ is.


```{block2, type='rmdinsight'}
**The precision of $\hat{\boldsymbol{\beta}}$ is affected by the value of $\boldsymbol{\beta}$**, which is hidden inside $\mathbf{V}$. This contrasts sharply with the linear model, where the precision of the least squares estimator was not affected by the value of the unknown coefficients (see \@ref(eq:normp)). The reason is partially due to the **heteroskedasticity** of logistic regression, which implies a dependence of the variance of $Y$ in the logistic curve, hence in $\boldsymbol{\beta}$.
```

```{r, lograndomcoefs, echo = FALSE, fig.cap = 'Illustration of the randomness of the fitted coefficients $(\\hat\\beta_0,\\hat\\beta_1)$ and the influence of $n$, $(\\beta_0,\\beta_1)$ and $s_x^2$. The sample predictors $x_1,\\ldots,x_n$ are fixed and new responses $Y_1,\\ldots,Y_n$ are generated each time from a logistic model $Y\\sim\\mathrm{Ber}(p(X))$.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/log-random/', height = '1000px')
```

Similar to linear regression, the problem with \@ref(eq:mle1) and \@ref(eq:mle2) is that *$\mathbf{V}$ is unknown* in practice because it depends on $\boldsymbol{\beta}$. Plugging-in the estimate $\hat{\boldsymbol{\beta}}$ to $\boldsymbol{\beta}$ in $\mathbf{V}$ results in $\hat{\mathbf{V}}$. Now we can use $\hat{\mathbf{V}}$ to get
\begin{align}
\frac{\hat\beta_j-\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)}\sim \mathcal{N}(0,1),\quad\hat{\mathrm{SE}}(\hat\beta_j)^2=v_j^2(\#eq:mle3)
\end{align}
where
\[
v_j\text{ is the $j$-th element of the diagonal of }(\mathbf{X}^T\hat{\mathbf{V}}\mathbf{X})^{-1}.
\]
The LHS of \@ref(eq:normp2) is the **Wald statistic** for $\beta_j$, $j=0,\ldots,k$. They are employed for building confidence intervals and hypothesis tests.

### Confidence intervals for the coefficients {#log-reg-confidence}

Thanks to \@ref(eq:mle3), we can have the $100(1-\alpha)\%$ CI for the coefficient $\beta_j$, $j=0,\ldots,k$:
\begin{align}
\left(\hat\beta_j\pm\hat{\mathrm{SE}}(\hat\beta_j)z_{\alpha/2}\right)(\#eq:ciplog)
\end{align}
where $z_{\alpha/2}$ is the *$\alpha/2$-upper quantile of the $\mathcal{N}(0,1)$*. In case we are interested in the CI for $e^{\beta_j}$, we can just simply take the exponential on the above CI. So the $100(1-\alpha)\%$ CI for $e^{\beta_j}$, $j=0,\ldots,k$, is
\[
e^{\left(\hat\beta_j\pm\hat{\mathrm{SE}}(\hat\beta_j)z_{\alpha/2}\right)}.
\]
Of course, this CI is **not** the same as $\left(e^{\hat\beta_j}\pm e^{\hat{\mathrm{SE}}(\hat\beta_j)z_{\alpha/2}}\right)$, which is not a CI for $e^{\hat\beta_j}$.

Let's see how we can compute the CIs. We return to the `challenger` dataset, so in case you do not have it loaded, you can download it [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/challenger.txt). We analyse the CI for the coefficients of `fail.field ~ temp`.
```{r, collapse = TRUE, cache = TRUE}
# Fit model
nasa <- glm(fail.field ~ temp, family = "binomial", data = challenger)

# Confidence intervals at 95%
confint(nasa)

# Confidence intervals at other levels
confint(nasa, level = 0.90)

# Confidence intervals for the factors affecting the odds
exp(confint(nasa))
```
In this example, the 95% confidence interval for $\beta_0$ is $(1.3364, 17.7834)$ and for $\beta_1$ is $(-0.9238, -0.1090)$. For $e^{\beta_0}$ and $e^{\beta_1}$, the CIs are $(3.8053, 5.2874\times10^7)$ and $(0.3070, 0.8967)$, respectively. Therefore, we can say with a 95% confidence that:

- When `temp`=0, the probability of `fail.field`=1 is significantly lager than the probability of `fail.field`=0 (using the CI for $\beta_0$). Indeed, `fail.field`=1 is between $3.8053$ and $5.2874\times10^7$ more likely than `fail.field`=0 (using the CI for $e^{\beta_0}$).
- `temp` has a significantly negative effect in the probability of `fail.field`=1  (using the CI for $\beta_1$). Indeed, each unit increase in `temp` produces a reduction of the odds of `fail.field` by a factor between $0.3070$ and $0.8967$ (using the CI for $e^{\beta_1}$).

```{block2, type = 'rmdexercise'}
Compute and interpret the CIs for the exponentiated coefficients, at level $\alpha=0.05$, for the following regressions (`challenger` dataset):

- `fail.field ~ temp + pres.field`
- `fail.nozzle ~ temp + pres.nozzle`
- `fail.field ~ temp + pres.nozzle`
- `fail.nozzle ~ temp + pres.field`

The interpretation of the variables is given above Table \@ref(tab:challengertable).
```

### Testing on the coefficients {#log-reg-testing}

The distributions in \@ref(eq:mle3) also allow to conduct a formal hypothesis test on the coefficients $\beta_j$, $j=0,\ldots,k$. For example, the test for significance:
\begin{align*}
H_0:\beta_j=0
\end{align*}
for $j=0,\ldots,k$. The test of $H_0:\beta_j=0$ with $1\leq j\leq k$ is specially interesting, since it allows to answer whether *the variable $X_j$ has a significant effect on $\mathbb{P}[Y=1]$*. The statistic used for testing for significance is the Wald statistic
\begin{align*}
\frac{\hat\beta_j-0}{\hat{\mathrm{SE}}(\hat\beta_j)},
\end{align*}
which is asymptotically distributed as a $\mathcal{N}(0,1)$ *under the (veracity of) the null hypothesis*. $H_0$ is tested *against* the *bilateral* alternative hypothesis $H_1:\beta_j\neq 0$.

The tests for significance are built-in in the `summary` function. However, a note of caution is required when applying the rule of thumb:

> Is the CI for $\beta_j$ below (above) $0$ at level $\alpha$?
>
>- Yes $\rightarrow$ reject $H_0$ at level $\alpha$.
>- No $\rightarrow$ the criterion is not conclusive.

```{block, type = 'rmdcaution'}
The significances given in `summary` and the output of `confint` are *slightly* incoherent and the previous rule of thumb **does not apply**. The reason is because `MASS`'s `confint` is using a more sophisticated method (profile likelihood) to estimate the standard error of $\hat\beta_j$, $\hat{\mathrm{SE}}(\hat\beta_j)$, and not the asymptotic distribution behind Wald statistic.

By changing `confint` to `R`'s default `confint.default`, the results of the latter will be completely equivalent to the significances in `summary`, and the rule of thumb still be completely valid. For the contents of this course we prefer `confint.default` due to its better interpretability.
```

To illustrate this we consider the regression of `fail.field ~ temp + pres.field`:
```{r, collapse = TRUE, cache = TRUE}
# Significances with asymptotic approximation for the standard errors
nasa2 <- glm(fail.field ~ temp + pres.field, family = "binomial",
             data = challenger)
summary(nasa2)

# CIs with asymptotic approximation - coherent with summary
confint.default(nasa2, level = 0.90)
confint.default(nasa2, level = 0.99)

# CIs with profile likelihood - incoherent with summary
confint(nasa2, level = 0.90) # intercept still significant
confint(nasa2, level = 0.99) # temp still significant
```

```{block, type = 'rmdexercise'}
For the previous exercise, check the differences of using `confint` or `confint.default` for computing the CIs.
```

## Prediction {#log-reg-prediction}

*Prediction* in logistic regression focuses mainly on predicting the values of the logistic curve
\[
p(x_1,\ldots,x_k)=\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k]=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+\ldots+\beta_kx_k)}}
\]
by means of
\[
\hat p(x_1,\ldots,x_k)=\hat{\mathbb{P}}[Y=1|X_1=x_1,\ldots,X_k=x_k]=\frac{1}{1+e^{-(\hat\beta_0+\hat\beta_1x_1+\ldots+\hat\beta_kx_k)}}.
\]
From the perspective of the linear model, this is the same as predicting the **conditional mean** (not the conditional response) of the response, but this time this conditional mean is also a **conditional probability**. The prediction of the conditional response is not so interesting since it follows immediately from $\hat p(x_1,\ldots,x_k)$:
\[
\hat{Y}|(X_1=x_1,\ldots,X_k=x_k)=\left\{\begin{array}{ll}1,&\text{with probability }\hat p(x_1,\ldots,x_k),\\0,&\text{with probability }1-\hat p(x_1,\ldots,x_k).\end{array}\right.
\]
As a consequence, we can predict $Y$ as $1$ if $\hat p(x_1,\ldots,x_k)>\frac{1}{2}$ and as $0$ if $\hat p(x_1,\ldots,x_k)<\frac{1}{2}$.

Let's focus then on how to make predictions and compute CIs in practice with `predict`. Similarly to the linear model, the objects required for `predict` are: first, the output of `glm`; second, a `data.frame` containing the locations $\mathbf{x}=(x_1,\ldots,x_k)$ where we want to predict $p(x_1,\ldots,x_k)$. However, there are **two differences** with respect to the use of `predict` for `lm`:

- The argument `type`. `type = "link"`, gives the predictions in the log-odds, this is, returns $\log\frac{\hat p(x_1,\ldots,x_k)}{1-\hat p(x_1,\ldots,x_k)}$. `type = "response"` gives the predictions in the probability space $[0,1]$, this is, returns $\hat p(x_1,\ldots,x_k)$.
- There is no `interval` argument for using `predict` for `glm`. That means that there is no easy way of computing CIs for prediction.

Since it is a bit cumbersome to compute by yourself the CIs, we can code the function `predictCIsLogistic` so that it compute them automatically for you, see below.
```{r, collapse = TRUE, cache = TRUE}
# Data for which we want a prediction
# Important! You have to name the column with the predictor name!
newdata <- data.frame(temp = -0.6)

# Prediction of the conditional log-odds - the default
predict(nasa, newdata = newdata, type = "link")

# Prediction of the conditional probability
predict(nasa, newdata = newdata, type = "response")

# Function for computing the predictions and CIs for the conditional probability
predictCIsLogistic <- function(object, newdata, level = 0.95) {

  # Compute predictions in the log-odds
  pred <- predict(object = object, newdata = newdata, se.fit = TRUE)

  # CI in the log-odds
  za <- qnorm(p = (1 - level) / 2)
  lwr <- pred$fit + za * pred$se.fit
  upr <- pred$fit - za * pred$se.fit

  # Transform to probabilities
  fit <- 1 / (1 + exp(-pred$fit))
  lwr <- 1 / (1 + exp(-lwr))
  upr <- 1 / (1 + exp(-upr))

  # Return a matrix with column names "fit", "lwr" and "upr"
  result <- cbind(fit, lwr, upr)
  colnames(result) <- c("fit", "lwr", "upr")
  return(result)

}

# Simple call
predictCIsLogistic(nasa, newdata = newdata)
# The CI is large because there is no data around temp = -0.6 and
# that makes the prediction more variable (and also because we only
# have 23 observations)
```

```{block, type = 'rmdexercise'}
For the `challenger` dataset, do the following:

- Regress `fail.nozzle` on `temp` and `pres.nozzle`.
- Compute the predicted probability of `fail.nozzle=1` for `temp`$=15$ and `pres.nozzle`$=200$. What is the predicted probability for `fail.nozzle=0`?
- Compute the confidence interval for the two predicted probabilities at level $95\%$.

```

Finally, Figure \@ref(fig:logcipred) gives an interactive visualization of the CIs for the conditional probability in simple logistic regression. Their interpretation is very similar to the CIs for the conditional mean in the simple linear model, see Section \@ref(prediction) and Figure \@ref(fig:cipred).

```{r, logcipred, echo = FALSE, fig.cap = 'Illustration of the CIs for the conditional probability in the simple logistic regression.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/log-ci-prediction/', height = '1000px')
```

## Deviance and model fit {#log-reg-deviance}

The **deviance** is a key concept in logistic regression. Intuitively, it measures the **deviance of the fitted logistic model with respect to a perfect model for $\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k]$**. This perfect model, known as the *saturated model*, denotes an abstract model that fits perfectly the sample, this is, the model such that
\[
\hat{\mathbb{P}}[Y=1|X_1=X_{i1},\ldots,X_k=X_{ik}]=Y_i,\quad i=1,\ldots,n.
\]
This model assigns probability $0$ or $1$ to $Y$ depending on the actual value of $Y_i$. To clarify this concept, Figure \@ref(fig:saturated) shows a saturated model and a fitted logistic regression.
```{r, saturated, echo = FALSE, results = 'hide', out.width = '70%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Fitted logistc regression versus *a* saturated model (several are possible depending on the interpolation between points) and the null model.', cache = TRUE}
set.seed(45671231)
x <- runif(50, 0, 1)
y <- rbinom(n = 50, size = 1, prob = 1/(1 + exp(-(1 - 6 * x))))
mod <- glm(y ~ x, family = "binomial")
xx <- seq(0, 1, l = 200)
o <- order(x)
plot(x, y, pch = 16)
lines(x[o], y[o], col = 4, lwd = 2)
lines(xx, 1/(1 + exp(-(mod$coefficients[1] + mod$coefficients[2] * xx))), col = 2, lwd = 2)
lines(xx, rep(sum(y == 1)/50, 200), col = 3, lwd = 3)
legend("topright", legend = c("Fitted logistic model", "A saturated model", "The null model"), lwd = 2, col = c(2, 4, 3))
```
More precisely, the deviance is defined as the difference of likelihoods between the fitted model and the saturated model:
\[
D=-2\log\text{lik}(\hat{\boldsymbol{\beta}})+2\log\text{lik}(\text{saturated model}).
\]
Since the likelihood of the saturated model is exactly one^[The probability of the sample according to the saturated is $1$ -- replace $p(\mathbf{X}_i)=Y_i$ in \@ref(eq:eq-lik).], then the deviance is simply another expression of the likelihood:
\[
D=-2\log\text{lik}(\hat{\boldsymbol{\beta}}).
\]
As a consequence, **the deviance is always larger or equal than zero, being zero only if the fit is perfect**.

A benchmark for evaluating the magnitude of the deviance is the **null deviance**,
\[
D_0=-2\log\text{lik}(\hat{\beta}_0),
\]
which is the **deviance of the worst model, the one fitted without any predictor, and the perfect model**:
\[
Y|(X_1=x_1,\ldots,X_k=x_k)\sim \mathrm{Ber}(\mathrm{logistic}(\beta_0)).
\]
In this case, $\hat\beta_0=\mathrm{logit}(\frac{m}{n})=\log\frac{\frac{m}{n}}{1-\frac{m}{n}}$ where $m$ is the number of $1$'s in $Y_1,\ldots,Y_n$ (see Figure \@ref(fig:saturated)).

The null deviance serves for comparing how much the model has improved by adding the predictors $X_1,\ldots,X_k$. This can be done by means of the **$R^2$ statistic**, which is a *generalization* of the determination coefficient in multiple linear regression:
\begin{align}
R^2=1-\frac{D}{D_0}=1-\frac{\text{deviance(fitted logistic, saturated model)}}{\text{deviance(null model, saturated model)}}.(\#eq:log-r2)
\end{align}
This global measure of fit is *similar* indeed shares some important properties with the  determination coefficient in linear regression:

1. It is a quantity between $0$ and $1$.
2. If the fit is perfect, then $D=0$ and $R^2=1$. If the predictors do not add anything to the regression, then $D=D_0$ and $R^2=0$.

```{block, type='rmdinsight'}
In logistic regression, $R^2$ does not have the same interpretation as in linear regression:

- Is **not the percentage of variance explained by the logistic model**, but rather a ratio indicating how close is the fit to being perfect or the worst.
- It is not related to any correlation coefficient.

```

```{block2, type='rmdinsight'}
The $R^2$ in \@ref(eq:log-r2) is valid for the whole family of *generalized linear models*, for which linear and logistic regression are particular cases. The connexion between \@ref(eq:log-r2) and the determination coefficient is given by the expressions of the deviance and null the deviance for the linear model:
\[
D=\mathrm{SSE}\text{ (or $D=\mathrm{RSS}$) and }D_0=\mathrm{SST}.
\]
```

Let's see how these concepts are given by the `summary` function:
```{r, collapse = TRUE, cache = TRUE}
# Summary of model
nasa <- glm(fail.field ~ temp, family = "binomial", data = challenger)
summaryLog <- summary(nasa)
summaryLog # 'Residual deviance' is the deviance; 'Null deviance' is the null deviance

# Null model - only intercept
null <- glm(fail.field ~ 1, family = "binomial", data = challenger)
summaryNull <- summary(null)
summaryNull

# Computation of the R^2 with a function - useful for repetitive computations
r2Log <- function(model) {

  summaryLog <- summary(model)
  1 - summaryLog$deviance / summaryLog$null.deviance

}

# R^2
r2Log(nasa)
r2Log(null)
```

Another way of evaluating the model fit is its **predictive accuracy**. The motivation is that most of the times we are interested simply in *classifying*, for an observation of the predictors, the value of $Y$ as either $0$ or $1$, but not in predicting the value of $p(x_1,\ldots,x_k)=\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k]$. The classification in prediction is simply done by the rule
\[
\hat{Y}=\left\{\begin{array}{ll}
1,&\hat{p}(x_1,\ldots,x_k)>\frac{1}{2},\\
0,&\hat{p}(x_1,\ldots,x_k)<\frac{1}{2}.
\end{array}\right.
\]
The overall predictive accuracy can be summarized with the **hit matrix**

| Reality vs. classified | $\hat Y=0$ | $\hat Y=1$ |
|----------|----------|----------|
| $Y=0$ | Correct$_{0}$ | Incorrect$_{01}$ |
| $Y=1$ | Incorrect$_{10}$ | Correct$_{1}$ |

and with the **hit ratio** $\frac{\text{Correct}_0+\text{Correct}_1}{n}$. The hit matrix is easily computed with the `table` function. The function, whenever called with two vectors, computes the cross-table between the two vectors.
```{r, collapse = TRUE, cache = TRUE}
# Fitted probabilities for Y = 1
nasa$fitted.values

# Classified Y's
yHat <- nasa$fitted.values > 0.5

# Hit matrix:
# - 16 correctly classified as 0
# - 4 correclty classified as 1
# - 3 incorrectly classified as 0
tab <- table(challenger$fail.field, yHat)
tab

# Hit ratio (ratio of correct classification)
(16 + 4) / 23 # Manually
sum(diag(tab)) / sum(tab) # Automatically
```

It is important to recall that the hit matrix will be always *biased towards unrealistc good classification rates* if it is computed in the same sample used for fitting the logistic model. A familiar analogy is asking to your mother (data) whether you (model) are a good-looking human being (good predictive accuracy) -- the answer will be highly positively biased. To get a fair hit matrix, the right approach is to split randomly the sample into two: a *training dataset*, used for fitting the model, and a *test dataset*, used for evaluating the predictive accuracy.

## Model selection and multicollinearity {#log-reg-selection}

The same discussion we did in Section \@ref(selection) is applicable to logistic regression with small changes:

1. The **deviance** of the model (reciprocally the likelihood and the $R^2$) **always decreases** (increase) with the inclusion of more predictors -- no matter whether they are significant or not.
2. The **excess of predictors** in the model is paid by a larger variability in the estimation of the model which results in less precise prediction.
3. **Multicollinearity** may hide significant variables, change the sign of them and result in an increase of the variability of the estimation at the expense of little improvement in .

The use of information criteria, `stepwise` and `vif` allow to efficiently fight back these issues. Let's review them quickly from the perspective of logistic regression.

First, remember that the BIC/AIC information criteria are based on a **balance between the model fitness, given by the likelihood, and its complexity**. In the logistic regression, the BIC is
\begin{align*}
\text{BIC}(\text{model}) &= -2\log \text{lik}(\hat{\boldsymbol{\beta}}) + (k + 1)\times\log n\\
&=D+(k+1)\times \log n,
\end{align*}
where $\text{lik}(\hat{\boldsymbol{\beta}})$ is the *likelihood of the model*. The AIC replaces $\log n$ by $2$, hence penalizing less model complexity. The BIC and AIC can be computed in `R` through the functions `BIC` and `AIC`, and we can check manually that they match with its definition.
```{r, collapse = TRUE, cache = TRUE}
# Models
nasa <- glm(fail.field ~ temp, family = "binomial", data = challenger)
nasa2 <- glm(fail.field ~ temp + pres.field, family = "binomial",
             data = challenger)

# nasa
summary1 <- summary(nasa)
summary1
BIC(nasa)
summary1$deviance + 2 * log(dim(challenger)[1])
AIC(nasa)
summary1$deviance + 2 * 2

# nasa2
summary2 <- summary(nasa2)
summary2
BIC(nasa2)
summary2$deviance + 3 * log(dim(challenger)[1])
AIC(nasa2)
summary2$deviance + 3 * 2
```
Second, `stepwise` works analogously to the linear regression situation. Here is an illustration for a binary variable that measures whether a Boston suburb (`Boston` dataset) is wealth or not. The binary variable is `medv > 25`: it is `TRUE` (`1`) for suburbs with median house value larger than 25000\$) and `FALSE` (`0`) otherwise. The cutoff 25000\$ corresponds to the 25\% richest suburbs.

```{r, collapse = TRUE, cache = TRUE}
# Boston dataset
data(Boston)

# Model whether a suburb has a median house value larger than 25000$
mod <- glm(I(medv > 25) ~ ., data = Boston, family = "binomial")
summary(mod)
r2Log(mod)

# With BIC - wnds up with only the significant variables and a similar R^2
modBIC <- stepwise(mod, trace = 0)
summary(modBIC)
r2Log(modBIC)
```

Finally, **multicollinearity can also be present in logistic regression**. Despite the nonlinear logistic curve, the predictors are combined linearly in \@ref(eq:eq-log). Due to this, if two or more predictors are highly correlated between them, the fit of the model will be compromised since the individual linear effect of each predictor is hard to disentangle from the rest of correlated predictors.

In addition to inspecting the correlation matrix and look for high correlations, a powerful tool to detect multicollinearity is the *generalized* Variance Inflation Factor (gVIF) of each coefficient $\hat\beta_j$. This is a *measure of how much the variability in the estimation has increased due to the addition of the predictor $X_j$*. The next rule of thumb gives direct insight into which predictors are multicollinear:

- gVIF close to 1: absence of multicollinearity.
- **gVIF larger than 5 or 10: problematic amount of multicollinearity**. Advised to remove the predictor with largest gVIF.

Here is an example illustrating the use of gVIF, through `vif`, in practice. It also shows also how the simple inspection of the covariance matrix is not enough for detecting collinearity in tricky situations.
```{r, collapse = TRUE, cache = TRUE}
# Create predictors with multicollinearity: x4 depends on the rest
set.seed(45678)
x1 <- rnorm(100)
x2 <- 0.5 * x1 + rnorm(100)
x3 <- 0.5 * x2 + rnorm(100)
x4 <- -x1 + x2 + rnorm(100, sd = 0.25)

# Response
z <- 1 + 0.5 * x1 + 2 * x2 - 3 * x3 - x4
y <- rbinom(n = 100, size = 1, prob = 1/(1 + exp(-z)))
data <- data.frame(x1 = x1, x2 = x2, x3 = x3, x4 = x4, y = y)

# Correlations - none seems suspicius
cor(data)

# Generalized variance inflation factors anormal: largest for x4, we remove it
modMultiCo <- glm(y ~ x1 + x2 + x3 + x4, family = "binomial")
vif(modMultiCo)

# Without x4
modClean <- glm(y ~ x1 + x2 + x3, family = "binomial")

# Comparison
summary(modMultiCo)
summary(modClean)
r2Log(modMultiCo)
r2Log(modClean)

# Genralized variance inflation factors normal
vif(modClean)
```

```{block, type = 'rmdexercise'}
For the `Boston` dataset, do the following:

1. Compute the hit matrix and hit ratio for the regression `I(medv > 25) ~ .` (hint: do `table(medv > 25, ...)`).
2. Fit `I(medv > 25) ~ .` but now using only the first 300 observations of `Boston`, the *training dataset* (hint: use `subset`).
3. For the previous model, predict the probability of the responses and classify them into `0` or `1` in the last 206 observations, the *testing dataset* (hint: use `predict` on that subset).
4. Compute the hit matrix and hit ratio for the new predictions. Check that the hit ratio is smaller than the one in the first point. The hit ratio on the testing dataset, and not the first hit rate, is an estimator of how well the model is going to classify future observations.

```

<!--
## Nonlinear relationships {#log-reg-nonlinear}

Assumptions: 3 plots everything fine, 3 plots wrong, focus on non-linearity on the logit

```{r, log-reg-transf, echo = FALSE, fig.cap = 'Illustration of the choice of the polynomial transformation.', screenshot.opts = list(delay = 15), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://ec2-35-177-34-200.eu-west-2.compute.amazonaws.com/log-reg-non-lin/', height = '1000px')
```
-->

<!--chapter:end:03-log-reg.Rmd-->

# Principal component analysis {#pca}

Principal Component Analysis (PCA) is a powerful multivariate technique designed to summarize the most important features and relations of $k$ numerical random variables $X_1,\ldots,X_k$. PCA does *dimension reduction* of the original dataset by computing a new set of variables, the principal components $\text{PC}_1,\ldots \text{PC}_k$, which explain the same information as $X_1,\ldots,X_k$ but in an *ordered* way: $\text{PC}_1$ explains the most of the information and $\text{PC}_k$ the least.

There is *no response* $Y$ or particular variable in PCA that deserves a particular attention -- all variables are treated equally.

## Examples and applications

### Case study: *Employement in European countries in the late 70s*

The purpose of this case study, motivated by @Hand1994 and @Bartholomew2008, is to reveal the structure of the job market and economy in different developed countries. The final aim is to have a meaningful and rigorous plot that is able to show the most important features of the countries in a concise form.

The dataset `eurojob` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/eurojob.txt)) contains the data employed in this case study. It contains the percentage of workforce employed in 1979 in 9 industries for 26 European countries. The industries measured are:

- Agriculture (`Agr`)
- Mining (`Min`)
- Manufacturing (`Man`)
- Power supply industries `(Pow`)
- Construction (`Con`)
- Service industries (`Ser`)
- Finance (`Fin`)
- Social and personal services (`Soc`)
- Transport and communications (`Tra`)

If the dataset is imported into `R` and the case names are set as `Country` (important in order to have only numerical variables), then the data should look like this:
```{r, eurotable, echo = FALSE, out.width = '90%', fig.align = 'center', cache = TRUE}
eurojob <- read.table(file = "datasets/eurojob.txt", header = TRUE)
knitr::kable(
  eurojob,
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'The `eurojob` dataset.'
)
row.names(eurojob) <- eurojob$Country
eurojob$Country <- NULL
```

So far, we know how to compute summaries for *each variable*, and how to quantify and visualize relations between variables with the correlation matrix and the scatterplot matrix. But even for a moderate number of variables like this, their results are hard to process.

```{r, collapse= TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Summary of the data - marginal
summary(eurojob)

# Correlation matrix
cor(eurojob)

# Scatterplot matrix
scatterplotMatrix(eurojob, reg.line = lm, smooth = FALSE, spread = FALSE,
                  span = 0.5, ellipse = FALSE, levels = c(.5, .9), id.n = 0,
                  diagonal = 'histogram')
```
We definitely need a way of visualizing and quantifying the relations between variables for a moderate to large amount of variables. PCA will be a handy way. In a nutshell, what PCA does is:

1. Takes the data for the variables $X_1,\ldots,X_k$.
2. Using this data, looks for new variables $\text{PC}_1,\ldots \text{PC}_k$ such that:
    - $\text{PC}_j$ is a **linear combination** of $X_1,\ldots,X_k$, $1\leq j\leq k$. This is, $\text{PC}_j=a_{1j}X_1+a_{2j}X_2+\ldots+a_{kj}X_k$.
    - $\text{PC}_1,\ldots \text{PC}_k$ are **sorted decreasingly in terms of variance**. Hence $\text{PC}_j$ has more variance than $\text{PC}_{j+1}$, $1\leq j\leq k-1$,
    - $\text{PC}_{j_1}$ and $\text{PC}_{j_2}$ are **uncorrelated**, for $j_1\neq j_2$.
    - $\text{PC}_1,\ldots \text{PC}_k$ have the **same information**, measured in terms of **total variance**, as $X_1,\ldots,X_k$.
3. Produces three key objects:
    - **Variances of the PCs**. They are sorted decreasingly and give an idea of which PCs are contain most of the information of the data (the ones with more variance).
    - **Weights of the variables in the PCs**. They give the interpretation of the PCs in terms of the original variables, as they are the coefficients of the linear combination. The weights of the variables $X_1,\ldots,X_k$ on the PC$_j$, $a_{1j},\ldots,a_{kj}$, are normalized: $a_{1j}^2+\ldots+a_{kj}^2=1$, $j=1,\ldots,k$. In `R`, they are called `loadings`.
    - **Scores of the data in the PCs**: this is the data with $\text{PC}_1,\ldots \text{PC}_k$ variables instead of $X_1,\ldots,X_k$. The **scores are uncorrelated**. Useful for knowing which PCs have more effect on a certain observation.

Hence, PCA rearranges our variables in an information-equivalent, but more convenient, layout where the variables are **sorted according to the ammount of information they are able to explain**. From this position, the next step is clear: **stick only with a limited number of PCs such that they explain most of the information** (e.g., 70\% of the total variance) and do *dimension reduction*. The effectiveness of PCA in practice varies from the structure present in the dataset. For example, in the case of highly dependent data, it could explain more than the 90\% of variability of a dataset with tens of variables with just two PCs.

Let's see how to compute a full PCA in `R`.
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# The main function - use cor = TRUE to avoid scale distortions
pca <- princomp(eurojob, cor = TRUE)

# What is inside?
str(pca)

# The standard deviation of each PC
pca$sdev

# Weights: the expression of the original variables in the PCs
# E.g. Agr = -0.524 * PC1 + 0.213 * PC5 - 0.152 * PC6 + 0.806 * PC9
# And also: PC1 = -0.524 * Agr + 0.347 * Man + 0256 * Pow + 0.325 * Con + ...
# (Because the matrix is orthogonal, so the transpose is the inverse)
pca$loadings

# Scores of the data on the PCs: how is the data reexpressed into PCs
head(pca$scores, 10)

# Scatterplot matrix of the scores - they are uncorrelated!
scatterplotMatrix(pca$scores, reg.line = lm, smooth = FALSE, spread = FALSE,
                  span = 0.5, ellipse = FALSE, levels = c(.5, .9), id.n = 0,
                  diagonal = 'histogram')

# Means of the variables - before PCA the variables are centered
pca$center

# Rescalation done to each variable
# - if cor = FALSE (default), a vector of ones
# - if cor = TRUE, a vector with the standard deviations of the variables
pca$scale

# Summary of the importance of components - the third row is key
summary(pca)

# Scree plot - the variance of each component
plot(pca)

# With connected lines - useful for looking for the "elbow"
plot(pca, type = "l")

# PC1 and PC2
pca$loadings[, 1:2]
```
```{block, type = 'rmdinsight'}
PCA produces **uncorrelated** variables from the original set $X_1,\ldots,X_k$. This implies that:

- The PCs are uncorrelated, **but not independent** (uncorrelated does not imply independent).
- An uncorrelated or independent variable in $X_1,\ldots,X_k$ will get a PC only associated to it. In the extreme case where all the $X_1,\ldots,X_k$ are uncorrelated, these coincide with the PCs (up to sign flips).

```

Based on the weights of the variables on the PCs, we can extract the following interpretation:

- PC1 is roughly a linear combination of `Agr`, with *negative* weight, and (`Man`, `Pow`, `Con`, `Ser`, `Soc`, `Tra`), with *positive* weights. So it can be interpreted as an *indicator* of the kind of economy of the country: agricultural (negative values) or industrial (positive values).
- PC2 has *negative* weights on (`Min`, `Man`, `Pow`, `Tra`) and *positive* weights in (`Ser`, `Fin`, `Soc`). It can be interpreted as the contrast between relatively large or small service sectors. So it tends to be negative in communist countries and positive in capitalist countries.

```{block, type="rmdtip"}
The interpretation of the PCs involves inspecting the weights and interpreting the linear combination of the original variables, which might be separating between two clear characteristics of the data
```

To conclude, let's see how we can represent our original data into a plot called *biplot* that summarizes all the analysis for two PCs.
```{r, out.width = '90%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Biplot - plot together the scores for PC1 and PC2 and the
# variables expressed in terms of PC1 and PC2
biplot(pca)
```

### Case studies: Analysis of `USArrests`, `USJudgeRatings` and *La Liga 2015/2016 metrics*

The selection of the number of PCs and their interpretation though the weights and biplots are key aspects in a successful application of PCA. In this section we will see examples of both points through the datasets `USArrests`, `USJudgeRatings` and *La Liga 2015/2016* ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/la-liga-2015-2016.xlsx)).

The **selection of the number of components** $l$, $1\leq l\leq k$^[We are implicitly assuming that $n>k$. Otherwise, the maximum number of PCs would be $\min(n-1,k)$.], is a tricky problem without a unique and well-established criterion for what is the *best* number of components. The reason is that selecting the number of PCs is a trade-off between the variance of the original data that we want to explain and the price we want to pay in terms of a more complex dataset. Obviously, except for particular cases^[For example, if PC$_1$ explains all the variance of $X_1,\ldots,X_k$ or if the variables are *uncorrelated*, in which case the PCs will be equal to the original variables.], none of the extreme situations $l=1$ (potential low explained variance) or $l=k$ (same number of PCs as the original data -- no dimension reduction) is desirable.

There are several heuristic rules in order to determine the number of components:

1. Select $l$ up to a **threshold of the percentage of variance explained**, such as $70\%$ or $80\%$. We do so by looking into the *third* row of the `summary(...)` of a PCA.
2. **Plot the variances of the PCs and look for an "elbow" in the graph** whose location gives $l$. Ideally, this elbow appears at the PC for which the next PC variances are *almost similar* and *notably smaller* when compared with the first ones. Use `plot(..., type = "l")` for creating the plot.
3. Select $l$ based on the **threshold of the individual variance of each component**. For example, select only the PCs with larger variance than the mean of the variances of all the PCs. If we are working with **standardized variables** (`cor = TRUE`), this equals to taking the **PCs with standard deviation larger than one**. We do so by looking into the *first* row of the `summary(...)` of a PCA.

In addition to these three heuristics, in practice we might apply a *justified bias* towards:

4. $l=1,2$, since these are the ones that allow to have a **simple graphical representation of the data**. Even if the variability explained by the $l$ PCs is *low* (lower than $50\%$), these graphical representations are usually insightful. $l=3$ is preferred as a second option since its graphical representation is more cumbersome (see the end of this section).
5. $l$'s such that they yield **intepretable PCs**. Interpreting PCs is not so straightforward as interpreting the original variables. Furthermore, it becomes more difficult the larger the index of the PC is, since it explains less information of the data.

Let's see these heuristics in practice with the `USArrests` dataset (arrest statistics and population of US states).
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Load data
data(USArrests)

# Snapshot of the data
head(USArrests)

# PCA
pcaUSArrests <- princomp(USArrests, cor = TRUE)
summary(pcaUSArrests)

# Plot of variances (screeplot)
plot(pcaUSArrests, type = "l")
```
The selections of $l$ for this PCA, based on the previous heuristics, are:

1. $l=2$, since it explains the $86\%$ of the variance and $l=1$ only the $62\%$.
2. $l=2$, since from $l=2$ onward the variances are very similar.
3. $l=1$, since the $\text{PC}_2$ has standard deviation smaller than $1$ (limit case).
4. $l=2$ is fine, it can be easily represented graphically.
5. $l=2$ is fine, both components are interpretable, as we will see later.

Therefore, we can conclude that *$l=2$ PCs is a good compromise* for representing the `USArrests` dataset.

Let's see what happens for the `USJudgeRatings` dataset (lawyers' ratings of US Superior Court judges).
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Load data
data(USJudgeRatings)

# Snapshot of the data
head(USJudgeRatings)

# PCA
pcaUSJudgeRatings <- princomp(USJudgeRatings, cor = TRUE)
summary(pcaUSJudgeRatings)

# Plot of variances (screeplot)
plot(pcaUSJudgeRatings, type = "l")
```
The selections of $l$ for this PCA, based on the previous heuristics, are:

1. $l=1$, since it explains alone the $84\%$ of the variance.
2. $l=1$, since from $l=1$ onward the variances are very similar compared to the first one.
3. $l=2$, since the $\text{PC}_3$ has standard deviation smaller than $1$.
4. $l=1,2$ are fine, they can be easily represented graphically.
5. $l=1,2$ are fine, both components are interpretable, as we will see later.

Based on the previous cirteria, we can conclude that *$l=1$ PC is a reasonable compromise* for representing the `USJudgeRatings` dataset.

We analyse now a slightly more complicated dataset. It contains the standings and team statistics for La Liga 2015/2016:
```{r, laliga, echo = FALSE, out.width = '90%', fig.align = 'center', message = FALSE, warning = FALSE, cache = TRUE}
library(RcmdrMisc)
laliga <- readXL("datasets/la-liga-2015-2016.xlsx")
rownames(laliga) <- laliga$Team
laliga$Team <- NULL
knitr::kable(
  laliga[, 1:5],
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'Selection of variables for La Liga 2015/2016 dataset.'
)
```

```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# PCA - we remove the second variable, matches played, since it is constant
pcaLaliga <- princomp(laliga[, -2], cor = TRUE)
summary(pcaLaliga)

# Plot of variances (screeplot)
plot(pcaLaliga, type = "l")
```
The selections of $l$ for this PCA, based on the previous heuristics, are:

1. $l=2,3$, since they explain the $79\%$ and $86\%$ of the variance (it depends on the threshold of the variance, $70\%$ or $80\%$).
2. $l=3$, since from $l=1$ onward the variances are very similar compared to the first one.
3. $l=3$, since the $\text{PC}_4$ has standard deviation smaller than $1$.
4. $l=2$ is preferred to $l=3$.
5. $l=1,2$ are fine, both components are interpretable, as we will see later. $l=3$ is harder to interpret.

Based on the previous citeria, we can conclude that *$l=2$ PCs is a reasonable compromise* for representing La Liga 2015/2016 dataset.

Let's focus now on the **interpretation of the PCs**. In addition to the weights present in the `loadings` slot, `biplot` provides a powerful and succinct way of displaying the relevant information for $1\leq l\leq 2$. The biplot shows:

1. The **scores of the data in PC1 and PC2** by points (with optional text labels, depending if there are case names). This is the representation of the data in the first two PCs.
2. The **variables represented in the PC1 and PC2 by the arrows**. These arrows are centered at $(0, 0)$.

Let's examine the arrow associated to the variable $X_j$. $X_j$ is expressed in terms of $\text{PC}_1$ and $\text{PC}_2$ by the *weights* $a_{j1}$ and $a_{j2}$:
\[
X_j=a_{j1}\text{PC}_{1} + a_{j2}\text{PC}_{2} + \ldots + a_{jk}\text{PC}_{k}\approx a_{j1}\text{PC}_{1} + a_{j2}\text{PC}_{2}.
\]
$a_{j1}$ and $a_{j2}$ have the same sign as $\mathrm{Cor}(X_j,\text{PC}_{1})$ and $\mathrm{Cor}(X_j,\text{PC}_{2})$, respectively. The arrow associated to $X_j$ is given by the segment joining $(0,0)$ and $(a_{j1},a_{j2})$. Therefore:

- If the arrow *points right* ($a_{j1}>0$), there is *positive correlation between $X_j$ and $\text{PC}_1$*. Analogous if the arrow points left.
- If the arrow is *approximately vertical* ($a_{j1}\approx0$), there is *uncorrelation between $X_j$ and $\text{PC}_1$*.

Analogously:

- If the arrow *points up* ($a_{j2}>0$), there is *positive correlation between $X_j$ and $\text{PC}_2$*. Analogous if the arrow points down.
- If the arrow is *approximately horizontal* ($a_{j2}\approx0$), there is *uncorrelation between $X_j$ and $\text{PC}_2$*.

In addition, the **magnitude of the arrow informs about the correlation**.

The biplot also provides the direct relation between variables, at sight of their expresions in $\text{PC}_1$ and $\text{PC}_2$. The **angle** of the arrows of variable $X_j$ and $X_m$ gives an **approximation to the correlation between them, $\mathrm{Cor}(X_j,X_m)$**:

- If $\text{angle}\approx 0^\circ$, the two variables are highly positively correlated.
- If $\text{angle}\approx 90^\circ$, they are approximately uncorrelated.
- If $\text{angle}\approx 180^\circ$, the two variables are highly negatively correlated.

The **approximation to the correlation by means of the arrow angles is as good as the percentage of variance explained** by $\text{PC}_1$ and $\text{PC}_2$.

Let see an in-depth illustration of the previous concepts for `pcaUSArrests`:
```{r, out.width = '90%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Weights and biplot
pcaUSArrests$loadings
biplot(pcaUSArrests)
```
We can extract the following conclusions regarding the arrows and PCs:

- `Murder`, `Assault` and `Rape` are negatively correlated with $\text{PC}_1$, which might be regarded as an indicator of the *absence of crime* (positive for less crimes, negative for more). The variables are highly correlated between them and the arrows are:
\begin{align*}
\vec{\text{Murder}} = (-0.536, 0.418)\\
\vec{\text{Assault}} = (-0.583, 0.188)\\
\vec{\text{Rape}} = (-0.543, -0.167)
\end{align*}

- `Murder` and `UrbanPop` are approximately uncorrelated.
- `UrbanPop` is the most correlated variable with $\text{PC}_2$ (positive for low urban population, negative for high). Its arrow is:
\begin{align*}
\vec{\text{UrbanPop}} = (-0.278 -0.873).
\end{align*}
Therefore, the biplot shows that states like Florida, South Carolina and California have high crime rate, whereas states like North Dakota or Vermont have low crime rate. California, in addition to have a high crime rate has a large urban population, whereas South Carolina has a low urban population. With the biplot, we can visualize the differences between states according to the crime rate and urban population in a simple way.

Let's see now the biplot for the `USJudgeRatings`, which has a clear interpretation:
```{r, out.width = '90%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Weights and biplot
pcaUSJudgeRatings$loadings
biplot(pcaUSJudgeRatings, cex = 0.75)
```
The $\text{PC}_1$ gives a *lawyer indicator of how badly the judge conducts a trial*. The variable `CONT`, which measures the number of contacts between judge and lawyer, is almost uncorrelated with the rest of variables and is captured by $\text{PC}_2$ (hence the rates of the lawyers are not affected by the number of contacts with the judge). We can identify the high-rated and low-rated judges in the left and right of the plot, respectively.

Let's see an application of the biplot in a La Liga 2015/2016, a dataset with more variables and a harder interpretation of PCs.
```{r, out.width = '90%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Weights and biplot
pcaLaliga$loadings
biplot(pcaLaliga, cex = 0.75)
```
Some interesting highlights:

- $\text{PC}_1$ can be regarded as the *non-performance of a team* during the season. It is negatively correlated with `Wins`, `Points`,\ldots and positively correlated with `Draws`, `Loses`, `Yellow.cards`,\ldots The best performing teams are not surprising: Barcelona, Real Madrid and Atlético Madrid. On the other hand, among the worst-performing teams are Levante, Getafe and Granada.
- $\text{PC}_2$ can be seen as the *inefficiency of a team* (conceding points with little participation in the game). Using this interpretation we can see that Rayo Vallecano and Real Madrid were the most inefficient teams and Atlético Madrid and Villareal were the most.
- `Offsides` is approximately uncorrelated with `Red.cards`.
- $\text{PC}_3$ does not have a clear interpretation.

If you are wondering about the 3D representation of the biplot, it can be computed through:
```{r, webgl = TRUE, cache = TRUE}
# Install this package with install.packages("pca3d")
library(pca3d)
pca3d(pcaLaliga, show.labels = TRUE, biplot = TRUE)
```

Finally, we mention that `R Commander` has a menu entry for performing PCA: `'Statistics' -> 'Dimensional analysis' -> 'Principal-components analysis...'`. Alternatively, the plug-in `FactoMineR` implements a PCA with more options and graphical outputs. It can be loaded (if installed) in `'Tools' -> 'Load Rcmdr plug-in(s)...' -> 'RcmdPlugin.FactoMineR'` (you will need to restart `R Commander`). For performing a PCA in `FactoMineR`, go to `'FactoMineR' -> 'Principal Component Analysis (PCA)'`. In that menu you will have more advanced options than in `R Commander`'s PCA.

<!--

## Geometry behind PCA

-->

<!--chapter:end:04-pca.Rmd-->

# Cluster analysis {#cluster}

Cluster analysis is the collection of techniques designed to find subgroups or *clusters* in a dataset of variables $X_1,\ldots,X_k$. Depending on the similarities between the observations, these are partitioned in homogeneous groups as separated as possible between them. Clustering methods can be classified into two main categories:

- **Partition methods**. Given a fixed number of cluster $k$, these methods aim to assign each observation of $X_1,\ldots,X_k$ to a unique cluster, in such a way that the *within-cluster variation* is as small as possible (the clusters are as homogeneous as possible) while the *between cluster variation* is as large as possible (the clusters are as separated as possible).
- **Hierarchical methods**. These methods construct a hierarchy for the observations in terms of their similitudes. This results in a tree-based representation of the data in terms of a *dendogram*, which depicts how the observations are clustered at different levels -- from the smallest groups of one element to the largest representing the whole dataset.

We will see the basics of the most well-known partition method, namely *$k$-means clustering*, and of the *agglomerative hierarchical clustering*.

## $k$-means clustering {#kmeans}

The $k$-means clustering looks for **$k$ clusters in the data such that they are as compact as possible and as separated as possible**. In clustering terminology, the clusters minimize the *with-in cluster variation* with respect to the cluster centroid while they maximize the *between cluster variation* among clusters. The distance used for measuring proximity is the usual **Euclidean distance** between points. As a consequence, this clustering method tend to yield spherical or rounded clusters and is not adequate for categorical variables.

```{r, kmeans, echo = FALSE, results = 'hide', out.width = '90%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'The $k$-means partitions for a two-dimensional dataset with $k=1,2,3,4$. Centers of each cluster are displayed with an asterisk.', cache = TRUE}
# Data with 3 clusters
set.seed(23456789)
n <- 20
x <- rbind(matrix(rnorm(n, sd = 0.3), ncol = 2),
           cbind(rnorm(n, sd = 0.3), rnorm(n, mean = 2, sd = 0.3)),
           matrix(rnorm(n, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")

par(mfrow = c(2, 2))
for (k in 1:4) {
  set.seed(23456789)
  cl <- kmeans(x, centers = k, nstart = 20)
  plot(x, col = cl$cluster, pch = 16, cex = 0.75, main = paste("k =", k))
  points(cl$centers, col = 1:k, pch = 8, cex = 2)
}
```
Let's analyze the possible clusters in a smaller subset of *La Liga 2015/2016* ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/la-liga-2015-2016.xlsx)) dataset, where the results can be easily visualized. To that end, import the data as `laliga`.

```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
library(RcmdrMisc)
library(car)
laliga <- readXL("datasets/la-liga-2015-2016.xlsx")
rownames(laliga) <- laliga$Team
laliga$Team <- NULL
```

```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# We consider only a smaller dataset (Points and Yellow.cards)
head(laliga, 2)
pointsCards <-  laliga[, c(1, 17)]
plot(pointsCards)

# kmeans uses a random initialization of the clusters, so the results may vary
# from one call to another. We use set.seed() to have reproducible outputs.
set.seed(2345678)

# kmeans call:
# - centers is the k, the number of clusters.
# - nstart indicates how many different starting assignments should be considered
# (useful for avoiding suboptimal clusterings)
k <- 2
km <- kmeans(pointsCards, centers = k, nstart = 20)

# What is inside km?
km
str(km)

# between_SS / total_SS gives a criterion to select k similar to PCA.
# Recall that between_SS / total_SS = 100% if k = n

# Centroids of each cluster
km$centers

# Assignments of observations to the k clusters
km$cluster

# Plot data wih colors according to clusters
plot(pointsCards, col = km$cluster)

# Add the names of the observations above the points
text(x = pointsCards, labels = rownames(pointsCards), col = km$cluster,
     pos = 3, cex = 0.75)

# Clustering with k = 3
k <- 3
set.seed(2345678)
km <- kmeans(pointsCards, centers = k, nstart = 20)
plot(pointsCards, col = km$cluster)
text(x = pointsCards, labels = rownames(pointsCards), col = km$cluster,
     pos = 3, cex = 0.75)

# Clustering with k = 4
k <- 4
set.seed(2345678)
km <- kmeans(pointsCards, centers = k, nstart = 20)
plot(pointsCards, col = km$cluster)
text(x = pointsCards, labels = rownames(pointsCards), col = km$cluster,
     pos = 3, cex = 0.75)
```

So far, we have only taken the information of two variables for performing clustering. Using PCA, we can visualize the clustering performed with all the available variables in the dataset.

By default, `kmeans` **does not standardize variables, which will affect the clustering result**. As a consequence, the clustering of a dataset will be different if one variable is expressed in millions or in tenths. If you want to avoid this distortion, you can use `scale` to automatically center and standardize a data frame (the result will be a matrix, so you need to transform it to a data frame again).

```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, cache = TRUE}
# Work with standardized data (and remove Matches)
laligaStd <- data.frame(scale(laliga[, -2]))

# Clustering with all the variables - unstandardized data
set.seed(345678)
kme <- kmeans(laliga, centers = 3, nstart = 20)
kme$cluster
table(kme$cluster)

# Clustering with all the variables - standardized data
set.seed(345678)
kme <- kmeans(laligaStd, centers = 3, nstart = 20)
kme$cluster
table(kme$cluster)

# PCA
pca <- princomp(laliga[, -2], cor = TRUE)
summary(pca)

# Biplot (the scores of the first two PCs)
biplot(pca)

# Redo the biplot with colors indicating the cluster assignments
plot(pca$scores[, 1:2], col = kme$cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = kme$cluster)

# Recall: this is a visualization with PC1 and PC2 of the clustering done with all the variables,
# not just PC1 and PC2

# Clustering with only the first two PCs - different and less accurate result, but still insightful
set.seed(345678)
kme2 <- kmeans(pca$scores[, 1:2], centers = 3, nstart = 20)
plot(pca$scores[, 1:2], col = kme2$cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = kme2$cluster)
```

$k$-means can also be performed through the help of `R Commander`. To do so, go to `'Statistics' -> 'Dimensional Analysis' -> 'Clustering' -> 'k-means cluster analysis...'`. If you do this for the `USArrests` dataset after rescaling it, select to `'Assign clusters to the data set'` and name the `'Assignment variable'` as `'KMeans'`, you should get something like this:
```{r, collapse = TRUE, cache = TRUE}
# Load data and scale it
data(USArrests)
USArrests <- as.data.frame(scale(USArrests))

# Statistics -> Dimensional Analysis -> Clustering -> k-means cluster analysis...
.cluster <-  KMeans(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop, USArrests),
  centers = 2, iter.max = 10, num.seeds = 10)
.cluster$size # Cluster Sizes
.cluster$centers # Cluster Centroids
.cluster$withinss # Within Cluster Sum of Squares
.cluster$tot.withinss # Total Within Sum of Squares
.cluster$betweenss # Between Cluster Sum of Squares
remove(.cluster)
.cluster <-  KMeans(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop, USArrests),
  centers = 2, iter.max = 10, num.seeds = 10)
.cluster$size # Cluster Sizes
.cluster$centers # Cluster Centroids
.cluster$withinss # Within Cluster Sum of Squares
.cluster$tot.withinss # Total Within Sum of Squares
.cluster$betweenss # Between Cluster Sum of Squares
biplot(princomp(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop, USArrests)),
  xlabs = as.character(.cluster$cluster))
USArrests$KMeans <- assignCluster(model.matrix(~-1 + Assault + Murder + Rape + UrbanPop,
  USArrests), USArrests, .cluster$cluster)
remove(.cluster)
```

```{block, type="rmdcaution"}
How many clusters $k$ do we need in practice? There is not a single answer: the advice is to try several and compare. Inspecting the `'between_SS / total_SS'` for a good trade-off between the number of clusters and the percentage of total variation explained usually gives a good starting point for deciding on $k$.
```

```{block, type='rmdexercise'}
For the `iris` dataset, do sequentially:

1. Apply `scale` to the dataset and save it as `irisStd`. Note: the fifth variable is a factor, so you must skip it.
2. Fix the seed to 625365712.
3. Run $k$-means with 20 runs for $k=2,3,4$. Save the results as `km2`, `km3` and `km4`.
4. Compute the PCA of `irisStd`.
5. Plot the first two scores, colored by the assignments of `km2`.
6. Do the same for `km3` and `km4`.
7. Which $k$ do you think it gives the most sensible partition based on the previous plots?

```

## Agglomerative hierarchical clustering {#hierarchical}

```{r, trees, echo = FALSE, results = 'hide', out.width = '90%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'The hierarchical clustering for a two-dimensional dataset with complete, single and average linkages.', cache = TRUE}
# Data with 3 clusters
set.seed(23456789)
n <- 10
x <- rbind(matrix(rnorm(n, sd = 0.3), ncol = 2),
           cbind(rnorm(n, sd = 0.3), rnorm(n, mean = 2, sd = 0.3)),
           matrix(rnorm(n, mean = 1, sd = 0.3), ncol = 2))
colnames(x) <- c("x", "y")

par(mfrow = c(2, 2))
plot(x, type = "n", cex = 0.75, main = "Data")
text(x, labels = as.character(1:(2 * n)))
for (method in c("average", "complete", "single")) {
  set.seed(23456789)
  cl <- hclust(dist(x), method = method)
  plot(cl, main = method)
}
```

Hierarchical clustering starts by considering that each observation is its own cluster, and then merges sequentially the clusters with a lower degree of *disimilarity* $d$ (the lower the similarity, the larger the similarity). For example, if there are three clusters, $A$, $B$ and $C$, and their dissimilarities are $d(A,B)=0.1$, $d(A,C)=0.5$, $d(B,C)=0.9$, then the three clusters will be reduced to just two: $(A,B)$ and $C$.

The advantages of hierarchical clustering are several:

- We do not need to specify a fixed number of clusters $k$.
- The clusters are naturally nested within each other, something that does not happen in $k$-means. Is possible to visualize this nested structure throughout the *dendogram*.
- It can deal with categorical variables, throughout the specification of proper dissimilarity measures. In particular, it can deal with numerical variables using the Euclidean distance.

The linkage employed by hierarchical clustering refers to how the cluster are fused:

- **Complete**. Takes the *maximal dissimilarity* between all the pairwise dissimilarities
between the observations in cluster A and cluster B.
- **Single**. Takes the *minimal dissimilarity* between all the pairwise dissimilarities
between the observations in cluster A and cluster B.
- **Average**. Takes the *average dissimilarity* between all the pairwise dissimilarities
between the observations in cluster A and cluster B.

Hierarchical clustering is quite sensible to the kind of dissimilarity employed and the kind of *linkage* used. In addition, the hierarchical property might force the clusters to unnatural behaviors. Particularly, *single* linkage may result in extended, chained clusters in which a single observation is added at a new level. As a consequence, *complete* and *average* are usually recommended in practice.

Let's illustrate how to perform hierarchical clustering in `laligaStd`.
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, warning = FALSE, cache = TRUE}
# Compute dissimilary matrix - in this case Euclidean distance
d <- dist(laligaStd)

# Hierarchical clustering with complete linkage
treeComp <- hclust(d, method = "complete")
plot(treeComp)

# With average linkage
treeAve <- hclust(d, method = "average")
plot(treeAve)

# With single linkage
treeSingle <- hclust(d, method = "single")
plot(treeSingle) # Chaining

# Set the number of clusters after inspecting visually the dendogram for "long" groups of hanging leaves
# These are the cluster assignments
cutree(treeComp, k = 2) # (Barcelona, Real Madrid) and (rest)
cutree(treeComp, k = 3) # (Barcelona, Real Madrid), (Atlético Madrid) and (rest)

# Compare differences - treeComp makes more sense than treeAve
cutree(treeComp, k = 4)
cutree(treeAve, k = 4)

# We can plot the results in the first two PCs, as we did in k-means
cluster <- cutree(treeComp, k = 2)
plot(pca$scores[, 1:2], col = cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = cluster)

cluster <- cutree(treeComp, k = 3)
plot(pca$scores[, 1:2], col = cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = cluster)

cluster <- cutree(treeComp, k = 4)
plot(pca$scores[, 1:2], col = cluster)
text(x = pca$scores[, 1:2], labels = rownames(pca$scores), pos = 3, col = cluster)
```
If categorical variables are present, replace `dist` by `daisy` from the `cluster` package (you need to do first `library(cluster)`). For example, let's cluster the `iris` dataset.
```{r, collapse = TRUE, out.width = '70%', fig.align = 'center', fig.asp = 1, warning = FALSE, cache = TRUE}
# Load data
data(iris)

# The fifth variable is a factor
head(iris)

# Compute dissimilarity matrix using the Gower dissimilarity measure
# This dissimilarity is able to handle both numerical and categorical variables
# daisy automatically detects whether there are factors present in the data and applies Gower
# (otherwise it applies the Euclidean distance)
library(cluster)
d <- daisy(iris)
tree <- hclust(d)

# 3 main clusters
plot(tree)

# The clusters correspond to the Species
cutree(tree, k = 3)
table(iris$Species, cutree(tree, k = 3))
```

```{block, type = "rmdcaution"}
Performing hierarchical clustering in practice depends on several decisions that may have big consequences on the final output:

  - What kind of dissimilarity and linkage should be employed? Not a single answer: try several and compare.
  - Where to cut the dendogram? The general advice is to look for groups of branches hanging for a long space and cut on their top.

Despite the general advice, there is not a single and best solution for the previous questions. What is advisable in practice is to analyse several choices, report the general patterns that arise and the different features of the data the methods expose.
```

Hierarchical clustering can also be performed through the help of `R Commander`. To do so, go to `'Statistics' -> 'Dimensional Analysis' -> 'Clustering' -> 'Hierar...'`. If you do this for the `USArrests` dataset after rescaling, you should get something like this:
```{r, collapse = TRUE, cache = TRUE}
HClust.1 <- hclust(dist(model.matrix(~-1 + Assault+Murder+Rape+UrbanPop, USArrests)) ,
  method= "complete")
plot(HClust.1, main= "Cluster Dendrogram for Solution HClust.1", xlab=
  "Observation Number in Data Set USArrests", sub="Method=complete; Distance=euclidian")
```

```{block, type="rmdexercise"}
Import the `eurojob`  ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/eurojob.txt)) dataset and standardize it properly. Perform a hierarchical clustering analysis for the three kind of linkages seen.
```

<!--chapter:end:05-cluster.Rmd-->

# (APPENDIX) Appendix {-}

# Glossary of important `R` commands

#### Basic usage {-}

The following table contains important `R` commands for its **basic usage**.

|             Description         |        `R`       |              Example                |
|---------------------------------|------------------|-------------------------------------|
| **Assign values to a variable** | `<-` | `x <- 1` |
| **Compute several expressions at once** | `;` | `x <- 1; 2 + 2; 3 * 8` |
| **Create vectors by concatenating numbers** | `c` | `c(1, 2, -1)` |
| **Create sequential integer vectors** | `:` | `1:10` |
| **Create a matrix by columns** | `cbind` | `cbind(1:3, c(0, 2, 0))` |
| **Create a matrix by rows** | `rbind` | `rbind(1:3, c(0, 2, 0))` |
| **Create a data frame** | `data.frame` | `data.frame(name1 = c(-1, 3), name2 = c(0.4, 1))` |
| **Create a list** | `list` | `list(obj1 = c(-1, 3), obj2 = -1:5, obj3 = rbind(1:2, 3:2))` |
| **Access elements of a...** |  | |
| ... vector | `[]`| `c(0.5, 2)[1], c(0.5, 2)[-1]; c(0.5, 2)[2:1]`|
| ... matrix | `[, ]`| `cbind(1:2, 3:4)[1, 2]; cbind(1:2, 3:4)[1, ]` |
| ... data frame | `[, ]` and `$`| `data.frame(name1 = c(-1, 3), name2 = c(0.4, 1))$name1; data.frame(name1 = c(-1, 3), name2 = c(0.4, 1))[2, 1]` |
| ... list | `$`| `list(x = 2, y = 7:0)$y` |
| **Summarize any object** | `summary` | `summary(1:10)` |

#### Linear regression {-}

Some useful commands for performing **simple and multiple linear regression** are given in the next table. We assume that:

- `dataset` is an imported dataset such that
    - `resp` is the response variable
    - `pred1` is first predictor
    - `pred2` is second predictor
    - ...
    - `predk` is the last predictor
- `model` is the result of applying `lm`
- `newPreds` is a `data.frame` with variables named as the predictors
- `num` is `1`, `2` or `3`
- `level` is a number between 0 and 1

|               Description          |              `R`             |
|------------------------------------|------------------------------|
| **Fit a simple linear model ** | `lm(response ~ pred1, data = dataset)` |
| **Fit a multiple linear model... ** | |
| ... on two predictors | `lm(response ~ pred1 + pred2, data = dataset)` |
| ... on all predictors | `lm(response ~ ., data = dataset)` |
| ... on all predictors except `pred1` | `lm(response ~ . - pred1, data = dataset)` |
|  **Summarize linear model**: coefficient estimates, standard errors, $t$-values, $p$-values for $H_0:\beta_j=0$, $\hat\sigma$ (Residual standard error), degrees of freedom, $R^2$, Adjusted $R^2$, $F$-test, $p$-value for $H_0:\beta_1=\ldots=\beta_k=0$ | `summary(model)` |
| **ANOVA decomposition** | `anova(model)` |
| **CIs coefficients** | `confint(model, level = level)` |
| **Prediction** | `predict(model, newdata = new)` |
| **CIs predicted mean** | `predict(model, newdata = new, interval = "confidence", level = level)` |
| **CIs predicted response** | `predict(model, newdata = new, interval = "prediction", level = level)` |
| **Variable selection** | `stepwise(model)` |
| **Multicollinearity detection** | `vif(model)` |
| **Compare model coefficients** | `compareCoefs(model1, model2)` |
| **Diagnostic plots** | `plot(model, num)` |

#### More basic usage {-}

The following table contains important `R` commands for its **basic usage**. We assume the following dataset is available:
```{r, cache = TRUE}
data <- data.frame(x = 1:10, y = c(-1, 2, 3, 0, 3, 1, -1, 3, 0, -1))
```


|             Description         |        `R`       |              Example                |
|---------------------------------|------------------|-------------------------------------|
| **Data frame management** | | |
| variable names | `names` | `names(data)` |
| structure | `str` | `str(data)` |
| dimensions | `dim` | `dim(data)` |
| beginning | `head` | `head(data)` |
| **Vector related functions** | | |
| create sequences | `seq` | `seq(0, 1, l = 10); seq(0, 1, by = 0.25)` |
| reverse a vector | `rev` | `rev(1:5)` |
| length of a vectors | `length` | `length(1:5)` |
| count repetitions in a vector | `table` | `table(c(1:5, 4:2))` |
| **Logical conditions** | | |
| relational operators | `<`, `<=`, `>`, `>=`, `==`, `!=` | `1 < 0; 1 <= 1; 2 > 1; 3 >= 4; 1 == 0; 1 != 0` |
| "and" | `&` | `TRUE & FALSE` |
| "or" | `|` | `TRUE | FALSE` |
| **Subsetting** | | |
| vector | | `data$x[data$x > 0]; data$x[data$x > 2 & data$x < 8]` |
| data frame| | `data[data$x > 0, ]; data[data$x < 2 | data$x > 8, ]` |
| **Distributions** | | |
| sampling | `rxxxx` | `rnorm(n = 10, mean = 0, sd = 1)` |
| density | `dxxxx` | `x <- seq(-4, 4, l = 20); dnorm(x = x, mean = 0, sd = 1)` |
| distribution | `pxxxx` | `x <- seq(-4, 4, l = 20); pnorm(q = x, mean = 0, sd = 1)` |
| quantiles | `qxxxx` | `p <- seq(0.1, 0.9, l = 10); qnorm(p = p, mean = 0, sd = 1)` |
| **Plotting** | | |
| scatterplot | `plot` | `plot(rnorm(100), rnorm(100))` |
| plot a curve | `plot`, `seq` | `x <- seq(0, 1, l = 100); plot(x, x^2, type = "l")` |
| add lines | `lines`, | `x <- seq(0, 1, l = 100); plot(x, x^2 + rnorm(100, sd = 0.1)); lines(x, x^2, col = 2, lwd = 2)` |

#### Logistic regression {-}

Some useful commands for performing **logistic regression** are given in the next table. We assume that:

- `dataset` is an imported dataset such that
    - `resp` is the response binary variable
    - `pred1` is first predictor
    - `pred2` is second predictor
    - ...
    - `predk` is the last predictor
- `model` is the result of applying `glm`
- `newPreds` is a `data.frame` with variables named as the predictors
- `level` is a number between 0 and 1

|               Description          |              `R`             |
|------------------------------------|------------------------------|
| **Fit a simple logistic model** | `glm(response ~ pred1, data = dataset, family = "binomial")` |
| **Fit a multiple logistic model... ** | |
| ... on two predictors | `glm(response ~ pred1 + pred2, data = dataset, family = "binomial")` |
| ... on all predictors | `glm(response ~ ., data = dataset, family = "binomial")` |
| ... on all predictors except `pred1` | `glm(response ~ . - pred1, data = dataset, family = "binomial")` |
|  **Summarize logistic model**: coefficient estimates, standard errors, Wald statistics (`'z value'`), $p$-values for $H_0:\beta_j=0$, Null deviance, deviance (`'Residual deviance'`), AIC, number of iterations | `summary(model)` |
| **CIs coefficients** | `confint(model, level = level); confint.default(model, level = level)` |
| **CIs exp-coefficients** | `exp(confint(model, level = level)); exp(confint.default(model, level = level))` |
| **Prediction** | `predict(model, newdata = new, type = "response")` |
| **CIs predicted probability** | Not immediate. Use `predictCIsLogistic(model, newdata = new, level = level)` as seen in Section \@ref(log-reg-prediction) |
| **Variable selection** | `stepwise(model)` |
| **Multicollinearity detection** | `vif(model)` |
| **$R^2$** | Not immediate. Use `r2Log(model = model)` as seen in Section \@ref(log-reg-selection) |
| **Hit matrix** | `table(data$resp, model$fitted.values > 0.5)` |

#### Principal Component Analysis {-}

Some useful commands for performing **logistic regression** are given in the next table. We assume that:

- `dataset` is an imported dataset with several **non-categorical variables** (the variables must be continuous or discrete).
- `pca` is a PCA object, this is, the output of `princomp`.

|               Description          |              `R`             |
|------------------------------------|------------------------------|
| **Compute a PCA... ** | |
| ... unnormalized (if variables have the same scale) | `princomp(dataset)` |
| ... normalized  (if variables have different scales) | `princomp(dataset, cor = TRUE)` |
|  **Summarize PCA**: standard deviation explained by *each* PC, proportion of variance explained by *each* PC, cumulative proportion of variance explained *up* to a given component | `summary(pca)` |
| **Weights** | `pca$loadings` |
| **Scores** | `pca$scores` |
| **Standard deviations of the PCs** | `pca$sdev` |
| **Means of the original variables** | `pca$center` |
| **Screeplot** | `plot(pca); plot(pca, type = "l")` |
| **Biplot** | `biplot(pca)` |

# Use of qualitative predictors in regression {#qualitative-predictors}

An important situation not covered in Chapters \@ref(simp), \@ref(mult) and \@ref(log-reg) is how to deal with *qualitative*, and not *quantitative*, predictors. Qualitative predictors, also known as *categorical* variables or, in `R`'s terminology, *factors*, are ubiquitous in social sciences. Dealing with them requires some care and proper understanding of how these variables are represented in statistical softwares such as `R`.

#### Two levels {-}

The simplest case is the situation with **two levels**, this is, the binary case covered in logistic regression. There we saw that a binary variable $C$ with two levels (for example, *a* and *b*) could be represented as
\[
D=\left\{\begin{array}{ll}1,&\text{if }C=b,\\0,& \text{if }C=a.\end{array}\right.
\]
$D$ now is a *dummy variable*: it **codifies with zeros and ones the two possible levels of the categorical variable**. An example of $C$ could be *gender*, which has levels *male* and *female*. The dummy variable associated is $D=0$ if the gender is male and $D=1$ if the gender is female.

The advantage of this *dummification* is its interpretability in regression models. Since level *a* corresponds to $0$, it can be seen as the *reference level* to which level *b* is compared. This is the key point in dummification: set one level as the reference and codify the rest as departures from them with ones.

The previous interpretation translates easily to regression models. Assume that the dummy variable $D$ is available together with other predictors $X_1,\ldots,X_k$. Then:

- **Linear model**
\[
\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k,D=d]=\beta_0+\beta_1X_1+\ldots+\beta_kX_k+\beta_{k+1}D.
\]
The coefficient associated to $D$ is easily interpretable. $\beta_{k+1}$ is the increment in mean of $Y$ associated to changing $D=0$ (reference) to $D=1$, while the rest of the predictors are fixed. Or in other words, $\beta_{k+1}$ is the increment in mean of $Y$ associated to changing of the level of the categorical variable from *a* to *b*.

- **Logistic model**
\[
\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k,D=d]=\text{logisitc}(\beta_0+\beta_1X_1+\ldots+\beta_kX_k+\beta_{k+1}D).
\]
We have two interpretations of $\beta_{k+1}$, either in terms of log-odds or odds:

    - $\beta_{k+1}$ is the *additive* increment in log-odds of $Y$ associated to changing of the level of the categorical variable from *a* (reference, $D=0$) to *b* ($D=1$).
    - $e^{\beta_{k+1}}$ is the *multiplicative* increment in odds of $Y$ associated to changing of the level of the categorical variable from *a* (reference, $D=0$) to *b* ($D=1$).

`R` does the dummification automatically (translates a categorical variable $C$ into its dummy version $D$) if it detects that a factor variable is present in the regression model. Let's see an example of this in linear and logistic regression.
```{r, collapse = TRUE, cache = TRUE}
# Load the Boston dataset
library(MASS)
data(Boston)

# Structure of the data
str(Boston)
# chas is a dummy variable measuring if the suburb is close to the river (1)
# or not (0). In this case it is not codifyed as a factor but as a 0 or 1.

# Summary of a linear model
mod <- lm(medv ~ chas + crim, data = Boston)
summary(mod)
# The coefficient associated to chas is 5.57772. That means that if the suburb
# is close to the river, the mean of medv increases in 5.57772 units.
# chas is significant (the presence of the river adds a valuable information
# for explaining medv)

# Create a binary response (1 expensive suburb, 0 inexpensive)
Boston$expensive <- Boston$medv > 25

# Summary of a logistic model
mod <- glm(expensive ~ chas + crim, data = Boston, family = "binomial")
summary(mod)
# The coefficient associated to chas is 1.04165. That means that if the suburb
# is close to the river, the log-odds of expensive increases by 1.04165.
# Alternatively, the odds of expensive increases by a factor of exp(1.04165).
# chas is significant (the presence of the river adds a valuable information
# for explaining medv)
```

#### More than two levels {-}

Let's see now the case with **more than two levels**, for example, a categorical variable $C$ with levels *a*, *b* and *c*. If we take *a* as the reference level, this variable can be represented by *two* dummy variables:
\[
D_1=\left\{\begin{array}{ll}1,&\text{if }C=b,\\0,& \text{if }C\neq b\end{array}\right.
\]
and
\[
D_2=\left\{\begin{array}{ll}1,&\text{if }C=c,\\0,& \text{if }C\neq c.\end{array}\right.
\]
Then $C=a$ is represented by $D_1=D_2=0$, $C=b$ is represented by $D_1=1,D_2=0$ and $C=c$ is represented by $D_1=0,D_2=1$. The interpretation of the regression models with the presence of $D_1$ and $D_2$ is the very similar to the one before. For example, for the linear model, the coefficient associated to $D_1$ gives the increment in mean of $Y$ when the category of $C$ changes from *a* to *b*. The coefficient for $D_2$ gives the increment in mean of $Y$ when it changes from *a* to *c*.

In general, if we have a categorical variable with $J$ levels, then the number of dummy variables required is $J-1$. Again, `R` does the dummification automatically for you if it detects that a factor variable is present in the regression model.
```{r, collapse = TRUE, cache = TRUE}
# Load dataset - factors in the last column
data(iris)
summary(iris)

# Summary of a linear model
mod1 <- lm(Sepal.Length ~ ., data = iris)
summary(mod1)
# Speciesversicolor (D1) coefficient: -0.72356. The average increment of
# Sepal.Length when the species is versicolor instead of setosa (reference).
# Speciesvirginica (D2) coefficient: -1.02350. The average increment of
# Sepal.Length when the species is virginica instead of setosa (reference).
# Both dummy variables are significant

# How to set a different level as reference (versicolor)
iris$Species <- relevel(iris$Species, ref = "versicolor")

# Same estimates except for the dummy coefficients
mod2 <- lm(Sepal.Length ~ ., data = iris)
summary(mod2)
# Speciessetosa (D1) coefficient: 0.72356. The average increment of
# Sepal.Length when the species is setosa instead of versicolor (reference).
# Speciesvirginica (D2) coefficient: -0.29994.s The average increment of
# Sepal.Length when the species is virginica instead of versicolor (reference).
# Both dummy variables are significant

# Coefficients of the model
confint(mod2)
# The coefficients of Speciesversicolor and Speciesvirginica are significantly
# negative. Therefore, there are significant
```

```{block, type='rmdcaution'}
**Do not codify a categorical variable as a discrete variable**. This constitutes a major methodological fail that will flaw the subsequent statistical analysis.

For example if you have a categorical variable `party` with levels `partyA`, `partyB` and `partyC`, do not encode it as a discrete variable taking the values `1`, `2` and `3`, respectively. If you do so:

- You assume implicitly an order in the levels of `party`, since `partyA` is closer to `partyB` than to `partyC`.
- You assume implicitly that `partyC` is three times larger than `partyA`.
- The codification is completely arbitrary -- why not considering `1`, `1.5` and `1.75` instead of?

The right way of dealing with categorical variables in regression is to set the variable as a factor and let `R` do internally the dummification.
```

# Multinomial logistic regression

The logistic model can be generalized to categorical variables $Y$ with more than two possible levels, namely $\{1,\ldots,J\}$. Given the predictors $X_1,\ldots,X_k$,  *multinomial logistic regression* models the probability of each level $j$ of $Y$ by
\begin{align}
p_j(\mathbf{x})=\mathbb{P}[Y=j|X_1=x_1,\ldots,X_k=x_k]=\frac{e^{\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k}}{1+\sum_{l=1}^{J-1}e^{\beta_{0l}+\beta_{1l}X_1+\ldots+\beta_{kl}X_k}} (\#eq:multinom-1)
\end{align}
for $j=1,\ldots,J-1$ and (for the last level $J$)
\begin{align}
p_J(\mathbf{x})=\mathbb{P}[Y=J|X_1=x_1,\ldots,X_k=x_k]=\frac{1}{1+\sum_{l=1}^{J-1}e^{\beta_{0l}+\beta_{1l}X_1+\ldots+\beta_{kl}X_k}}. (\#eq:multinom-2)
\end{align}
Note that \@ref(eq:multinom-1) and \@ref(eq:multinom-2) implies that $\sum_{j=1}^J p_j(\mathbf{x})=1$ and that there are $(J-1)\times(k+1)$ coefficients ($(J-1)$ intercepts and $(J-1)\times k$ slopes). Also, \@ref(eq:multinom-2) reveals that the last level, $J$, is given a different treatment. This is because it is the *reference level* (it could be a different one, but is tradition to choose the last one).

The multinomial logistic model has an interesting interpretation in terms of logistic regressions. Taking the quotient between \@ref(eq:multinom-1) and \@ref(eq:multinom-2) gives
\begin{align}
\frac{p_j(\mathbf{x})}{p_J(\mathbf{x})}=e^{\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k}(\#eq:multinom-4)
\end{align}
for $j=1,\ldots,J-1$. Therefore, applying a logarithm to both sides we have:
\begin{align}
\log\frac{p_j(\mathbf{x})}{p_J(\mathbf{x})}=\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k.(\#eq:multinom-3)
\end{align}
This equation is indeed very similar to \@ref(eq:eq-odds-log2). If $J=2$, is the same up to a change in the codes for the levels: the logistic regression giving the probability of $Y=1$ versus $Y=2$. On the LHS of \@ref(eq:multinom-3) we have the log of the ratio of two probabilities and on the RHS a linear combination of the predictors. **If the probabilities on the LHS were complementary** (if they add one), then we would have a **log-odds and hence a logistic regression** for $Y$. This is not the situation, but is close: instead of odds and log-odds, we have *ratios* and *log-ratios* of non complementary probabilities. Also, it gives a good insight on what the multinomial logistic regression is: **a set of $J-1$ independent "logistic regressions" for the probability of $Y=j$ versus the probability of the reference $Y=J$**.

Equation \@ref(eq:multinom-4) gives also interpretation on the coefficients of the model since
\[
p_j(\mathbf{x})=e^{\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k}p_J(\mathbf{x}).
\]
Therefore:

- $e^{\beta_{0j}}$: is the ratio between $p_j(\mathbf{0})/p_J(\mathbf{0})$, the probabilities of $Y=j$ and $Y=J$ when $X_1=\ldots=X_k=0$. If $e^{\beta_{0j}}>1$ (equivalently, $\beta_{0j}>0$), then $Y=j$ is more likely than $Y=J$. If $e^{\beta_{0j}}<1$ ($\beta_{0j}<0$), then $Y=j$ is less likely than $Y=J$.
- $e^{\beta_{lj}}$, $l\geq1$: is the **multiplicative** increment of the ratio between $p_j(\mathbf{x})/p_J(\mathbf{x})$ for an increment of one unit in $X_l=x_l$, provided that the remaining variables $X_1,\ldots,X_{l-1},X_{l+1},\ldots,X_k$ *do not change*. If $e^{\beta_{lj}}>1$ (equivalently, $\beta_{lj}>0$), then $Y=j$ becomes more likely than $Y=J$ for each increment in $X_j$. If $e^{\beta_{lj}}<1$ ($\beta_{lj}<0$), then $Y=j$ becomes less likely than $Y=J$.

The following code illustrates how to compute a basic multinomial regression in `R`.
```{r, collapse = TRUE, cache = TRUE}
# Package included in R that implements multinomial regression
library(nnet)

# Data from the voting intentions in the 1988 Chilean national plebiscite
data(Chile)
summary(Chile)
# vote is a factor with levels A (abstention), N (against Pinochet),
# U (undecided), Y (for Pinochet)

# Fit of the model done by multinom: Response ~ Predictors
# It is an iterative procedure (maxit sets the maximum number of iterations)
# Read the documentation in ?multinom for more information
mod1 <- multinom(vote ~ age + education + statusquo, data = Chile,
                 maxit = 1e3)

# Each row of coefficients gives the coefficients of the logistic
# regression of a level versus the reference level (A)
summary(mod1)

# Set a different level as the reference (N) for easening interpretations
Chile$vote <- relevel(Chile$vote, ref = "N")
mod2 <- multinom(vote ~ age + education + statusquo, data = Chile,
                 maxit = 1e3)
summary(mod2)
exp(coef(mod2))
# Some highlights:
# - intercepts do not have too much interpretation (correspond to age = 0).
#   A possible solution is to center age by its mean (so age = 0 would
#   represent the mean of the ages)
# - both age and statusquo increase the probability of voting Y, A or U
#   with respect to voting N -> conservativeness increases with ages
# - both age and statusquo increase more the probability of voting Y and U
#   than A -> elderly and status quo supporters more decided to participate
# - a PS level of education increases the probability of voting N. Same for
#   a S level of education, but more prone to A

# Prediction of votes - three profile of voters
newdata <- data.frame(age = c(23, 40, 50),
                      education = c("PS", "S", "P"),
                      statusquo = c(-1, 0, 2))

# Probabilities of belonging to each class
predict(mod2, newdata = newdata, type = "probs")

# Predicted class
predict(mod2, newdata = newdata, type = "class")
```

```{block, type = "rmdcaution"}
Multinomial logistic regression will suffer from numerical instabilities and its iterative algorithm might even fail to converge if the levels of the categorical variable are very separated (e.g., two data clouds clearly separated corresponding to a different level of the categorical variable).
```

```{block, type = "rmdcaution"}
The multinomial model employs $(J-1)(k+1)$ parameters. It is easy to end up with **complex models** -- that require a **large sample size** to be fitted properly -- if the response has a few number of levels and there are several predictors. For example, with $5$ levels and $8$ predictors we will have $36$ parameters. Estimating this model with $50-100$ observations will probably result in **overfitting**.
```

# Reporting with `R` and `R Commander`

A nice feature of `R Commander` is that integrates seamless with `R Markdown`, which is able to create `.html`, `.pdf` and `.docx` reports directly from the outputs of `R`. Depending on the kind of report that we want, we will need the following auxiliary software^[Alternatively, the `'Tools' -> 'Install auxiliary software [if not already installed]'` will redirect you to the download links for the auxiliary software.]:

- `.html`. No extra software is required.
- `.docx` and `.rtf`. You must install `Pandoc`, a document converter software. Download it [here](http://pandoc.org/installing.html).
- `.pdf` (only recommended for experts). An installation of LaTeX, additionally to `Pandoc`, is needed. Download LaTeX [here](https://www.latex-project.org/get/).

The workflow is simple. Once you have done some statistical analysis, either by using `R Commander`'s menus or `R` code directly, you will end up with an `R` script, on the `'R Script'` tab, that contains all the commands you have run so far. Switch then to the `'R Markdown'` tab and you will see the commands you have entered in a different layout, which essentially encapsulates the code into chunks delimited by `` ```{r}`` and `` ``` ``. This will generate a report once you click in the `'Generate report'` button.

Let's illustrate this process through an example. Suppose we were analyzing the `Boston` dataset, as we did in Section \@ref(boston). *Ideally*^[This is, assuming we have performed the right steps in the analysis without making any mistake.] our final script would be something like this:
```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
library(RcmdrMisc)
```
```{r, eval = FALSE}
# A simple and non-exhaustive analysis for the price of the houses in the Boston
# dataset. The purpose is to quantify, by means of a multiple linear model,
# the effect of 14 variables in the price of a house in the suburbs of Boston.

# Import data
library(MASS)
data(Boston)

# Make a multiple linear regression od medv in the rest of variables
mod <- lm(medv ~ ., data = Boston)
summary(mod)

# Check the linearty assumption
plot(mod, 1) # Clear non-linearity

# Let's consider the transformations given in Harrison and Rubinfeld (1978)
modTransf <- lm(I(log(medv * 1000)) ~ I(rm^2) + age + log(dis) +
                  log(rad) + tax + ptratio + I(black / 1000) +
                  I(log(lstat / 100)) + crim + zn + indus + chas +
                  I((10 * nox)^2), data = Boston)
summary(modTransf)

# The non-linearity is more subtle now
plot(modTransf, 1)

# Look for the best model in terms of the BIC
modTransfBIC <- stepwise(modTransf)
summary(modTransfBIC)

# Let's explore the most significant variables, to see if the model can be
# reduced drastically in complexity
mod3D <- lm(I(log(medv * 1000)) ~ I(log(lstat / 100)) + crim, data = Boston)
summary(mod3D)

# With only 2 variables, we explain the 72% of variability.
# Compared with the 80% with 10 variables, it is an important improvement
# in terms of simplicity.

# Let's add these variables to the dataset, so we can call scatterplotMatrix
# and scatter3d through R Commander's menu
Boston$logMedv <- log(Boston$medv * 1000)
Boston$logLstat <- log(Boston$lstat / 100)

# Visualize the pair-by-pair relations of the response and two predictors
scatterplotMatrix(~ crim + logLstat + logMedv, reg.line = lm, smooth = FALSE,
                  spread = FALSE, span = 0.5, ellipse = FALSE,
                  levels = c(.5, .9), id.n = 0, diagonal = 'histogram',
                  data = Boston)

# Visualize the full relation between the response and the two predictors
scatter3d(logMedv ~ crim + logLstat, data = Boston, fit = "linear",
          residuals = TRUE, bg = "white", axis.scales = TRUE, grid = TRUE,
          ellipsoid = FALSE)
```
This contains all the major points in the analysis, that now can be expanded and detailed. You can download the script [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report-script.R), open it through `'File' -> 'Open script file...'` and run it by yourself in `R Commander`. If you so, and then switch to the `R Markdown` tab, you will see this:

    ---
    title: "Replace with Main Title"
    author: "Your Name"
    date: "AUTOMATIC"
    ---

    `r ''````{r echo=FALSE, message=FALSE}
    # include this code chunk as-is to set options
    knitr::opts_chunk$set(comment=NA, prompt=TRUE)
    library(Rcmdr)
    library(car)
    library(RcmdrMisc)
    ```

    `r ''````{r echo=FALSE}
    # include this code chunk as-is to enable 3D graphs
    library(rgl)
    knitr::knit_hooks$set(webgl = hook_webgl)
    ```

    `r ''````{r}
    # A simple and non-exhaustive analysis for the price of the houses in the Boston
    ```

    `r ''````{r}
    # dataset. The purpose is to quantify, by means of a multiple linear model,
    ```

    `r ''````{r}
    # the effect of 14 variables in the price of a house in the suburbs of Boston.
    ```

    `r ''````{r}
    # Import data
    ```

    `r ''````{r}
    library(MASS)
    ```

    `r ''````{r}
    data(Boston)
    ```

    `r ''````{r}
    # Make a multiple linear regression of medv in the rest of variables
    ```

    `r ''````{r}
    mod <- lm(medv ~ ., data = Boston)
    ```

    `r ''````{r}
    summary(mod)
    ```

    [More outputs - omitted]
    ```

The complete, lengthy, file can be downloaded [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/crude-report.Rmd). This is an `R Markdown` file, which has extension `.Rmd`. As you can see, by default, `R Commander` will generate a *code chunk* like

    `r ''````{r}
    code line
    ```

for each `code line` you run in `R Commander`. You probably will want to modify this *crude* report manually by merging chunks of code, removing comments or adding more information in between chunks of code. To do so, go to `'Edit' -> 'Edit Markdown document'`. Here you can also remove unnecessary chunks of code resulting from any mistake or irrelevant analyses.

The following file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.Rmd)) could be a final report. Pay attention to the numerous changes with respect to the previous one:

    ---
    title: "What makes a house valuable?"
    subtitle: "A reproducible analysis in the Boston suburbs"
    author: "Outstanding student 1, Awesome student 2 and Great student 3"
    date: "31/11/16"
    ---

    `r ''````{r echo=FALSE, message=FALSE, warning=FALSE}
    # include this code chunk as-is to set options
    knitr::opts_chunk$set(comment=NA, prompt=TRUE)
    library(Rcmdr)
    library(car)
    library(RcmdrMisc)
    ```

    `r ''````{r echo=FALSE, message=FALSE, warning=FALSE}
    # include this code chunk as-is to enable 3D graphs
    library(rgl)
    knitr::knit_hooks$set(webgl = hook_webgl)
    ```

    This short report shows a simple and non-exhaustive analysis for the price of
    the houses in the `Boston` dataset. The purpose is to quantify, by means of a
    multiple linear model, the effect of 14 variables in the price of a house in
    the suburbs of Boston.

    We start by importing the data into `R` and considering a multiple linear
    regression of `medv` (median house value) in the rest of variables:
    `r ''````{r}
    # Import data
    library(MASS)
    data(Boston)
    ```

    `r ''````{r}
    mod <- lm(medv ~ ., data = Boston)
    summary(mod)
    ```
    The variables `indus` and `age` are non-significant in this model. Also,
    although the adjusted R-squared is high, there seems to be a clear
    non-linearity:
    `r ''````{r}
    plot(mod, 1)
    ```

    In order to bypass the non-linearity, we are going to consider the
    non-linear transformations given in Harrison and Rubinfeld (1978)
    for both the response and the predictors:
    `r ''````{r}
    modTransf <- lm(I(log(medv * 1000)) ~ I(rm^2) + age + log(dis) +
                    log(rad) + tax + ptratio + I(black / 1000) +
                    I(log(lstat / 100)) + crim + zn + indus + chas +
                    I((10*nox)^2), data = Boston)
    summary(modTransf)
    ```
    The adjusted R-squared is now higher and, what is more important, the
    non-linearity now is more subtle (it is still not linear but closer
    than before):
    `r ''````{r}
    plot(modTransf, 1)
    ```

    However, `modTransf` has more non-significant variables. Let\'s see if
    we can improve over the previous model by removing some of the
    non-significant variables? To see this, we look for the best model in
    terms of the Bayesian Information Criterion (BIC) by `stepwise`:
    `r ''````{r}
    modTransfBIC <- stepwise(modTransf, trace = 0)
    summary(modTransfBIC)
    ```
    The resulting model has a slightly higher adjusted R-squared than `modTransf`
    with all the variables significant.

    We explore the most significant variables to see if the model can be reduced
    drastically in complexity.
    `r ''````{r}
    mod3D <- lm(I(log(medv * 1000)) ~ I(log(lstat / 100)) + crim, data = Boston)
    summary(mod3D)
    ```

    It turns out that **with only 2 variables, we explain the 72% of variability**.
    Compared with the 80% with 10 variables, it is an important improvement
    in terms of simplicity: the logarithm of `lstat` (percent of lower status of
    the population) and `crim` (crime rate) alone explain the 72% of the
    variability in the house prices.

    We add these variables to the dataset, so we can call `scatterplotMatrix` and
    `scatter3d` through `R Commander`,
    `r ''````{r}
    Boston$logMedv <- log(Boston$medv * 1000)
    Boston$logLstat <- log(Boston$lstat / 100)
    ```
    and conclude with the visualization of:

    1. the pair-by-pair relations of the response and the two predictors;
    2. the full relation between the response and the two predictors.
    `r ''````{r}
    # 1
    scatterplotMatrix(~ crim + logLstat + logMedv, reg.line = lm, smooth = FALSE,
                      spread = FALSE, span = 0.5, ellipse = FALSE,
                      levels = c(.5, .9), id.n = 0, diagonal = 'histogram',
                      data = Boston)
    ```
    `r ''````{r webgl = TRUE}
    # 2
    scatter3d(logMedv ~ crim + logLstat, data = Boston, fit = "linear",
              residuals = TRUE, bg = "white", axis.scales = TRUE, grid = TRUE,
              ellipsoid = FALSE)
    ```

When we click on `'Generate report'` for the above `R Markdown` file, we should get the following output files:

- `.html`: [visualize](http://htmlpreview.github.io/?https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.html) and [download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.html). Once it is produced, this file is difficult to modify, but very easy to distribute (anyone with a browser can see it).
- `.docx`: [visualize and download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.docx). Easy to modify in a document processor like Microsoft Office. Easy to distribute.
- `.rtf`: [download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.rtf). Easy to modify in a document processor, not very elegant.
- `.pdf`: [visualize and download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.pdf). Elegant and easy to distribute, but hard to modify once it is produced.

```{block, type = 'rmdtip'}
For advanced users, there is a lot of information on mastering `R Markdown` [here](http://rmarkdown.rstudio.com/lesson-1.html) by using `RStudio`, a more advanced framework than `R Commander`.
```

# Group project

#### Groups {-}

You will team up in groups of 3 to 5 members. It is up to you to form the groups based on your grade expectations, affinity, complementary skills, etc. You must communicate the group compositions **no later than November the 30th** by a (single) email to <edgarcia@est-econ.uc3m.es> detailing the members of the group and, if you have it, a preliminary description about the topic of the project (~ 3 lines).

#### Aim of the project {-}

You will analyse a real dataset of your choice using the statistical methodology that we have seen in the lessons and labs. The purpose is to demonstrate that you know how to apply and interpret some of the studied statistical techniques (such as simple/multiple linear regression, logistic regression, or any other methods covered in the course) in a real-case scenario that is appealing for you.

#### Structure of the report {-}

Use the following *mandatory* structure when writing your report:

0. **Abstract**. Provide a concise summary of the project. It must not exceed 250 words.
1. **Introduction**. State what is the problem to be studied. Provide some context, the question(s) that you want to address, a motivation of its importance, references, etc. Remember how we introduced the case studies covered in the course as a template (but you will need to elaborate more).
2. **Statistical analysis**. Make use of some of the aforementioned statistical techniques, the ones that are more convenient to your particular case study. You can choose between covering several at a more superficial level, or one or two in more depth. Justify their adequacy and obtain analyses, explaining how you did it, in the form of plots and summaries. Provide a critical discussion about the outputs and give insights about them.
3. **Conclusions**. Summary of what was addressed in the project and of the most important conclusions. Takeaway messages. The conclusions are not required to be spectacular, but *fair and honest* in terms of what you discovered.
4. **References**. Refer to the sources of information that you have employed (for the data, for information on the data, for the statistical analyses, etc).

*Mandatory* format guidelines:

- Structure: title, authors, abstract, first section, second section and so on. Like [this](http://cje.oxfordjournals.org/content/38/2/257.full.pdf+html). Do not use a cover.
- Font size: 12 points.
- Spacing: single space, single column.
- Length limit: less than 5000 words **and** 15 pages. You are **not** required to make use of all the space.

#### Grading {-}

All students in a group will be graded evenly. Take this into account when forming the group. The grading is on a scale of 0-10 (plus 2 bonus points) and will be performed according to the following breakdown:

- **Originality** of the problem studied and data acquisition process (up to 2 points).
- **Statistical analyses presented** and their depth (up to 3 points). At least two different techniques should be employed (simple and multiple linear regression count as different, but the use of other techniques as well is mostly encouraged). Graded depending on their adequacy to the problem studied and the evidence you demonstrate about your knowledge of them.
- Accuracy of the **interpretation** of the analyses (up to 2 points). Graded depending on the detail and rigor of the insights you elaborate from the statistical analyses.
- **Reproducibility** of the study (1.5 point). Awarded if the code for reproducing the study, as well as the data, is provided in a ready-to-use way (e.g. the outputs from `R Commander`'s report mode along with the data).
- **Presentation** of the report (1.5 point). This involves the correct usage of English, the readability, the conciseness and the overall presentation quality.
- **Excellence** (2 bonus points). Awarded for creativity of the analysis,  use of advanced statistical tools, use of points briefly covered in lessons/labs, advanced insights into the methods, completeness of the report, use of advanced presentation tools, etc. Only awarded if the sum of regular points is above 7.5.

The ratio "quality project"/"group size" might be taken into account in extreme cases (e.g. poor report written by 5 people, extremely good report written by 3 people).

#### Academic fraud {-}

Evidences of academic fraud will have serious consequences, such as a zero grade for the whole group and the reporting of the fraud detection to the pertinent academic authorities. Academic fraud includes (but is not limited to) plagiarism, use of sources without proper credit, project outsourcing and the use of external tutoring not mentioned explicitly.

#### Tips {-}

- Think about a topic that could be reused for other subjects, or take inspiration from previous projects you did. In that way, this project could serve as the *quantification* of another subject's project. If you do this, **add an explicit mention** in the report.
- Data sources. Here are some useful data sources:
	* A [list of all the datasets included in `R`](http://vincentarelbundock.github.io/Rdatasets/datasets.html). See [Section 2.9.1](https://bookdown.org/egarpor/SSS2-UC3M/exercises-and-case-studies.html) of lab notes for how to load them.
	* [Some datasets employed in the course](https://bookdown.org/egarpor/SSS2-UC3M/datasets-for-the-course.html).
	* [The World Bank](http://data.worldbank.org) contains a huge collection of economic and sociological variables for countries and regions, for long periods of time.
	* [SIPRI](https://www.sipri.org/) contains several databases about international transfers of arms.
	* The [Global Health Observatory](http://www.who.int/gho/database/en/) is the World Health Organization's main health statistics repository.
	* Sport statistics (teams, players) are a great source if you like sports. Sport webpages usually have a section on statistics.
- Inspiration for the project's topic.
	* The case studies covered (and left as exercises) in the lab notes might serve as a good starting point for defining a project.
	* [The Economist](http://www.economist.com/) usually has some good and up-to-date political/economical analyses that could serve as motivation.
	* Try to quantify the impact in society of certain laws (traffic, education, gender violence, etc).
	* Is there a continuous variable that you would like to predict from others? (linear regression)
	* Is there a binary variable that you would like to predict from others? (logistic regression)
	* Would you like to assess which combination of variables explain most of the variability of your data, so you can visualize it in 2D or 3D? (principal component analysis)
	* Would you like aggregate individuals according to several characteristics in order to classify them? (clustering)
- Use `R Commander`'s report mode (Appendix \@ref(reporting-with-r-and-r-commander)) to simplify the generation of graphs and summaries directly from the statistical analysis. Use that code to make the analysis reproducible.
- Make use of office hours before it is too late.
- Pro-tip: if you come to my office with a printed draft of the project, I can provide you some quick feedback on what could be improved.

#### Deadline {-}

Submit the reports before December the 23rd at 16:59 through Aula Global. **Not by email**. Reports received after the deadline will not be evaluated.

<!--chapter:end:06-appendix.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:07-references.Rmd-->

