# (APPENDIX) Appendix {-}

# Glossary of important `R` commands {#appendix-Rglossary}

#### Basic usage {-}

The following table contains important `R` commands for its **basic usage**.

|             Description         |        `R`       |              Example                |
|---------------------------------|------------------|-------------------------------------|
| **Assign values to a variable** | `<-` | `x <- 1` |
| **Compute several expressions at once** | `;` | `x <- 1; 2 + 2; 3 * 8` |
| **Create vectors by concatenating numbers** | `c` | `c(1, 2, -1)` |
| **Create sequential integer vectors** | `:` | `1:10` |
| **Create a matrix by columns** | `cbind` | `cbind(1:3, c(0, 2, 0))` |
| **Create a matrix by rows** | `rbind` | `rbind(1:3, c(0, 2, 0))` |
| **Create a data frame** | `data.frame` | `data.frame(name1 = c(-1, 3), name2 = c(0.4, 1))` |
| **Create a list** | `list` | `list(obj1 = c(-1, 3), obj2 = -1:5, obj3 = rbind(1:2, 3:2))` |
| **Access elements of a...** |  | |
| ... vector | `[]`| `c(0.5, 2)[1], c(0.5, 2)[-1]; c(0.5, 2)[2:1]`|
| ... matrix | `[, ]`| `cbind(1:2, 3:4)[1, 2]; cbind(1:2, 3:4)[1, ]` |
| ... data frame | `[, ]` and `$`| `data.frame(name1 = c(-1, 3), name2 = c(0.4, 1))$name1; data.frame(name1 = c(-1, 3), name2 = c(0.4, 1))[2, 1]` |
| ... list | `$`| `list(x = 2, y = 7:0)$y` |
| **Summarize any object** | `summary` | `summary(1:10)` |

#### Linear regression {-}

Some useful commands for performing **simple and multiple linear regression** are given in the next table. We assume that:

- `dataset` is an imported dataset such that
    - `resp` is the response variable
    - `pred1` is first predictor
    - `pred2` is second predictor
    - ...
    - `predk` is the last predictor
- `model` is the result of applying `lm`
- `newPreds` is a `data.frame` with variables named as the predictors
- `num` is `1`, `2` or `3`
- `level` is a number between 0 and 1

|               Description          |              `R`             |
|------------------------------------|------------------------------|
| **Fit a simple linear model ** | `lm(response ~ pred1, data = dataset)` |
| **Fit a multiple linear model... ** | |
| ... on two predictors | `lm(response ~ pred1 + pred2, data = dataset)` |
| ... on all predictors | `lm(response ~ ., data = dataset)` |
| ... on all predictors except `pred1` | `lm(response ~ . - pred1, data = dataset)` |
|  **Summarize linear model**: coefficient estimates, standard errors, $t$-values, $p$-values for $H_0:\beta_j=0$, $\hat\sigma$ (Residual standard error), degrees of freedom, $R^2$, Adjusted $R^2$, $F$-test, $p$-value for $H_0:\beta_1=\ldots=\beta_k=0$ | `summary(model)` |
| **ANOVA decomposition** | `anova(model)` |
| **CIs coefficients** | `confint(model, level = level)` |
| **Prediction** | `predict(model, newdata = new)` |
| **CIs predicted mean** | `predict(model, newdata = new, interval = "confidence", level = level)` |
| **CIs predicted response** | `predict(model, newdata = new, interval = "prediction", level = level)` |
| **Variable selection** | `stepwise(model)` |
| **Multicollinearity detection** | `vif(model)` |
| **Compare model coefficients** | `compareCoefs(model1, model2)` |
| **Diagnostic plots** | `plot(model, num)` |

#### More basic usage {-}

The following table contains important `R` commands for its **basic usage**. We assume the following dataset is available:
```{r, cache = TRUE}
data <- data.frame(x = 1:10, y = c(-1, 2, 3, 0, 3, 1, -1, 3, 0, -1))
```


|             Description         |        `R`       |              Example                |
|---------------------------------|------------------|-------------------------------------|
| **Data frame management** | | |
| variable names | `names` | `names(data)` |
| structure | `str` | `str(data)` |
| dimensions | `dim` | `dim(data)` |
| beginning | `head` | `head(data)` |
| **Vector related functions** | | |
| create sequences | `seq` | `seq(0, 1, l = 10); seq(0, 1, by = 0.25)` |
| reverse a vector | `rev` | `rev(1:5)` |
| length of a vectors | `length` | `length(1:5)` |
| count repetitions in a vector | `table` | `table(c(1:5, 4:2))` |
| **Logical conditions** | | |
| relational operators | `<`, `<=`, `>`, `>=`, `==`, `!=` | `1 < 0; 1 <= 1; 2 > 1; 3 >= 4; 1 == 0; 1 != 0` |
| "and" | `&` | `TRUE & FALSE` |
| "or" | `|` | `TRUE | FALSE` |
| **Subsetting** | | |
| vector | | `data$x[data$x > 0]; data$x[data$x > 2 & data$x < 8]` |
| data frame| | `data[data$x > 0, ]; data[data$x < 2 | data$x > 8, ]` |
| **Distributions** | | |
| sampling | `rxxxx` | `rnorm(n = 10, mean = 0, sd = 1)` |
| density | `dxxxx` | `x <- seq(-4, 4, l = 20); dnorm(x = x, mean = 0, sd = 1)` |
| distribution | `pxxxx` | `x <- seq(-4, 4, l = 20); pnorm(q = x, mean = 0, sd = 1)` |
| quantiles | `qxxxx` | `p <- seq(0.1, 0.9, l = 10); qnorm(p = p, mean = 0, sd = 1)` |
| **Plotting** | | |
| scatterplot | `plot` | `plot(rnorm(100), rnorm(100))` |
| plot a curve | `plot`, `seq` | `x <- seq(0, 1, l = 100); plot(x, x^2, type = "l")` |
| add lines | `lines`, | `x <- seq(0, 1, l = 100); plot(x, x^2 + rnorm(100, sd = 0.1)); lines(x, x^2, col = 2, lwd = 2)` |

#### Logistic regression {-}

Some useful commands for performing **logistic regression** are given in the next table. We assume that:

- `dataset` is an imported dataset such that
    - `resp` is the response binary variable
    - `pred1` is first predictor
    - `pred2` is second predictor
    - ...
    - `predk` is the last predictor
- `model` is the result of applying `glm`
- `newPreds` is a `data.frame` with variables named as the predictors
- `level` is a number between 0 and 1

|               Description          |              `R`             |
|------------------------------------|------------------------------|
| **Fit a simple logistic model** | `glm(response ~ pred1, data = dataset, family = "binomial")` |
| **Fit a multiple logistic model... ** | |
| ... on two predictors | `glm(response ~ pred1 + pred2, data = dataset, family = "binomial")` |
| ... on all predictors | `glm(response ~ ., data = dataset, family = "binomial")` |
| ... on all predictors except `pred1` | `glm(response ~ . - pred1, data = dataset, family = "binomial")` |
|  **Summarize logistic model**: coefficient estimates, standard errors, Wald statistics (`'z value'`), $p$-values for $H_0:\beta_j=0$, Null deviance, deviance (`'Residual deviance'`), AIC, number of iterations | `summary(model)` |
| **CIs coefficients** | `confint(model, level = level); confint.default(model, level = level)` |
| **CIs exp-coefficients** | `exp(confint(model, level = level)); exp(confint.default(model, level = level))` |
| **Prediction** | `predict(model, newdata = new, type = "response")` |
| **CIs predicted probability** | Not immediate. Use `predictCIsLogistic(model, newdata = new, level = level)` as seen in Section \@ref(logreg-prediction) |
| **Variable selection** | `stepwise(model)` |
| **Multicollinearity detection** | `vif(model)` |
| **$R^2$** | Not immediate. Use `r2Log(model = model)` as seen in Section \@ref(logreg-modsel) |
| **Hit matrix** | `table(data$resp, model$fitted.values > 0.5)` |

#### Principal component analysis {-}

Some useful commands for performing **logistic regression** are given in the next table. We assume that:

- `dataset` is an imported dataset with several **non-categorical variables** (the variables must be continuous or discrete).
- `pca` is a PCA object, this is, the output of `princomp`.

|               Description          |              `R`             |
|------------------------------------|------------------------------|
| **Compute a PCA... ** | |
| ... unnormalized (if variables have the same scale) | `princomp(dataset)` |
| ... normalized  (if variables have different scales) | `princomp(dataset, cor = TRUE)` |
|  **Summarize PCA**: standard deviation explained by *each* PC, proportion of variance explained by *each* PC, cumulative proportion of variance explained *up* to a given component | `summary(pca)` |
| **Weights** | `pca$loadings` |
| **Scores** | `pca$scores` |
| **Standard deviations of the PCs** | `pca$sdev` |
| **Means of the original variables** | `pca$center` |
| **Screeplot** | `plot(pca); plot(pca, type = "l")` |
| **Biplot** | `biplot(pca)` |

# Use of qualitative predictors in regression {#appendix-qualpred}

An important situation not covered in Chapters \@ref(simplin), \@ref(multlin) and \@ref(logreg) is how to deal with *qualitative*, and not *quantitative*, predictors. Qualitative predictors, also known as *categorical* variables or, in `R`'s terminology, *factors*, are ubiquitous in social sciences. Dealing with them requires some care and proper understanding of how these variables are represented in statistical softwares such as `R`.

#### Two levels {-}

The simplest case is the situation with **two levels**, this is, the binary case covered in logistic regression. There we saw that a binary variable $C$ with two levels (for example, *a* and *b*) could be represented as
\[
D=\left\{\begin{array}{ll}1,&\text{if }C=b,\\0,& \text{if }C=a.\end{array}\right.
\]
$D$ now is a *dummy variable*: it **codifies with zeros and ones the two possible levels of the categorical variable**. An example of $C$ could be *gender*, which has levels *male* and *female*. The dummy variable associated is $D=0$ if the gender is male and $D=1$ if the gender is female.

The advantage of this *dummification* is its interpretability in regression models. Since level *a* corresponds to $0$, it can be seen as the *reference level* to which level *b* is compared. This is the key point in dummification: set one level as the reference and codify the rest as departures from it with ones.

The previous interpretation translates easily to regression models. Assume that the dummy variable $D$ is available together with other predictors $X_1,\ldots,X_k$. Then:

- **Linear model**
\[
\mathbb{E}[Y|X_1=x_1,\ldots,X_k=x_k,D=d]=\beta_0+\beta_1X_1+\ldots+\beta_kX_k+\beta_{k+1}D.
\]
The coefficient associated to $D$ is easily interpretable. $\beta_{k+1}$ is the increment in mean of $Y$ associated to changing $D=0$ (reference) to $D=1$, while the rest of the predictors are fixed. Or in other words, $\beta_{k+1}$ is the increment in mean of $Y$ associated to changing of the level of the categorical variable from *a* to *b*.

- **Logistic model**
\[
\mathbb{P}[Y=1|X_1=x_1,\ldots,X_k=x_k,D=d]=\text{logisitic}(\beta_0+\beta_1X_1+\ldots+\beta_kX_k+\beta_{k+1}D).
\]
We have two interpretations of $\beta_{k+1}$, either in terms of log-odds or odds:

    - $\beta_{k+1}$ is the *additive* increment in log-odds of $Y$ associated to changing of the level of the categorical variable from *a* (reference, $D=0$) to *b* ($D=1$).
    - $e^{\beta_{k+1}}$ is the *multiplicative* increment in odds of $Y$ associated to changing of the level of the categorical variable from *a* (reference, $D=0$) to *b* ($D=1$).

`R` does the dummification automatically (translates a categorical variable $C$ into its dummy version $D$) if it detects that a factor variable is present in the regression model. Let's see an example of this in linear and logistic regression.
```{r, collapse = TRUE, cache = TRUE}
# Load the Boston dataset
library(MASS)
data(Boston)

# Structure of the data
str(Boston)
# chas is a dummy variable measuring if the suburb is close to the river (1)
# or not (0). In this case it is not codified as a factor but as a 0 or 1.

# Summary of a linear model
mod <- lm(medv ~ chas + crim, data = Boston)
summary(mod)
# The coefficient associated to chas is 5.57772. That means that if the suburb
# is close to the river, the mean of medv increases in 5.57772 units.
# chas is significant (the presence of the river adds a valuable information
# for explaining medv)

# Create a binary response (1 expensive suburb, 0 inexpensive)
Boston$expensive <- Boston$medv > 25

# Summary of a logistic model
mod <- glm(expensive ~ chas + crim, data = Boston, family = "binomial")
summary(mod)
# The coefficient associated to chas is 1.04165. That means that if the suburb
# is close to the river, the log-odds of expensive increases by 1.04165.
# Alternatively, the odds of expensive increases by a factor of exp(1.04165).
# chas is significant (the presence of the river adds a valuable information
# for explaining medv)
```

#### More than two levels {-}

Let's see now the case with **more than two levels**, for example, a categorical variable $C$ with levels *a*, *b* and *c*. If we take *a* as the reference level, this variable can be represented by *two* dummy variables:
\[
D_1=\left\{\begin{array}{ll}1,&\text{if }C=b,\\0,& \text{if }C\neq b\end{array}\right.
\]
and
\[
D_2=\left\{\begin{array}{ll}1,&\text{if }C=c,\\0,& \text{if }C\neq c.\end{array}\right.
\]
Then $C=a$ is represented by $D_1=D_2=0$, $C=b$ is represented by $D_1=1,D_2=0$ and $C=c$ is represented by $D_1=0,D_2=1$. The interpretation of the regression models with the presence of $D_1$ and $D_2$ is the very similar to the one before. For example, for the linear model, the coefficient associated to $D_1$ gives the increment in mean of $Y$ when the category of $C$ changes from *a* to *b*. The coefficient for $D_2$ gives the increment in mean of $Y$ when it changes from *a* to *c*.

In general, if we have a categorical variable with $J$ levels, then the number of dummy variables required is $J-1$. Again, `R` does the dummification automatically for you if it detects that a factor variable is present in the regression model.
```{r, collapse = TRUE, cache = TRUE}
# Load dataset - factors in the last column
data(iris)
summary(iris)

# Summary of a linear model
mod1 <- lm(Sepal.Length ~ ., data = iris)
summary(mod1)
# Speciesversicolor (D1) coefficient: -0.72356. The average increment of
# Sepal.Length when the species is versicolor instead of setosa (reference).
# Speciesvirginica (D2) coefficient: -1.02350. The average increment of
# Sepal.Length when the species is virginica instead of setosa (reference).
# Both dummy variables are significant

# How to set a different level as reference (versicolor)
iris$Species <- relevel(iris$Species, ref = "versicolor")

# Same estimates except for the dummy coefficients
mod2 <- lm(Sepal.Length ~ ., data = iris)
summary(mod2)
# Speciessetosa (D1) coefficient: 0.72356. The average increment of
# Sepal.Length when the species is setosa instead of versicolor (reference).
# Speciesvirginica (D2) coefficient: -0.29994.s The average increment of
# Sepal.Length when the species is virginica instead of versicolor (reference).
# Both dummy variables are significant

# Coefficients of the model
confint(mod2)
# The coefficients of Speciesversicolor and Speciesvirginica are significantly
# negative. Therefore, there are significant
```

```{block, type = 'rmdcaution'}
**Do not codify a categorical variable as a discrete variable**. This constitutes a major methodological fail that will flaw the subsequent statistical analysis.

For example if you have a categorical variable `party` with levels `partyA`, `partyB` and `partyC`, do not encode it as a discrete variable taking the values `1`, `2` and `3`, respectively. If you do so:

- You assume implicitly an order in the levels of `party`, since `partyA` is closer to `partyB` than to `partyC`.
- You assume implicitly that `partyC` is three times larger than `partyA`.
- The codification is completely arbitrary -- why not considering `1`, `1.5` and `1.75` instead of?

The right way of dealing with categorical variables in regression is to set the variable as a factor and let `R` do internally the dummification.
```

# Multinomial logistic regression {#appendix-multinomialreg}

The logistic model can be generalized to categorical variables $Y$ with more than two possible levels, namely $\{1,\ldots,J\}$. Given the predictors $X_1,\ldots,X_k$, *multinomial logistic regression* models the probability of each level $j$ of $Y$ by
\begin{align}
p_j(\mathbf{x})=\mathbb{P}[Y=j|X_1=x_1,\ldots,X_k=x_k]=\frac{e^{\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k}}{1+\sum_{l=1}^{J-1}e^{\beta_{0l}+\beta_{1l}X_1+\ldots+\beta_{kl}X_k}} (\#eq:multinom-1)
\end{align}
for $j=1,\ldots,J-1$ and (for the last level $J$)
\begin{align}
p_J(\mathbf{x})=\mathbb{P}[Y=J|X_1=x_1,\ldots,X_k=x_k]=\frac{1}{1+\sum_{l=1}^{J-1}e^{\beta_{0l}+\beta_{1l}X_1+\ldots+\beta_{kl}X_k}}. (\#eq:multinom-2)
\end{align}
Note that \@ref(eq:multinom-1) and \@ref(eq:multinom-2) imply that $\sum_{j=1}^J p_j(\mathbf{x})=1$ and that there are $(J-1)\times(k+1)$ coefficients ($(J-1)$ intercepts and $(J-1)\times k$ slopes). Also, \@ref(eq:multinom-2) reveals that the last level, $J$, is given a different treatment. This is because it is the *reference level* (it could be a different one, but is tradition to choose the last one).

The multinomial logistic model has an interesting interpretation in terms of logistic regressions. Taking the quotient between \@ref(eq:multinom-1) and \@ref(eq:multinom-2) gives
\begin{align}
\frac{p_j(\mathbf{x})}{p_J(\mathbf{x})}=e^{\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k}(\#eq:multinom-4)
\end{align}
for $j=1,\ldots,J-1$. Therefore, applying a logarithm to both sides we have:
\begin{align}
\log\frac{p_j(\mathbf{x})}{p_J(\mathbf{x})}=\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k.(\#eq:multinom-3)
\end{align}
This equation is indeed very similar to \@ref(eq:eq-odds-log2). If $J=2$, it is the same up to a change in the codes for the levels: the logistic regression giving the probability of $Y=1$ versus $Y=2$. On the LHS of \@ref(eq:multinom-3) we have the logarithm of the ratio of two probabilities and on the RHS a linear combination of the predictors. **If the probabilities on the LHS were complementary** (if they added up to one), then we would have a **log-odds and hence a logistic regression** for $Y$. This is not the situation, but it is close: instead of odds and log-odds, we have *ratios* and *log-ratios* of non complementary probabilities. Also, it gives a good insight on what the multinomial logistic regression is: **a set of $J-1$ independent "logistic regressions" for the probability of $Y=j$ versus the probability of the reference $Y=J$**.

Equation \@ref(eq:multinom-4) gives also interpretation on the coefficients of the model since
\[
p_j(\mathbf{x})=e^{\beta_{0j}+\beta_{1j}X_1+\ldots+\beta_{kj}X_k}p_J(\mathbf{x}).
\]
Therefore:

- $e^{\beta_{0j}}$: is the ratio between $p_j(\mathbf{0})/p_J(\mathbf{0})$, the probabilities of $Y=j$ and $Y=J$ when $X_1=\ldots=X_k=0$. If $e^{\beta_{0j}}>1$ (equivalently, $\beta_{0j}>0$), then $Y=j$ is more likely than $Y=J$. If $e^{\beta_{0j}}<1$ ($\beta_{0j}<0$), then $Y=j$ is less likely than $Y=J$.
- $e^{\beta_{lj}}$, $l\geq1$: is the **multiplicative** increment of the ratio between $p_j(\mathbf{x})/p_J(\mathbf{x})$ for an increment of one unit in $X_l=x_l$, provided that the remaining variables $X_1,\ldots,X_{l-1},X_{l+1},\ldots,X_k$ *do not change*. If $e^{\beta_{lj}}>1$ (equivalently, $\beta_{lj}>0$), then $Y=j$ becomes more likely than $Y=J$ for each increment in $X_j$. If $e^{\beta_{lj}}<1$ ($\beta_{lj}<0$), then $Y=j$ becomes less likely than $Y=J$.

The following code illustrates how to compute a basic multinomial regression in `R`.
```{r, collapse = TRUE, cache = TRUE}
# Package included in R that implements multinomial regression
library(nnet)

# Data from the voting intentions in the 1988 Chilean national plebiscite
data(Chile)
summary(Chile)
# vote is a factor with levels A (abstention), N (against Pinochet),
# U (undecided), Y (for Pinochet)

# Fit of the model done by multinom: Response ~ Predictors
# It is an iterative procedure (maxit sets the maximum number of iterations)
# Read the documentation in ?multinom for more information
mod1 <- multinom(vote ~ age + education + statusquo, data = Chile,
                 maxit = 1e3)

# Each row of coefficients gives the coefficients of the logistic
# regression of a level versus the reference level (A)
summary(mod1)

# Set a different level as the reference (N) for easening interpretations
Chile$vote <- relevel(Chile$vote, ref = "N")
mod2 <- multinom(vote ~ age + education + statusquo, data = Chile,
                 maxit = 1e3)
summary(mod2)
exp(coef(mod2))
# Some highlights:
# - intercepts do not have too much interpretation (correspond to age = 0).
#   A possible solution is to center age by its mean (so age = 0 would
#   represent the mean of the ages)
# - both age and statusquo increase the probability of voting Y, A or U
#   with respect to voting N -> conservativeness increases with ages
# - both age and statusquo increase more the probability of voting Y and U
#   than A -> elderly and status quo supporters are more decided to participate
# - a PS level of education increases the probability of voting N. Same for
#   a S level of education, but more prone to A

# Prediction of votes - three profile of voters
newdata <- data.frame(age = c(23, 40, 50),
                      education = c("PS", "S", "P"),
                      statusquo = c(-1, 0, 2))

# Probabilities of belonging to each class
predict(mod2, newdata = newdata, type = "probs")

# Predicted class
predict(mod2, newdata = newdata, type = "class")
```

```{block, type = 'rmdcaution'}
Multinomial logistic regression will suffer from numerical instabilities and its iterative algorithm might even fail to converge if the levels of the categorical variable are very separated (e.g., two data clouds clearly separated corresponding to a different level of the categorical variable).
```

```{block, type = 'rmdcaution'}
The multinomial model employs $(J-1)(k+1)$ parameters. It is easy to end up with **complex models** -- that require a **large sample size** to be fitted properly -- if the response has a few number of levels and there are several predictors. For example, with $5$ levels and $8$ predictors we will have $36$ parameters. Estimating this model with $50-100$ observations will probably result in **overfitting**.
```

# Reporting with `R` and `R Commander` {#appendix-reporting}

A nice feature of `R Commander` is that integrates seamless with `R Markdown`, which is able to create `.html`, `.pdf` and `.docx` reports directly from the outputs of `R`. Depending on the kind of report that we want, we will need the following auxiliary software^[Alternatively, the `'Tools' -> 'Install auxiliary software [if not already installed]'` will redirect you to the download links for the auxiliary software.]:

- `.html`. No extra software is required.
- `.docx` and `.rtf`. You must install `Pandoc`, a document converter software. Download it [here](http://pandoc.org/installing.html).
- `.pdf` (only recommended for experts). An installation of LaTeX, additionally to `Pandoc`, is needed. Download LaTeX [here](https://www.latex-project.org/get/).

The workflow is simple. Once you have done some statistical analysis, either by using `R Commander`'s menus or `R` code directly, you will end up with an `R` script, on the `'R Script'` tab, that contains all the commands you have run so far. Switch then to the `'R Markdown'` tab and you will see the commands you have entered in a different layout, which essentially encapsulates the code into chunks delimited by `` ```{r}`` and `` ``` ``. This will generate a report once you click in the `'Generate report'` button.

Let's illustrate this process through an example. Suppose we were analyzing the `Boston` dataset, as we did in Section \@ref(multlin-examps-boston). *Ideally*^[This is, assuming we have performed the right steps in the analysis without making any mistake.] our final script would be something like this:
```{r, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
library(RcmdrMisc)
```
```{r, eval = FALSE}
# A simple and non-exhaustive analysis for the price of the houses in the Boston
# dataset. The purpose is to quantify, by means of a multiple linear model,
# the effect of 14 variables in the price of a house in the suburbs of Boston.

# Import data
library(MASS)
data(Boston)

# Make a multiple linear regression of medv in the rest of variables
mod <- lm(medv ~ ., data = Boston)
summary(mod)

# Check the linearity assumption
plot(mod, 1) # Clear non-linearity

# Let's consider the transformations given in Harrison and Rubinfeld (1978)
modTransf <- lm(I(log(medv * 1000)) ~ I(rm^2) + age + log(dis) +
                  log(rad) + tax + ptratio + I(black / 1000) +
                  I(log(lstat / 100)) + crim + zn + indus + chas +
                  I((10 * nox)^2), data = Boston)
summary(modTransf)

# The non-linearity is more subtle now
plot(modTransf, 1)

# Look for the best model in terms of the BIC
modTransfBIC <- stepwise(modTransf)
summary(modTransfBIC)

# Let's explore the most significant variables, to see if the model can be
# reduced drastically in complexity
mod3D <- lm(I(log(medv * 1000)) ~ I(log(lstat / 100)) + crim, data = Boston)
summary(mod3D)

# With only 2 variables, we explain the 72% of variability.
# Compared with the 80% with 10 variables, it is an important improvement
# in terms of simplicity.

# Let's add these variables to the dataset, so we can call scatterplotMatrix
# and scatter3d through R Commander's menu
Boston$logMedv <- log(Boston$medv * 1000)
Boston$logLstat <- log(Boston$lstat / 100)

# Visualize the pair-by-pair relations of the response and two predictors
scatterplotMatrix(~ crim + logLstat + logMedv, reg.line = lm, smooth = FALSE,
                  spread = FALSE, span = 0.5, ellipse = FALSE,
                  levels = c(.5, .9), id.n = 0, diagonal = 'histogram',
                  data = Boston)

# Visualize the full relation between the response and the two predictors
scatter3d(logMedv ~ crim + logLstat, data = Boston, fit = "linear",
          residuals = TRUE, bg = "white", axis.scales = TRUE, grid = TRUE,
          ellipsoid = FALSE)
```
This contains all the major points in the analysis, that now can be expanded and detailed. You can download the script [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report-script.R), open it through `'File' -> 'Open script file...'` and run it by yourself in `R Commander`. If you so, and then switch to the `R Markdown` tab, you will see this:

    ---
    title: "Replace with Main Title"
    author: "Your Name"
    date: "AUTOMATIC"
    ---

    `r ''````{r echo=FALSE, message=FALSE}
    # include this code chunk as-is to set options
    knitr::opts_chunk$set(comment=NA, prompt=TRUE)
    library(Rcmdr)
    library(car)
    library(RcmdrMisc)
    ```

    `r ''````{r echo=FALSE}
    # include this code chunk as-is to enable 3D graphs
    library(rgl)
    knitr::knit_hooks$set(webgl = hook_webgl)
    ```

    `r ''````{r}
    # A simple and non-exhaustive analysis for the price of the houses in the Boston
    ```

    `r ''````{r}
    # dataset. The purpose is to quantify, by means of a multiple linear model,
    ```

    `r ''````{r}
    # the effect of 14 variables in the price of a house in the suburbs of Boston.
    ```

    `r ''````{r}
    # Import data
    ```

    `r ''````{r}
    library(MASS)
    ```

    `r ''````{r}
    data(Boston)
    ```

    `r ''````{r}
    # Make a multiple linear regression of medv in the rest of variables
    ```

    `r ''````{r}
    mod <- lm(medv ~ ., data = Boston)
    ```

    `r ''````{r}
    summary(mod)
    ```

    [More outputs - omitted]
    ```

The complete, lengthy, file can be downloaded [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/crude-report.Rmd). This is an `R Markdown` file, which has extension `.Rmd`. As you can see, by default, `R Commander` will generate a *code chunk* like

    `r ''````{r}
    code line
    ```

for each `code line` you run in `R Commander`. You probably will want to modify this *crude* report manually by merging chunks of code, removing comments or adding more information in between chunks of code. To do so, go to `'Edit' -> 'Edit Markdown document'`. Here you can also remove unnecessary chunks of code resulting from any mistake or irrelevant analyses.

The following file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.Rmd)) could be a final report. Pay attention to the numerous changes with respect to the previous one:

    ---
    title: "What makes a house valuable?"
    subtitle: "A reproducible analysis in the Boston suburbs"
    author: "Outstanding student 1, Awesome student 2 and Great student 3"
    date: "31/11/16"
    ---

    `r ''````{r echo=FALSE, message=FALSE, warning=FALSE}
    # include this code chunk as-is to set options
    knitr::opts_chunk$set(comment=NA, prompt=TRUE)
    library(Rcmdr)
    library(car)
    library(RcmdrMisc)
    ```

    `r ''````{r echo=FALSE, message=FALSE, warning=FALSE}
    # include this code chunk as-is to enable 3D graphs
    library(rgl)
    knitr::knit_hooks$set(webgl = hook_webgl)
    ```

    This short report shows a simple and non-exhaustive analysis for the price of
    the houses in the `Boston` dataset. The purpose is to quantify, by means of a
    multiple linear model, the effect of 14 variables in the price of a house in
    the suburbs of Boston.

    We start by importing the data into `R` and considering a multiple linear
    regression of `medv` (median house value) in the rest of variables:
    `r ''````{r}
    # Import data
    library(MASS)
    data(Boston)
    ```

    `r ''````{r}
    mod <- lm(medv ~ ., data = Boston)
    summary(mod)
    ```
    The variables `indus` and `age` are non-significant in this model. Also,
    although the adjusted R-squared is high, there seems to be a clear
    non-linearity:
    `r ''````{r}
    plot(mod, 1)
    ```

    In order to bypass the non-linearity, we are going to consider the
    non-linear transformations given in Harrison and Rubinfeld (1978)
    for both the response and the predictors:
    `r ''````{r}
    modTransf <- lm(I(log(medv * 1000)) ~ I(rm^2) + age + log(dis) +
                    log(rad) + tax + ptratio + I(black / 1000) +
                    I(log(lstat / 100)) + crim + zn + indus + chas +
                    I((10*nox)^2), data = Boston)
    summary(modTransf)
    ```
    The adjusted R-squared is now higher and, what is more important, the
    non-linearity now is more subtle (it is still not linear but closer
    than before):
    `r ''````{r}
    plot(modTransf, 1)
    ```

    However, `modTransf` has more non-significant variables. Let\'s see if
    we can improve over the previous model by removing some of the
    non-significant variables? To see this, we look for the best model in
    terms of the Bayesian Information Criterion (BIC) by `stepwise`:
    `r ''````{r}
    modTransfBIC <- stepwise(modTransf, trace = 0)
    summary(modTransfBIC)
    ```
    The resulting model has a slightly higher adjusted R-squared than `modTransf`
    with all the variables significant.

    We explore the most significant variables to see if the model can be reduced
    drastically in complexity.
    `r ''````{r}
    mod3D <- lm(I(log(medv * 1000)) ~ I(log(lstat / 100)) + crim, data = Boston)
    summary(mod3D)
    ```

    It turns out that **with only 2 variables, we explain the 72% of variability**.
    Compared with the 80% with 10 variables, it is an important improvement
    in terms of simplicity: the logarithm of `lstat` (percent of lower status of
    the population) and `crim` (crime rate) alone explain the 72% of the
    variability in the house prices.

    We add these variables to the dataset, so we can call `scatterplotMatrix` and
    `scatter3d` through `R Commander`,
    `r ''````{r}
    Boston$logMedv <- log(Boston$medv * 1000)
    Boston$logLstat <- log(Boston$lstat / 100)
    ```
    and conclude with the visualization of:

    1. the pair-by-pair relations of the response and the two predictors;
    2. the full relation between the response and the two predictors.
    `r ''````{r}
    # 1
    scatterplotMatrix(~ crim + logLstat + logMedv, reg.line = lm, smooth = FALSE,
                      spread = FALSE, span = 0.5, ellipse = FALSE,
                      levels = c(.5, .9), id.n = 0, diagonal = 'histogram',
                      data = Boston)
    ```
    `r ''````{r webgl = TRUE}
    # 2
    scatter3d(logMedv ~ crim + logLstat, data = Boston, fit = "linear",
              residuals = TRUE, bg = "white", axis.scales = TRUE, grid = TRUE,
              ellipsoid = FALSE)
    ```

When we click on `'Generate report'` for the above `R Markdown` file, we should get the following output files:

- `.html`: [visualize](http://htmlpreview.github.io/?https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.html) and [download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.html). Once it is produced, this file is difficult to modify, but very easy to distribute (anyone with a browser can see it).
- `.docx`: [visualize and download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.docx). Easy to modify in a document processor like Microsoft Office. Easy to distribute.
- `.rtf`: [download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.rtf). Easy to modify in a document processor, not very elegant.
- `.pdf`: [visualize and download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/reporting/report.pdf). Elegant and easy to distribute, but hard to modify once it is produced.

```{block, type = 'rmdtip'}
For advanced users, there is a lot of information on mastering `R Markdown` [here](http://rmarkdown.rstudio.com/lesson-1.html) by using `RStudio`, a more advanced framework than `R Commander`.
```

# Group project {#appendix-project}

#### Groups {-}

You will team up in groups of 3 to 5 members. It is up to you to form the groups based on your grade expectations, affinity, complementary skills, etc. You must communicate the group compositions **no later than November the 30th** by a (single) email to <edgarcia@est-econ.uc3m.es> detailing the members of the group and, if you have it, a preliminary description about the topic of the project (~ 3 lines).

#### Aim of the project {-}

You will analyze a real dataset of your choice using the statistical methodology that we have seen in the lessons and labs. The purpose is to demonstrate that you know how to apply and interpret some of the studied statistical techniques (such as simple/multiple linear regression, logistic regression, or any other methods covered in the course) in a real-case scenario that is appealing for you.

#### Structure of the report {-}

Use the following *mandatory* structure when writing your report:

0. **Abstract**. Provide a concise summary of the project. It must not exceed 250 words.
1. **Introduction**. State what is the problem to be studied. Provide some context, the question(s) that you want to address, a motivation of its importance, references, etc. Remember how we introduced the case studies covered in the course as a template (but you will need to elaborate more).
2. **Statistical analysis**. Make use of some of the aforementioned statistical techniques, the ones that are more convenient to your particular case study. You can choose between covering several at a more superficial level, or one or two in more depth. Justify their adequacy and obtain analyses, explaining how you did it, in the form of plots and summaries. Provide a critical discussion about the outputs and give insights about them.
3. **Conclusions**. Summary of what was addressed in the project and of the most important conclusions. Takeaway messages. The conclusions are not required to be spectacular, but *fair and honest* in terms of what you discovered.
4. **References**. Refer to the sources of information that you have employed (for the data, for information on the data, for the statistical analyses, etc).

*Mandatory* format guidelines:

- Structure: title, authors, abstract, first section, second section and so on. Like [this](http://cje.oxfordjournals.org/content/38/2/257.full.pdf+html). Do not use a cover.
- Font size: 12 points.
- Spacing: single space, single column.
- Length limit: less than 5000 words **and** 15 pages. You are **not** required to make use of all the space.

#### Grading {-}

All students in a group will be graded evenly. Take this into account when forming the group. The grading is on a scale of 0-10 (plus 2 bonus points) and will be performed according to the following breakdown:

- **Originality** of the problem studied and data acquisition process (up to 2 points).
- **Statistical analyses presented** and their depth (up to 3 points). At least two different techniques should be employed (simple and multiple linear regression count as different, but the use of other techniques as well is mostly encouraged). Graded depending on their adequacy to the problem studied and the evidence you demonstrate about your knowledge of them.
- Accuracy of the **interpretation** of the analyses (up to 2 points). Graded depending on the detail and rigor of the insights you elaborate from the statistical analyses.
- **Reproducibility** of the study (1.5 point). Awarded if the code for reproducing the study, as well as the data, is provided in a ready-to-use way (e.g. the outputs from `R Commander`'s report mode along with the data).
- **Presentation** of the report (1.5 point). This involves the correct usage of English, the readability, the conciseness and the overall presentation quality.
- **Excellence** (2 bonus points). Awarded for creativity of the analysis,  use of advanced statistical tools, use of points briefly covered in lessons/labs, advanced insights into the methods, completeness of the report, use of advanced presentation tools, etc. Only awarded if the sum of regular points is above 7.5.

The ratio "quality project"/"group size" might be taken into account in extreme cases (e.g. poor report written by 5 people, extremely good report written by 3 people).

#### Academic fraud {-}

Evidences of academic fraud will have serious consequences, such as a zero grade for the whole group and the reporting of the fraud detection to the pertinent academic authorities. Academic fraud includes (but is not limited to) plagiarism, use of sources without proper credit, project outsourcing and the use of external tutoring not mentioned explicitly.

#### Tips {-}

- Think about a topic that could be reused for other subjects, or take inspiration from previous projects you did. In that way, this project could serve as the *quantification* of another subject's project. If you do this, **add an explicit mention** in the report.
- Data sources. Here are some useful data sources:
	* A [list of all the datasets included in `R`](http://vincentarelbundock.github.io/Rdatasets/datasets.html). See [Section 2.9.1](https://bookdown.org/egarpor/SSS2-UC3M/exercises-and-case-studies.html) of lab notes for how to load them.
	* [Some datasets employed in the course](https://bookdown.org/egarpor/SSS2-UC3M/datasets-for-the-course.html).
	* [The World Bank](http://data.worldbank.org) contains a huge collection of economic and sociological variables for countries and regions, for long periods of time.
	* [SIPRI](https://www.sipri.org/) contains several databases about international transfers of arms.
	* The [Global Health Observatory](http://www.who.int/gho/database/en/) is the World Health Organization's main health statistics repository.
	* Sport statistics (teams, players) are a great source if you like sports. Sport webpages usually have a section on statistics.
- Inspiration for the project's topic.
	* The case studies covered (and left as exercises) in the lab notes might serve as a good starting point for defining a project.
	* [The Economist](http://www.economist.com/) usually has some good and up-to-date political/economical analyses that could serve as motivation.
	* Try to quantify the impact in society of certain laws (traffic, education, gender violence, etc).
	* Is there a continuous variable that you would like to predict from others? (linear regression)
	* Is there a binary variable that you would like to predict from others? (logistic regression)
	* Would you like to assess which combination of variables explain most of the variability of your data, so you can visualize it in 2D or 3D? (principal component analysis)
	* Would you like aggregate individuals according to several characteristics in order to classify them? (clustering)
- Use `R Commander`'s report mode (Appendix \@ref(reporting-with-r-and-r-commander)) to simplify the generation of graphs and summaries directly from the statistical analysis. Use that code to make the analysis reproducible.
- Make use of office hours before it is too late.
- Pro-tip: if you come to my office with a printed draft of the project, I can provide you some quick feedback on what could be improved.

#### Deadline {-}

Submit the reports before December the 23rd at 16:59 through Aula Global. **Not by email**. Reports received after the deadline will not be evaluated.
