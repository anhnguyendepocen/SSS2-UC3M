# Simple linear regression {#simp}

The simple linear regression is a *simple* but useful statistical tool. It allows us to quantify the linear relation between two variables in a proper way. Let's see what it can do in real-case scenarios!

## Examples and applications

### PISA scores and GDPp

The Programme for International Student Assessment (PISA) is a study carried out in 65 countries with the purpose of evaluating the performance of 15-year-old pupils on mathematics, science, and reading.

A phenomena observed over years is that ''wealthy countries tend to achieve larger average scores''. The purpose of this case study is to answer these two questions:

- *Is the educational level of a country influenced by its economic wealth?*
- *If so, up to what precise extent?*

The `pisa.csv` file contains the 65 rows corresponding to the countries that took part on the PISA study. Each row has the variables `Country`, `MeanMath`, `MathShareLow`, `MathShareTop`, `ReadingMean`, `ScienceMean`, `GDPp`, `logGDPp` and `HighIncome`. A small subset of the data is shown in Table \@ref(tab:pisatable). 

```{r pisatable, echo = FALSE}
pisa <- read.csv(file = "datasets/pisa.csv", header = TRUE, row.names = 1)
knitr::kable(
  head(pisa[, c(1, 4, 5, 7, 8)], 10),
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'First 10 rows of the `pisa` dataset for a selection of variables.'
)
```

We definitely need a way of summarizing this ammount of information! What we are going to do is:

1. **Import the data into `R Commander`**.

    1.1. Go to `'Data' -> 'Import data' -> 'from text file, clipboard, or URL...'`. The following window will pop-up:
    
    ```{r read, echo = FALSE, out.width = '40%', fig.cap = 'Screenshot '}
    knitr::include_graphics("images/screenshots/read.png")
    ```

    1.2. Select the appropaite options given the kind file (open it in a text editor helps). 
    
    1.3. Click on `'View data set'` to see if the importation was fine.
    
2. **Fit a linear model**.

    2.1. XXX
    
    ```{r lm, echo = FALSE, out.width = '70%', fig.cap = 'Screenshot '}
    knitr::include_graphics("images/screenshots/lm.png")
    ```
    
    2.2. XXX
    
    2.3. XXX
    
    ```{r}
    pisaLinearModel <- lm(MathMean ~ logGDPp, data = pisa)
    summary(pisaLinearModel)
    ```

3. **Visualize the fitted regression line**.

    2.1. XXX
    
    ```{r scatter, echo = FALSE, out.width = '45%', fig.cap = 'Screenshot ', multiplot = TRUE}
    knitr::include_graphics(c("images/screenshots/scatterplot1.png", "images/screenshots/scatterplot2.png"))
    ```

    2.2. XXX
    
    2.3. XXX
    
    ```{r echo = FALSE, warning = FALSE}
    library(splines)
    library(car)
    library(sandwich)
    ```
    ```{r}
    scatterplot(MathMean ~ logGDPp, reg.line = lm, smooth = FALSE, spread = FALSE, id.method = 'mahal', id.n = 3, boxplots = FALSE, span = 0.5, ellipse = FALSE, levels = c(.5, .9), main = "Average Math score vs. GDPp", pch = c(16), data = pisa)
    ```


```{block, type = 'rmdnote'}
Repeat the regression analysis for:
- `MeanReading` on `logGDPp`. 
- `MeanScience` on `logGDPp`. 
- `MeanReading` on `MeanMath`. 
```


<!-- 
### Sustained increasing of tuition fees in the US

> **Exercise**
### Approportion 
- Escaños por provincia vs población Europa y US
- Incremento tasas universitarias

## Model formulation and assumptions

## Estimation of model parameters by least squares

## Inference for model parameters

## Forecasting

## Assesing model fit and ANOVA

## Model diagnostics

## Nonlinear relationships

- Exponential growth
- High order terms

## Nottingham 

### Motivating example

Suppose we have a production line where we measured the time (in minutes) required to produce a number of solicited items. There are two random variables: `y` = "time required to produce an order" and `x` = "number of items in the order". We want to explain/predict `y` from `x` from a linear model.

```{r}

# Production time
y <- c(175, 189, 344, 88, 114, 338, 271, 173, 284, 277, 337, 58, 146, 277, 123, 227, 63, 337, 146, 68)

# Number of units in the order
x <- c(195, 215, 243, 162, 185, 231, 234, 166, 253, 196, 220, 168, 207, 225, 169, 215, 147, 230, 208, 172)

# Data scatterplot (pch = 16 for filled points)
plot(y ~ x, pch = 16)

```

### The `lm` function

The linear fit in `R` is done by the `lm` function and the formula `y ~ x`, used to denote that we are interested in regressing `y` over `x`. 

```{r}

# Fit a linear model y = beta0 + beta1 * x
reg <- lm(y ~ x)

# The result is a list with several objects
names(reg)

# The fitted coefficients beta0 (intercept) and beta1 (slope)
reg$coefficients

# The regression line with the minimized distances
plot(y ~ x, pch = 16)
abline(reg, col = 2)
segments(x0 = x, y0 = y, x1 = x, y1 = reg$fitted.values)

```

### The least squares estimate

We can check and visualize that `reg$coefficients` indeed contains the least squares estimates and that they minimize the Residual Sum of Squares (RSS).

```{r}

# Create the design matrix
X <- cbind(1, x)

# The analytical solution
betahat <- solve(t(X) %*% X) %*% t(X) %*% y
betahat

# Minimal RSS
sum((y - X %*% betahat)^2)
```

You can try to get a better RSS. Good luck! :)

```{r, echo = FALSE}

# # Define input
# inputPanel(
#   
#   sliderInput("beta0", label = "Intercept:",
#               min = -500, max = 300, value = reg$coefficients[1] + 600, step = 5),
#   sliderInput("beta1", label = "Slope:",
#               min = -3, max = 8, value = 0, step = 0.01)
#   
# )
# 
# # Create plot
# renderPlot({
#   plot(y ~ x, main = paste("RSS:", round(sum((input$beta0 + input$beta1 * x - y)^2), 1)), pch = 16, xlim = c(100, 300), ylim = c(0, 400))
#   abline(a = input$beta0, b = input$beta1, col = 2)
#   segments(x0 = x, y0 = y, x1 = x, y1 = input$beta0 + input$beta1 * x)
# })
  
```

```{r, echo = FALSE}
# 
#  ### Define a line
#   data_line <- data.frame(
#     x_rng = c(0, 1), 
#     y_rng = c(reg$coefficients[1], reg$coefficients[1] + reg$coefficients[2])
#   )     
#   
# p <- ggvis(data.frame("x" = x, "y" = y), x = ~x, y = ~y)
# layer_points(p) %>%
# layer_model_predictions(model = "lm", stroke := "red") %>%
#     layer_paths(x = ~x_rng, y = ~y_rng, stroke := "blue", data = data_line) %>% 
#   scale_numeric("x", domain = c(140, 270), nice = FALSE, clamp = TRUE) %>% 
#   scale_numeric("y", domain = c(0, 400), nice = FALSE, clamp = TRUE)
# 
```


### Summary of the model

The `summary` function applied to a `lm` object gives the fitted coefficients and its significances ("Pr(>|t|)"), the $R^2$ ("Multiple R-squared") and the fitted error variance ("Residual standard error").
 
```{r}

# Summary of the fit
summary(reg)

```

### Prediction

Prediction of a new observation can be done via the function `predict`, which also provides conficence intervals. The `newdata` argument of `predict` needs a `data.frame`.

```{r}

# Point in which we want a prediction for y
newx <- data.frame(x = 200)

# Prediction with 95% confidence interval
predict(reg, newdata = newx, interval = "prediction", level = 0.95)

# The same prediction
reg$coefficients %*% c(1, 200)

```

### Some words of caution with $R^2$

$R^2$ does not measure the correctness of a linear model but the **usefulness assuming the model is correct**. 

```{r}

# Fixed design
xf <- seq(0, 1, l = 50)

# Errors with different variance
set.seed(123456)
eps1 <- rnorm(50, sd = 0.1)
eps2 <- rnorm(50, sd = 1)

# Responses generated following a linear model
y1 <- 1 + 2 * xf + eps1
y2 <- 1 + 2 * xf + eps2

# Fits
reg1 <- lm(y1 ~ xf)
reg2 <- lm(y2 ~ xf)

# R^2 depends on the 
summary(reg1)
summary(reg2)

# Plot
par(mfrow = c(1, 2))
plot(y1 ~ xf, pch = 16, ylim = c(-1, 5))
abline(a = 1, b = 2, col = 2)
abline(a = reg1$coefficients[1], b = reg1$coefficients[2], col = 3)
plot(y2 ~ xf, pch = 16, ylim = c(-1, 5))
abline(a = 1, b = 2, col = 2)
abline(a = reg2$coefficients[1], b = reg2$coefficients[2], col = 3)

```

A large $R^2$ means *nothing* if the **assumptions of the model do not hold**.

```{r}

# Create data that: 
# 1) does NOT follow a linear model
# 2) the error is heteroskedastic
xf <- seq(0.15, 1, l = 100)
y3 <- 0.2 * sin(2 * pi * xf) + rnorm(n = 100, sd = 0.1 * xf^2)

# Great R^2!?
reg3 <- lm(y3 ~ xf)
summary(reg3)

# But predicting is obviously problematic
plot(y3 ~ xf, pch = 16)
lines(0.2 * sin(2 * pi * xf) ~ xf, col = 2)
abline(reg3, col = 3)

```
Simple linear regression

$$Y_i=a+bX_i+\varepsilon,\quad i=1,\ldots,n.$$

## Bookdown help

You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods).

Figures and tables with captions will be placed in `figure` and `table` environments, respectively.

```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab).

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(iris, 20), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

You can write citations, too. For example, we are using the **bookdown** package [@R-bookdown] in this sample book, which was built on top of R Markdown and **knitr** [@xie2015].
 --> 
