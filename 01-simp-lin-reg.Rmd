
# Simple linear regression {#simp}

The simple linear regression is a *simple* but useful statistical model. In short, it allows to analyse the (assumed) linear relation between two variables in a proper way. To convince you why is useful, let's see what it can do in real-case scenarios!

## Case study I: PISA scores and GDPp {#pisa}

The Programme for International Student Assessment (PISA) is a study carried out by the Organisation for Economic Co-operation and Development (OECD) in 65 countries with the purpose of evaluating the performance of 15-year-old pupils on mathematics, science, and reading. A phenomena observed over years is that *wealthy countries tend to achieve larger average scores*. The purpose of this case study, motivated by the @PISA inform, is to answer two questions related with the previous statement:

- Q1. *Is the educational level of a country influenced by its economic wealth?*
- Q2. *If so, up to what precise extent?*

The `pisa.csv` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/pisa.csv)) contains 65 rows corresponding to the countries that took part on the PISA study. The data was obtained merging the [statlink](http://dx.doi.org/10.1787/888932937035) in @PISA2012 with @WB data. Each row has the following variables: `Country`; `MathMean`, `ReadingMean` and `ScienceMean` (the average performance of the students in mathematics, reading and science); `MathShareLow` and `MathShareTop` (percentages of students with a low and top performance in mathematics); `GDPp` and `logGDPp` (the Gross Domestic Product per capita and its logarithm); `HighIncome` (whether the country has a GDPp larger than 20000\$ or not). The GDPp of a country is a measure of how many economic resources are available per citizen. The `logGDPp` is the logarithm of the GDPp, taken in order to avoid scale distortions. A small subset of the data is shown in Table \@ref(tab:pisatable). 

```{r, pisatable, echo = FALSE, out.width = '90%', fig.align = 'center'}
pisa <- read.csv(file = "datasets/pisa.csv", header = TRUE)
knitr::kable(
  head(pisa[, c(1, 2, 5, 6, 8, 9)], 10),
  booktabs = TRUE,
  longtable = TRUE,
  caption = 'First 10 rows of the `pisa` dataset for a selection of variables. Note the `NA` (*Not Available*) in Chinese Taipei (or Taiwan).'
)
```

We definitely need a way of **summarizing this ammount of information**!

We are going to do the following. First, import the data into `R Commander` and do a basic manipulation of it. Second, fit a linear model and interpret its output. Finally, visualize the fitted line and the data.

1. **Import the data into `R Commander`**.

    * Go to `'Data' -> 'Import data' -> 'from text file, clipboard, or URL...'`. A window like Figure \@ref(fig:read) will pop-up. Select the appropiate formatting options of the data file: whether the first row contains the name of the variables, what is the indicator for missing data, what is the field separator, and what is the decimal point character. Then click `'OK'`. 
      
        ```{block, type = 'rmdtip'}
        Inspecting the data file in a text editor will give you the right formatting choices for importing the data.
        ```
      
        ```{r, read, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Data importation options.'}
        row.names(pisa) <- as.character(pisa$Country)
        pisa$Country <- NULL
        knitr::include_graphics("images/screenshots/read.png")
        ```

    * Click on `'View data set'` to check that the importation was fine. If the data looks weird, then recheck the structure of the data file and restart from the above point. 
    
    * Since each row corresponds to a different country, we are going to name the rows as the value of the variable `Country`. To that end, go to `'Data' -> 'Active data set' -> 'Set case names...'` and select the variable `Country` and click `'OK'`. The dataset should look like Figure \@ref(fig:view).

        ```{r, view, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Correct importation of the `pisa` dataset.'}
        knitr::include_graphics("images/screenshots/view.png")
        ```
    
        ```{block, type = 'rmdcaution'}
        In UC3M computers, **altering the location of a downloaded file may cause errors** in its importation to `R Commander`!
        
        Example:
        
        - Default download path: `'C:/Users/g15s4021/Downloads/pisa.csv'`. Importation from that path works fine.
        - If you **move the file another location** (e.g. to `'C:/Users/g15s4021/Desktop/pisa.csv'`). Importation generates an **error**.
        ```

2. **Fit a simple linear regression**.

    * Go to `'Statistics' -> 'Fit models' -> 'Linear regression...'`. A window like Figure \@ref(fig:lm) will pop-up. 
    
        ```{r, lm, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Window for performing linear regression.'}
        knitr::include_graphics("images/screenshots/lm.png")
        ```
        
        Select the *response variable*. This is the variable denoted by $Y$ that we want to predict/explain. Then select the *explanatory variable* (also known as the *predictor*). It is denoted by $X$ and is the variable used to predict/explain $Y$. Recall the form of the linear model:
        $$
        Y=\beta_0+\beta_1X+\varepsilon
        $$
        
        In our case $Y=$`MathMean` and  $X=$`logGDPp`, so select them and click `'OK'`^[In principle, you could pick more than one explanatory variables using the `'Control'` or `'Shift'` keys, but that corresponds to the *multiple linear regression* (covered in Chapter 3).]. 
    
        ```{block, type = 'rmdtip'}
        If you want to deselect an option in an `R Commander` menu, use `'Control' + 'Mouse click'`.
        ```
        
        ```{block, type = 'rmdtip'}
        Four buttons are common in the menus of `R Commander`:
        
        - `'OK'`: executes the selected action, then closes the window.
        - `'Apply'`: executes the selected action but leaves the window open. Useful if you are experimenting with different options.
        - `'Reset'`: resets the fields and boxes of the window to their defaults.
        - `'Cancel'`: exits the window without performing any action.
        ```
    
    * The window in Figure \@ref(fig:lm) generates this code and output:
    
        ```{r}
        pisaLinearModel <- lm(MathMean ~ logGDPp, data = pisa)
        summary(pisaLinearModel)
        ```
    
        This is the linear model of `MathMean` regressed on `logGDPp` (first line) and its summary (second line). The summary gives the coefficients of the line and the $R^2$ (`'Multiple R-squared'`), which -- as we will see in Section \@ref(fit) -- it can be regarded as an *indicator of the strength of the linear relation between the variables*. ($R^2=1$ is a perfect linear fit -- all the points lay in a line -- and $R^2=0$ is the poorest fit.)
        
        The fitted regression line is `MathMean` $= 185.16 + 28.79\,\times$ `logGDPp`. The slope coefficient is positive, which indicates that there is a positive correlation between the wealth of a country and its performance in the PISA Mathematics test (this answers Q1). Hence, the evidence that *wealthy countries tend to achieve larger average scores* is indeed true (at least for the Mathematics test). We can be more precise on the effect of the wealth of a country. According to the fitted linear model, an increase of 1 unit in the `logGDPp` of a country is associated with achieving, on average, 28.79 additional points in the test (Q2).
    
3. **Visualize the fitted regression line**.

    * Go to `'Graphs' -> 'Scatterplot...'`. A window with two panels will pop-up (Figures \@ref(fig:scatter1) and \@ref(fig:scatter2)).
    
        ```{r, scatter1, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot window, `\'Data\'` panel.'}
        knitr::include_graphics("images/screenshots/scatterplot1.png")
        ```
        ```{r, scatter2, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot window, `\'Options\'` panel. Remember to tick the `\'Least-squares line\'` box in order to display the fitted regression line.'}
        knitr::include_graphics("images/screenshots/scatterplot2.png")
        ```
    
        On the `'Data'` panel, select the $X$ and $Y$ variables to be displayed in the scatterplot. On the `'Options'` panel, check the `'Least-squares line'` box and choose to identify `'3'` points `'Automatically'`^[The decision of which points are the most *different* from the rest is done automatically by a method known as the *Mahalanobis depth*.]. This will identify what are the three^[The default GUI option is set to identify `'2'` points. However, we know after a preliminary plot that there are three very different points in the dataset, hence this particular choice.] most different observations of the data.

    * The following `R` code will be generated. It produces a scatterplot of `MathMean` vs `logGDPp`, with its scorresponding regression line.
        ```{r, echo = FALSE, warning = FALSE}
        library(splines)
        library(car)
        library(sandwich)
        ```
        ```{r, out.width = '90%', fig.asp = 1, fig.align = 'center'}
        scatterplot(MathMean ~ logGDPp, reg.line = lm, smooth = FALSE, spread = FALSE,
                    id.method = 'mahal', id.n = 3, boxplots = FALSE, span = 0.5,
                    ellipse = FALSE, levels = c(.5, .9), main = "Average Math score
                    vs. logGDPp", pch = c(16), data = pisa)
        ```
        
        There are three clear *outliers*^[The outliers have a considerable impact on the regression line, as we will see later.]: Vietnam, Shanghai-China and Qatar. The first two are non high-income economies that perform exceptionally well in the test (although Shanghai-China is a cherry-picked region of China). On the other hand, Qatar is a high-income economy that has really poor scores.
    
        We can identify countries that are above and below the linear trend in the plot. This is particularly interesting: we can assess whether a country is performing better or worse with respect to its *expected PISA score according to its economic status* (this adds more insight into Q2). To do so, we want to display the text labels in the points of the scatterplot. We can take a shortcut: copy and run in the input panel the next piece of code. It is a slightly modified version of the previous code (what are the differences?).
    
        ```{r, out.width = '90%', fig.asp = 1, fig.align = 'center', results = 'hide'}
        scatterplot(MathMean ~ logGDPp, reg.line = lm, smooth = FALSE, spread = FALSE, 
                    id.method = 'mahal', id.n = 65, id.cex = 0.75, boxplots = FALSE, 
                    span = 0.5, ellipse = FALSE, levels = c(.5, .9), main = "Average 
                    Math score vs. logGDPp", pch = c(16), cex = 0.75, data = pisa)
        ```
    
If you understood the previous analysis, then you should be able to perform the next ones on your own.

```{block, type = 'rmdexercise', out.width = '90%'}
Repeat the regression analysis (steps 2--3) for:

- `ReadingMean` regressed on `logGDPp`. Are the results similar to `MathMean` on `logGDPp`?
- `MathMean` regressed on `ReadingMean`. Compare it with `MathMean` on `ScienceMean`. Which pair of variables has the highest linear relation? Is that something expected?

Save the new models with different names to avoid overwriting the previous models!
```

## Case study II: Apportionment in the EU and US {#euus}

> Apportionment is the process by which seats in a legislative body are distributed among administrative divisions entitled to representation.
>
> --- Wikipedia article on [Apportionment (politics)](https://en.wikipedia.org/wiki/Apportionment_(politics))

The *European Parliament* and the *US House of Representatives* are two of the most important macro legislative bodies in the world. The distribution of seats in both cameras is designed to represent the different states that conform the federation (US) or union (EU). Both chambers were created under very different historical and political circumstances, which is reflected in the kinds of apportionment that they present. More specifically:

- In the **US**, the apportionment is neatly *fixed by the US Constitution*. Each of the 50 states is apportioned a number of seats that corresponds to its share of the total population of the 50 states, according to the most recent decennial census. Every state is guaranteed *at least 1 seat*. There are 435 seats.

- Until now, the apportionment in the **EU** was set by *treaties* (Nice, Lisbon), in which *negotiations* between countries took place. The [last accepted composition](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32013D0312) gives an allocation of seats based on the principle of "degressive proportionality"^[Less populated states are given more weight than its corresponding proportional share.] and somehow vague guidelines. It concludes with a commitment to establish a system to "allocate the seats between Member States in an objective, fair, durable and transparent way, translating the principle of degressive proportionality". The Cambridge Compromise [@CamCom] was a proposal in that direction that was not effectively implemented. Currently, every state is guaranteed a *minimum of 6 seats* and a *maximum of 96* for a grand total of 750 seats.

We know that there exist qualitative dissimilarities between both chambers, but we can not be more specific with the description at hand. The purpose of this case study is to quantify and visualize what are the differences between the apportionments of the two chambers, and how the linear regression can add insights on what is actually going on with the EU apportionment. The questions we want to answer are:

- Q1. *Can we quantify which chamber is more proportional?*
- Q2. *What are the over-represented and under-represented states in both chambers?*
- Q3. *How can we quantify the 'degressive proportionality' in the EU approportionment system? Was the Cambridge Compromise proposing a fairer representation?*

Let's begin by reading the data:

1. The `US_apportionment.xlsx` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/US_apportionment.xlsx)) contains the 50 US states entitled to representation. The variables are `State`, `Population2010` (from the last census) and `Seats2013â€“2023`. This is an Excel file that we can read using `'Data' -> 'Import data' -> 'from Excel file...'`. A window will pop-up, asking for the right options. We set them as in Figure \@ref(fig:excel), since we want the variable `State` to be the case names. After clicking in `'View dataset'`, the data should look like Figure \@ref(fig:US).

    ```{r, excel, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Importation of an Excel file.'}
    knitr::include_graphics("images/screenshots/excel.png")
    ```

    ```{r, US, echo = FALSE, out.width = '45%', fig.align = 'center', fig.cap = 'Correct importation of the `US` dataset.'}
    knitr::include_graphics("images/screenshots/US.png")
    ```

2. The `EU_apportionment.txt` file ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/EU_apportionment.txt)) contains 28 rows with the member states of the EU (`Country`), the number of seats assigned under different years (`Seats2011`, `Seats2014`), the Cambridge Compromise apportionment (`CamCom2011`), and the countries population^[According to [EuroStat](http://ec.europa.eu/eurostat/tgm/table.do?tab=table&init=1&language=en&pcode=tps00001) and the population stated in the Cambridge Compromise report.] (`Population2010`,`Population2013`). 

    ```{block, type = 'rmdexercise'}
    For this file, you should know how to:
    
    1. Inspect the file in a text editor and determine its formatting.
    2. Decide the right importation options and load it with the name `EU`.
    3. Set the case names as the variable `Country`.
    ```
    
    ```{r, EU, echo = FALSE}
    US <- RcmdrMisc::readXL("datasets/US_apportionment.xlsx", rownames = TRUE, header = TRUE, na = "", sheet = "Hoja1", stringsAsFactors = TRUE)
    EU <- read.table("datasets/EU_apportionment.txt", header = TRUE, na = "-", sep = "\t")
    row.names(EU) <- EU$Country
    EU$Country <- NULL
    knitr::kable(
      EU,
      booktabs = TRUE,
      longtable = TRUE,
      caption = 'The `EU` dataset with `Country` set as the case names.'
    )
    ```

We start by analyzing the `US` dataset. If there is indeed a direct proportionality in the apportionment, we would expect a direct, 1:1, relation between the *ratios of seats* and the *population per state*. Let's start by constructing these variables:

1. Switch the active dataset to `US`. An alternative way to do so is by `'Data' -> 'Active data set' -> 'Select active data set...'`.
2. Go to `'Data' -> 'Manage variables in active dataset...' -> 'Compute new variable...'`. 
3. Create the the variable `RatioSeats2013.2023` as shown in Figure \@ref(fig:newvar). Be **careful to not overwrite** the variable `Seats2013.2023`.

    ```{r, newvar, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Creation of the new variable `RatioSeats2013.2023`. The expression to compute is `Seats2013.2023/sum(Seats2013.2023)`.'}
    knitr::include_graphics("images/screenshots/newvar.png")
    ```

4. `'View dataset'` to check that the new variable is available.

```{block, type = 'rmdexercise'}
Repeat steps 1--3, conveniently adapted, to create the new variable `RatioPopulation2010`.
```
```{r, echo = FALSE}
US$RatioSeats2013.2023 <- US$Seats2013.2023 / sum(US$Seats2013.2023)
US$RatioPopulation2010 <- US$Population2010 / sum(US$Population2010)
```

Let's fit a regression line to the `US` data, with `RatioSeats2013.2023` as the response and `RatioPopulation2010` as the explanatory variable. If we name the model as `appUS`, you should get the following code and output:
```{r}
appUS <- lm(RatioSeats2013.2023 ~ RatioPopulation2010, data = US)
summary(appUS)
```
The fitted regression line is `RatioSeats2013.2023`$=0.000+1.005\,\times$`RatioPopulation2010` and has an $R^2=0.9991$ (`'Multiple R-squared'`), which means that the data is almost perfectly linearly distributed. Furthermore, the *intercept coefficient is not significant*. This is seen in the column `'Pr(>|t|)'`, which gives the $p$-values for the null hypotheses $H_0:\beta_0=0$ and $H_0:\beta_1=0$, respectively. The null hypothesis $H_0:\beta_0=0$ is *not rejected* ($p\text{-value}=0.407$; non-significant) whereas $H_0:\beta_1=0$ is rejected ($p\text{-value}=0$; significant)^[We will be able to say more about how these test are performed after Section \@ref(inference).]. Hence, we can conclude that the appropriation of seats in the US House of Representatives is indeed directly proportional to the population of each state (partially answers Q1).

If we make the scatterplot for the `US` dataset, we can see the almost perfect (up to integer rounding) 1:1 relation between the ratios "state seats"/"total seats" and "state population"/"aggregated population". We can set the scatterplot to automatically label the `'25'` most different points (select the numeric box with the mouse and type `'25'` -- the arrow buttons are limited to `'10'`) with their case names. As it is seen in Figure \@ref(fig:ratios), there is no state clearly over- or under-represented (Q2).

```{r, ratios, echo = FALSE, out.width = '90%', fig.asp = 1, results = 'hide', fig.align = 'center', fig.cap = 'The apportionment in the US House of Representatives compared with a linear fit.'}
scatterplot(RatioSeats2013.2023 ~ RatioPopulation2010, reg.line = lm, smooth = FALSE, 
            spread = FALSE, id.method = 'mahal', id.n = 25, id.cex = 0.75, 
            boxplots = FALSE, span = 0.5, ellipse = FALSE, levels = c(.5, .9), 
            main = "Approportionment in the US House of Representatives", pch = c(16), 
            cex = 0.75, data = US)
```

Let's switch to the `EU` dataset, for which we will focus on the 2011 variables. A quick way of visualizing this dataset and, in general, of **visualizing multivariate data** (up to a moderate number of dimensions) is to use a *matrix scatterplot*. Essentially, it displays the scatterplots between all the pairs of variables. To do it, go to `'Graphs' -> 'Scatterplot matrix...'` and select the number of variables to be displayed. If you select them as in Figures \@ref(fig:mscatter1a) and \@ref(fig:mscatter1b), you should get an output like Figure \@ref(fig:mscatter2). 

```{r, mscatter1a, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot matrix window, `\'Data\'` panel.'}
knitr::include_graphics("images/screenshots/mscatter1a.png")
```
```{r, mscatter1b, echo = FALSE, out.width = '70%', fig.align = 'center', fig.cap = 'Scatterplot matrix window, `\'Options\'` panel. Be sure to tick the `\'Least-squares line\'` box in order to display the fitted regression line.'}
knitr::include_graphics("images/screenshots/mscatter1b.png")
```
```{r, mscatter2, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Scatterplot matrix for the variables `CamCom2011`, `Population2010` and `Seats2011` of `EU` dataset with boxplots in the central panels.'}
knitr::include_graphics("images/screenshots/mscatter2.png")
```

```{block, type = 'rmdexercise'}
The scatterplot matrix has a central panel displaying one-variable summary plots: histogram, density estimate, boxplot and QQ-plot. Experiment and understand them.
```

The most interesting panels in Figure \@ref(fig:mscatter2) for our study are `CamCom2011` vs `Population2010` -- panel (1,2) -- and `Seats2011` vs `Population2010` -- panel (3,2). At first sight, it seems that the Cambridge Compromise was favoring a fairer allocation of seats than what it was actually being used in the EU parliament in 2011 (recall the step-wise patterns in (3,2)). Let's explore in depth the scatterplot `Seats2011` vs `Population2010`.

```{r, echo = FALSE, results = 'hide', out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = '`Seats2011` vs `Population2010` in the `EU` dataset.'}
scatterplot(Seats2011 ~ Population2010, reg.line = lm, smooth = FALSE, spread = FALSE, 
            id.method = 'mahal', id.n = 25, id.cex = 0.75, boxplots = FALSE, span = 0.5, 
            ellipse = FALSE, levels = c(.5, .9), main = "Approportionment in the EU 
            parliament in 2011", pch = c(16), cex = 0.75, data = EU)
```

There are some countries clearly detrimented and benefited by this apportionment. For example, France and Spain are under-represented and, on the other hand, Germany, Hungary and Czech Republic are over-represented (Q2). 

Let's compute the regression line of `Seats2011` on `Population2010`, which we save in the model `appEU2011`.

```{r}
appEU2011 <- lm(Seats2011 ~ Population2010, data = EU)
summary(appEU2011)
```

The fitted line is `Seats2011`$=7.91+1.078\,\times10^{-6}\,\times$`Population2010`. The intercept is not zero and, indeed, the fitted intercept is significantly different from zero. Therefore, **there is no proportionality** in the apportionment. Recall that the fitted slope, despite being very small (why?), is also significantly different from zero. The $R^2$ is slightly smaller than in the `US` dataset, but definitely very high. Two conclusions stem from this analysis:

- The US House of Representatives is a proportional chamber whereas the EU parliament is definitely not, but is close to perfect linearity (completes Q1). 

- The principle of **digressive proportionality**, in practice, means an almost **linear allocation** of seats with respect to population (Q3). The main point is the presence of a **non-zero intercept** -- that is, a minimum number of seats corresponding to a country -- in order to over-represent smaller countries with respect to its corresponding proportional share.

The question that remains to be answered is whether the Cambridge Compromise was favoring a fairer allocation of seats than the 2011 official agreement. In Figure \@ref(fig:mscatter2) we can see that indeed it seems like that, but there is an outlier outside the linear pattern: Germany. There is an explanation for that: the EU commission imposed a cap to the maximum number of seats per country, 96, to the development of the Cambridge Compromise. With this rule, Germany is notably under-represented.

In order to avoid this distortion, we will exclude Germany from our comparison. To do so, we specify in the `'Subset expression'` field, of either `'Linear regression...'` or `'Scatterplot...'`, a `'-1'`. This tells `R` to exclude the first row of `EU` dataset, corresponding to Germany. Then, we compare the linear models for the official allocation, `appEUNoGer2011`, and the Cambridge Compromise, `appCamComNoGer2011`. The outputs are the following.

```{r}
appEUNoGer2011 <- lm(Seats2011 ~ Population2010, data = EU, subset = -1)
summary(appEUNoGer2011)
```
```{r}
appCamComNoGer2011 <- lm(CamCom2011 ~ Population2010, data = EU, subset = -1)
summary(appCamComNoGer2011)
```

We see that the Cambridge Compromise has a larger $R^2$ and a lower intercept than the official allocation of seats. This means that it favors a more proportional allocation, which is fairer in the sense that the deviations from the linear trend are smaller (Q3). We conclude the case study by illustrating both fits.

```{r, echo = FALSE, results = 'hide', out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = '`Seats2011` vs `Population2010` in the `EU` dataset, Germany excluded.'}
scatterplot(Seats2011 ~ Population2010, reg.line = lm, smooth = FALSE, spread = FALSE, 
            id.method = 'mahal', id.n = 25, id.cex = 0.75, boxplots = FALSE, span = 0.5,
            ellipse = FALSE, levels = c(.5, .9), main = "Approportionment in the EU 
            parliament (Germany excl.)", pch = c(16), cex = 0.75, data = EU, 
            subset = -1)
```

```{r, echo = FALSE, results = 'hide', out.width = '90%', fig.asp = 1, fig.align = 'center', fig.cap = '`CamCom2011` vs `Population2010` in the `EU` dataset, Germany excluded.'}
scatterplot(CamCom2011 ~ Population2010, reg.line = lm, smooth = FALSE, spread = FALSE, 
            id.method = 'mahal', id.n = 25, id.cex = 0.75, boxplots = FALSE, span = 0.5, 
            ellipse = FALSE, levels = c(.5, .9), main = "Proposal for approportionment 
            in the EU parliament (Germany excl.)", pch = c(16), cex = 0.75, data = EU, 
            subset = -1)
```

```{block, type = 'rmdexercise'}
In 2014 it was negotiated a new EU apportionment, collected in `Seats2014`, according to the population of 2013, `Population2013`, and due to the inclusion of Croatia in the EU. Answer these questions:

- Which countries were the most favored and unfavored by such apportionment?
- Was the apportionment proportional?
- Was the degree of linearity higher or lower than the 2011 apportionment? (Exclude Germany.)
- Was the degree of linearity higher or lower than the Cambridge Compromise for 2011? (Exclude Germany.)

```

***

We have performed a decent number of operations in `R Commander`. If we have to exit the session, we can save the data and models in a `.RData` file, which contains all the objects we have computed so far (but **not** the code -- this has to be saved differently). 

```{block, type = 'rmdtip'}
To exit `R Commander`, save all your progress and reload it later, do:

1. Save `.RData` file. Go to `'File' -> 'Save R workspace as...'`.
2. Save `.R` file. Go to `'File' -> 'Save script as...'`.
3. Exit `R Commander` + `R`. Go to `'File' -> 'Exit' -> 'From Commander and R'`. Choose to not save any file.
4. Start `R Commander` and load your files:

    - `.RData` file in `'Data' -> 'Load data set...'`,
    - `.R` file in `'File' -> 'Open script file...'`.

```

```{block, type = 'rmdtip'}
If you just want to save a dataset, you have two options:

- `'Data' -> 'Active data set' -> 'Save active data set...'`: it will be saved as an `.RData` file. The easiest way of importing it back to `R`.
- `'Data' -> 'Active data set' -> 'Export active data set...'`: it will be saved as a text file with the format that you choose. Useful for exporting data to other programs.

```

## Some `R` basics

By this time you probably had realized that some pieces of `R` code are repeated over and over, and that it is simpler to just modify them than to navigate the menus. For example, the codes `lm` and `scatterplot` always appear related with linear models and scatterplots. It is important to know some of the `R` basics in order to understand what are these pieces of text actually doing. Do not worry, the menus will always be there to generate the proper code for you -- but you need to have a *general idea* of the code.

In the following sections, **type** -- not copy and paste systematically -- the code in the `'R Script'` panel and send it to the output panel (on the selected expression, either with the `'Submit'` button or with `'Control' + 'R'`). 

We begin with the `lm` function, since it is the one you are more used to. In the following, you should get the same outputs (which are preceded by `## [1]`).

### The `lm` function

We are going to employ the `EU` dataset from Section \@ref(euus), with the case names set as the `Country`. In case you do not have it loaded, you can download it [here](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/EU.RData) as a `.RData` file.

```{r, collapse = TRUE, error = TRUE}
# First of all, this is a comment. Its purpose is to explain what the code is doing
# Comments are preceeded by a #

# lm has the syntax: lm(formula = response ~ explanatory, data = data)
# For example (you need to load first the EU dataset)
mod <- lm(formula = Seats2011 ~ Population2010, data = EU)

# We have saved the linear model into mod, which now contains all the output of lm
# You can see it by typing
mod

# mod is indeed a list of objects whose names are
names(mod)

# We can access these elements by $
# For example
mod$coefficients

# The residuals
mod$residuals

# The fitted values
mod$fitted.values

# Summary of the model
sumMod <- summary(mod)
sumMod
```


The following table contains a handy cheat sheet of equivalences between `R` code and some of the statistical concepts associated to linear regression.

|              `R`                       |        Statistical concept          |
|----------------------------------------|-------------------------------------|
| `x` | Predictor $X_1,\ldots,X_n$    | 
| `y` | Response $Y_1,\ldots,Y_n$    |
| `data <- data.frame(x = x, y = y)` | Sample $(X_1,Y_1),\ldots,(X_n,Y_n)$    | 
| `model <- lm(y ~ x, data = data)` | Fitted linear model |
| `model$coefficients` | Fitted coefficients $\hat\beta_0,\hat\beta_1$ |
| `model$residuals` | Fitted residuals $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$ |
| `model$fitted.values` | Fitted values $\hat Y_1,\ldots,\hat Y_n$ |
| `model$df.residual` | Degrees of freedom $n-2$ |
| `summaryModel <- summary(model)` | Summary of the fitted linear model |
| `summaryModel$sigma` | Fitted residual standard deviation $\hat\sigma$ |
| `summaryModel$r.squared` | Coefficient of determination $R^2$ |
| `summaryModel$fstatistic` | $F$-test |
| `anova(model)` | ANOVA table |

```{block, type = 'rmdexercise'}
Do the following:

- Compute the regression of `CamCom2011` into `Population2010`. Save that model as the variable `myModel`.
- Access the objects `residuals` and `coefficients` of `myModel`.
- Compute the summary of `myModel` and store it as the variable `summaryMyModel`.
- Access the object `sigma` of `myModel`.
- Repeat the previous steps changing the names of `myModel` and `summaryMyModel` to `otherMod` and `infoOtherMod`, respectively.
```

Now you know how to fit and summarize a linear model with a few keystrokes. Let's see more of the basics of `R` -- it will be useful for the next sections.

### Simple computations

```{r, echo = TRUE, error = TRUE, collapse = TRUE}
# These are some simple operations
# The console can act as a simple calculator
1.0 + 1.1
2 * 2
3/2
2^3
1/0
0/0

# Use ; for performing several operations in the same line
(1 + 3) * 2 - 1; 1 + 3 * 2 - 1

# Mathematical functions
sqrt(2); 2^0.5
sqrt(-1)
exp(1)
log(10); log10(10); log2(10)
sin(pi); cos(0); asin(0)
```

```{r, echo = TRUE, error = TRUE, collapse = TRUE}
# Remember to complete the expressions
1 + 
(1 + 3
```

### Variables and assignment

```{r, echo = TRUE, error = TRUE, collapse = TRUE}
# Any operation that you perform in R can be stored in a variable (or object)
# with the assignment operator "<-"
a <- 1

# To see the value of a variable, we simply type it
a

# A variable can be overwritten
a <- 1 + 1

# Now the value of a is 2 and not 1, as before
a 

# Careful with capitalization
A

# Different
A <- 3
a; A

# The variables are stored in your workspace (.RData file)
# A handy tip to see what variables are in the workspace
ls()
# Now you know which variables can be accessed!

# Remove variables
rm(a)
a
```

```{block, type = 'rmdexercise'}
Do the following:

- Store $-123$ in the variable `b`.
- Get the log of the square of `b`. (Answer: `9.624369`)
- Remove variable `b`.
```

### Vectors

```{r, echo = TRUE, collapse = TRUE, error = TRUE}

# These are vectors - arrays of numbers
# We combine numbers with the function c
c(1, 3)
c(1.5, 0, 5, -3.4)

# A handy way of creating sequences is the operator :
# Sequence from 1 to 5
1:5 

# Storing some vectors
myData <- c(1, 2)
myData2 <- c(-4.12, 0, 1.1, 1, 3, 4)
myData
myData2

# Entry-wise operations
myData + 1 
myData^2

# If you want to access a position of a vector, use [position]
myData[1]
myData2[6]

# You also can change elements
myData[1] <- 0
myData

# Think on what you want to access...
myData2[7]
myData2[0]

# If you want to access all the elements except a position, use [-position]
myData2[-1]
myData2[-2]

# Also with vectors as indexes
myData2[1:2]
myData2[myData]

# And also
myData2[-c(1, 2)]

# But do not mix positive and negative indexes!
myData2[c(-1, 2)]
```

```{block, type = 'rmdexercise'}
Do the following:

- Create the vector $x=(1, 7, 3, 4)$. 
- Create the vector $y=(100, 99, 98, ..., 2, 1)$.
- Compute $x_2+y_4$ and $\cos(x_3) + \sin(x_2) e^{-y_2}$. (Answers: `104`, `-0.9899925`)
- Set $x_{2}=0$ and $y_{2}=-1$. Recompute the previous expressions. (Answers: `97`, `2.785875`)
- Index $y$ by $x+1$ and store it as `z`. What is the output? (Answer: `z` is `c(-1, 100, 97, 96)`)

```

### Functions

```{r, echo = TRUE, collapse = TRUE, error = TRUE}
# Functions take arguments between parenthesis and transform them into an output
sum(myData)
prod(myData)

# Summary of an object
summary(myData)

# Length of the vector
length(myData)

# Mean, standard deviation, variance, covariance
mean(myData)
var(myData)
cov(myData, myData^2)
quantile(myData)

# Maximum and minimum of vectors
min(myData)
which.min(myData)

# Usually the functions have several arguments, which are set by "argument = value"
# In this case, the second argument is a logical flag to indicate the kind of sorting
sort(myData) # If nothing is specified, decreasing = FALSE is assumed
sort(myData, decreasing = TRUE)

# Don't know what are the arguments of a function? Use args and help!
args(sort)
?sort
```

```{block, type = 'rmdexercise'}
Do the following:

- Compute the mean, median and variance of $y$. (Answers: `49.5`, `49.5`, `843.6869`)
- Do the same for $y+1$. What are the differences?
- What is the maximum of $y$? Where is it placed?
- Sort $y$ increasingly and obtain the 5th and 76th positions. (Answer: `c(4,75)`)
- Compute the covariance between $y$ and $y$. Compute the variance of $y$. Why do you get the same result?
```

### Matrices, data frames and lists

```{r, collapse = TRUE, error = TRUE}
# A matrix is an array of vectors
A <- matrix(1:4, nrow = 2, ncol = 2)
A

# Another matrix
B <- matrix(1, nrow = 2, ncol = 2, byrow = TRUE)
B

# Binding by rows or columns
rbind(1:3, 4:6)
cbind(1:3, 4:6)

# Entry-wise operations
A + 1
A * B

# Accessing elements
A[2, 1] # Element (2, 1)
A[1, ] # First row
A[, 2] # First column

# A data frame is a matrix with column names
# Useful when you have multiple variables
myDf <- data.frame(var1 = 1:2, var2 = 3:4)
myDf

# You can change names
names(myDf) <- c("newname1", "newname2")
myDf

# The nice thing is that you can access variables by its name with the $ operator
myDf$newname1

# A list is a collection of arbitrary variables
myList <- list(myData = myData, A = A, myDf = myDf) 

# Access elements by names
myList$myData
myList$A
myList$myDf

# Reveal the structure of an object
str(myList)
str(myDf)

# A less lengthy output
names(myList)
```

```{block, type = 'rmdexercise'}
Do the following:

- Create a matrix called `M` with rows given by `y[3:5]`, `y[3:5]^2` and `log(y[3:5])`.
- Create a data frame called `myDataFrame` with column names "y", "y2" and "logy" containing the vectors `y[3:5]`, `y[3:5]^2` and `log(y[3:5])`, respectively.
- Create a list, called `l`, with entries for `x` and `M`. Access the elements by their names.
- Compute the squares of `myDataFrame` and save the result as `myDataFrame2`.
- Compute the log of the sum of `myDataFrame` and `myDataFrame2`. Answer:

          ##         y       y2     logy
          ## 1 9.180087 18.33997 3.242862
          ## 2 9.159678 18.29895 3.238784
          ## 3 9.139059 18.25750 3.234656

```

## Model formulation and estimation by least squares

The simple linear model is a statistical tool for describing the relation between two random variables, $X$ and $Y$. For example, in the `pisa` dataset, $X$ could be `ReadingMean` and $Y=$ `MathMean`. The linear model is *constructed by assuming* that the linear relation
\begin{equation}
Y = \beta_0 + \beta_1 X + \varepsilon \label{eq:1}
\end{equation}
holds between $X$ and $Y$. In (\@ref(eq:1)), $\beta_0$ and $\beta_1$ are known as the *intercept* and *slope*, respectively. $\varepsilon$ is a random variable with mean zero and independent from $X$. It describes the *error* around the mean, or the effect of other variables that we do not model. Another way of looking at (\@ref(eq:1)) is
\begin{equation}
\mathbb{E}(Y|X=x)=\beta_0+\beta_1x, \label{eq:2}
\end{equation}
since $\mathbb{E}(\varepsilon|X=x)=0$. 

The Left Hand Side (LHS) of (\@ref(eq:2)) is the *conditional expectation* of $Y$ given $X$. It represents how the mean of the random variable $Y$ is changing according to a particular value, denoted by $x$, of the random variable $X$. With the RHS, what we are saying is that the mean of $Y$ is changing in a *linear* fashion with respect to the value of $X$. Hence the interpretation of the coefficients:

- $\beta_0$: is the mean of $Y$ when $X=0$.
- $\beta_1$: is the increment in mean of $Y$ for an increment of one unit in $X=x$.

If we have a sample $(X_1,Y_1),\ldots,(X_n,Y_n)$ for our random variables $X$ and $Y$, we can estimate the unknown coefficients $\beta_0$ and $\beta_1$. In the `pisa` dataset, the sample are the observations for `ReadingMean` and `MathMean`. A possible way of estimating $(\beta_0,\beta_1)$ is by minimizing the *Residual Sum of Squares* (RSS):
$$
\text{RSS}(\beta_0,\beta_1)=\sum_{i=1}^n(Y_i-\beta_0-\beta_1X_i)^2.
$$
In other words, we look for the estimators $(\hat\beta_0,\hat\beta_1)$ such that
$$
(\hat\beta_0,\hat\beta_1)=\arg\min_{(\beta_0,\beta_1)\in\mathbb{R}^2} \text{RSS}(\beta_0,\beta_1).
$$

It can be seen that *the minimizers of the RSS*^[They are unique and always exist. They can be obtained by solving $\frac{\partial}{\partial \beta_0}\text{RSS}(\beta_0,\beta_1)=0$ and $\frac{\partial}{\partial \beta_1}\text{RSS}(\beta_0,\beta_1)=0$.] are
\begin{equation}
\hat\beta_0=\bar{Y}-\hat\beta_1\bar{X},\quad \hat\beta_1=\frac{S_{xy}}{S_x^2},\label{eq:3}
\end{equation}
where:

- $\bar{X}=\frac{1}{n}\sum_{i=1}^nX_i$ is the *sample mean*.
- $S_x^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2$ is the *sample variance*. The sample standard deviation is $S_x=\sqrt{S_x^2}$.
- $S_{xy}=\frac{1}{n}\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})$ is the *sample covariance*. It measures the degree of linear association between $X_1,\ldots,X_n$ and $Y_1,\ldots,Y_n$. Once scaled by $S_xS_y$, it gives the *sample correlation coefficient*, $r=\frac{S_{xy}}{S_xS_y}$.

There are some important points hidden behind the election of RSS as the error criterion for obtaining $(\hat\beta_0,\hat\beta_1)$:

- *Why the vertical distances and not horizontal or perpendicular?* Because we want to minimize the error in the *prediction* of $Y$! Note that the treatment of the variables is *not symmetrical*^[In Chapter 5 we will consider perpendicular distances.].
- *Why the squares in the distances and not the absolute value?* Due to mathematical convenience. Squares are nice to differentiate and are closely related with the Normal distribution.

Figure \@ref(fig:leastsquares) illustrates the influence of the distance employed in the sum of squares. Try to minimize the sum of squares for the different datasets. Is the best choice of intercept and slope independent of the type of distance?  

```{r, leastsquares, echo = FALSE, fig.cap = 'The effect of the kind of distance in the error criterion. The choices of intercept and slope that minimize the sum of squared distances for a kind of distance are not the optimal for a different kind of distance.', screenshot.opts = list(delay = 10), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://egarpor.shinyapps.io/least-squares/', height = '900px')
```

The data of the figure has been generated with the following code:
```{r}
set.seed(34567) # Fixes the seed for the random generator
x <- rnorm(n = 50) # Generates 50 points from a N(0, 1)
eps <- rnorm(n = 50) # Generates 50 points from a N(0, 1)
yLin <- -0.5 + 1.5 * x + eps
yQua <- -0.5 + 1.5 * x^2 + eps
yExp <- -0.5 + 1.5 * 2^x + eps
leastSquares <- data.frame(x = x, yLin = yLin, yQua = yQua, yExp = yExp)
```

```{block, type = 'rmdexercise'}
The minimizers of the error in the above illustration are indeed the coefficients given by the `lm` function. Check this for the three types of responses: `yLin`, `yQua` and `yExp`.
```

```{block, type = 'rmdinsight'}
The *population regression coefficients*, $(\beta_0,\beta_1)$, **are not the same** as the *estimated regression coefficients*, $(\hat\beta_0,\hat\beta_1)$:

- $(\beta_0,\beta_1)$ are the theoretical and **always** unknown quantities (except under controlled scenarios).
- $(\hat\beta_0,\hat\beta_1)$ are the estimates computed from the data. In particular, they are the output of `lm`. They are *random variables*, since they are computed from the random sample $(X_1,Y_1),\ldots,(X_n,Y_n)$.

In an abuse of notation, the term *regression line* is often used to denote both the *theoretical* ($y=\beta_0+\beta_1x$) and the *estimated* ($y=\hat\beta_0+\hat\beta_1x$) regression lines.
```

Once we have the least squares estimates $(\beta_0,\beta_1)$, we can define the next two concepts:

- The *fitted values* $\hat Y_1,\ldots,\hat Y_n$, where 
$$
\hat Y_i=\hat\beta_0+\hat\beta_1X_i,\quad i=1,\ldots,n.
$$
They are the vertical projections of $Y_1,\ldots,Y_n$ into the fitted line (see Figure \@ref(fig:leastsquares)).

- The *estimated residuals* $\hat \varepsilon_1,\ldots,\hat \varepsilon_n$, where 
$$
\hat\varepsilon_i=Y_i-\hat Y_i,\quad i=1,\ldots,n.
$$
They are the vertical distances between actual data and fitted data. Hence, another way of writing the minimum RSS is $\sum_{i=1}^n(Y_i-\hat\beta_0-\hat\beta_1Y_i)^2=\sum_{i=1}^n\hat\varepsilon_i^2$.

To conclude this section, we check that the regression coefficients given by `lm` are indeed the ones given in (\@ref(eq:3)).

```{r, echo = TRUE, collapse = TRUE}
# Covariance
Sxy <- cov(x, yLin)

# Variance
Sx2 <- var(x)

# Coefficients
beta1 <- Sxy / Sx2
beta0 <- mean(yLin) - beta1 * mean(x) 
c(beta0, beta1)

# Output from lm
mod <- lm(yLin ~ x, data = leastSquares)
mod$coefficients
```

```{block, type = 'rmdexercise'}
Adapt the code conveniently for doing the same checking with

- $X=$`ReadingMean` and $Y=$`MathMean` from the `pisa` dataset. 
- $X=$`logGDPp` and $Y=$`MathMean`.
- $X=$`Population2010` and $Y=$`Seats2013.2023` from the `US` dataset.
- $X=$`Population2010` and $Y=$`Seats2011` from the `EU` dataset. 
```

## Assumptions of the model

Why do we need assumptions? To make **inference** on the model parameters. In other words, to infer properties about a the *unknown* population coefficients $\beta_0$ and $\beta_1$ from the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$.

```{r, linearmodel, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The key concepts of the linear model.'}
knitr::include_graphics("images/R/linearmodel.png")
```

The assumptions of the linear model are:

i. **Linearity**: $\mathbb{E}(Y|X=x)=\beta_0+\beta_1x$.
ii. **Homocedasticity**: $\mathbb{V}\text{ar}(\varepsilon_i)=\sigma^2$, with $\sigma^2$ constant for $i=1,\ldots,n$.
iii. **Normality**: $\varepsilon_i\sim\mathcal{N}(0,\sigma^2)$ for $i=1,\ldots,n$.
iv. **Independence of the errors**: $\varepsilon_1,\ldots,\varepsilon_n$ are independent (or uncorrelated, $\mathbb{E}(\varepsilon_i\varepsilon_j)=0$, $i\neq j$, since they are assumed to be Normal by iii).

A good one-line summary of the linear model is (independence is assumed)
$$
Y|X=x\sim \mathcal{N}(\beta_0+\beta_1x,\sigma^2)
$$

```{block, type = 'rmdinsight'}
Recall:

- Nothing is said about the distribution of $X$. Indeed, $X$ could be deterministic (called *fixed design*) or random (*random design*). 

- The linear model **assumes that $Y$ is continuous** due to the normality of the errors. However, **$X$ can be discrete**!
```

Figures \@ref(fig:linearmodelgood) and \@ref(fig:linearmodelbad) represent situations where the assumptions of the model are respected and violated, respectively. For the moment, we will focus on building the intuition for checking the assumptions visually. In the next chapter we will see more sophisticated detection methods and the alternatives to their failure.

```{r, linearmodelgood, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Perfectly valid linear models (all the assumptions are verified).'}
knitr::include_graphics("images/R/linearmodelgood.png")
```

```{r, linearmodelbad, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'Problematic linear models (a single assumption does not hold).'}
knitr::include_graphics("images/R/linearmodelbad.png")
```

```{block, type = 'rmdexercise'}
The dataset `assumptions.RData` ([download](https://raw.githubusercontent.com/egarpor/SSS2-UC3M/master/datasets/assumptions.RData)) contains the variables `x1`, ..., `x9` and `y1`, ..., `y9`. For each regression `y1 ~ x1`, ..., `y9 ~ x9`:

- Check whether the assumptions of the linear model are being satisfied. 
- State which assumption(s) are violated and justify your answer.
```

## Inference for the model coefficients {#inference}

The assumptions introduced in the previous section allow to specify what is the distribution of the *random variables* $\hat\beta_0$ and $\hat\beta_1$. As we will see, this is a key point for making inference on $\beta_0$ and $\beta_1$.

The distributions are derived conditionally on the sample predictors $X_1,\ldots,X_n$. In other words, we assume that the randomness of $Y_i=\beta_0+\beta_1X_i+\varepsilon_i$, $i=1,\ldots,n$, comes only from the error terms and not from the predictors. To denote this, we employ lowercase for the sample predictors $x_1,\ldots,x_n$.

### Distributions of the fitted coefficients

The distributions of $\hat\beta_0$ and $\hat\beta_1$ are:
\begin{equation}
\hat\beta_0\sim\mathcal{N}\left(\beta_0,\mathrm{SE}(\hat\beta_0)^2\right),\quad\hat\beta_1\sim\mathcal{N}\left(\beta_1,\mathrm{SE}(\hat\beta_1)^2\right)\label{eq:norm1}
\end{equation}
where
\begin{equation}
\mathrm{SE}(\hat\beta_0)^2=\frac{\sigma^2}{n}\left[1+\frac{\bar X^2}{S_x^2}\right],\quad \mathrm{SE}(\hat\beta_1)^2=\frac{\sigma^2}{nS_x^2}.\label{eq:se1}
\end{equation}
Recall that an equivalent form for (\@ref(eq:norm1)) is (why?)
$$
\frac{\hat\beta_0-\beta_0}{\mathrm{SE}(\hat\beta_0)}\sim\mathcal{N}(0,1),\quad\frac{\hat\beta_1-\beta_1}{\mathrm{SE}(\hat\beta_1)}\sim\mathcal{N}(0,1).
$$

Some important remarks on (\@ref(eq:norm1)) and (\@ref(eq:se1)) are

- **Bias**. Both estimates are unbiased. That means that their expectations are the true coefficients.
- **Variance**. The variances $\mathrm{SE}(\hat\beta_0)^2$ and $\mathrm{SE}(\hat\beta_1)^2$ have an interesting interpretation:

    - *Sample size $n$*. As the sample size grows, the precision of the estimators increases, since both variances decrease.
    - *Error variance $\sigma^2$*. The more disperse the $Y$'s are, the less precise are the estimates are.
    - *Predictor variance $S_x^2$*. If the predictor is spread out (large $S_x^2$), then it is easier to "anchor" the slope. 
    - *Mean $\bar X$*. It has influence only on the precision of $\hat\beta_0$. The larger $\bar X$ is, the less precise $\hat\beta_0$ is.


```{r, randomcoefs, echo = FALSE, fig.cap = 'Illustration of the randomness of the fitted coefficients $(\\hat\\beta_0,\\hat\\beta_1)$ and the influence of $n$, $\\sigma^2$ and  $S_x^2$. The sample predictors $x_1,\\ldots,x_n$ are fixed and new responses $Y_1,\\ldots,Y_n$ are generated each time from a linear model $Y=\\beta_0+\\beta_1X+\\varepsilon$.', screenshot.opts = list(delay = 10), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://egarpor.shinyapps.io/lm-random/', height = '1000px')
```

The problem with (\@ref(eq:norm1)) and (\@ref(eq:se1)) is that *$\sigma^2$ is unknown* in practice, so we need to estimate $\sigma^2$ from the data. We do so by computing the *sample variance of the fitted residuals* $\hat\varepsilon_1,\ldots,\hat\varepsilon_n$. First note that the fitted residuals have zero mean, as it can be easily seen:
\begin{equation}
\bar\varepsilon =\frac{1}{n}\sum_{i=1}^n(Y_i-\hat\beta_0-\hat\beta_1X_i)=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar Y+\hat\beta_1\bar X-\hat\beta_1X_i)=0.
\end{equation}
Due to this, we can and we can do it by computing a *rescaled* sample variance of the fitted residuals:
$$
\hat\sigma^2=\frac{\sum_{i=1}^n\hat\varepsilon_i^2}{n-2}.
$$
Note the $n-2$ in the denominator, instead of $n$! $n-2$ are the *degrees of freedom* and is the number of data points minus the number of already fitted parameters. The interpretation is that "we have consumed $2$ degrees of freedom of the sample on fitting $\hat\beta_0$ and $\hat\beta_1$".

If we use the estimate $\hat\sigma^2$ instead of $\sigma^2$, we get different -- and more useful -- distributions for $\beta_0$ and $\beta_1$:
\begin{equation}
\frac{\hat\beta_0-\beta_0}{\hat{\mathrm{SE}}(\hat\beta_0)}\sim t_{n-2},\quad\frac{\hat\beta_1-\beta_1}{\hat{\mathrm{SE}}(\hat\beta_1)}\sim t_{n-2}\label{eq:norm3}
\end{equation}
where $t_{n-2}$ represents the *Student's $t$ distribution*^[The Student's $t$ distribution has *heavier tails* than the normal, which means that large observations in absolute value are more likely. $t_n$ converges to a $\mathcal{N}(0,1)$ when $n$ is large. For example, with $n$ larger than $30$, the normal is a good approximation.] with $n-2$ degrees of freedom and
\begin{equation}
\hat{\mathrm{SE}}(\hat\beta_0)^2=\frac{\hat\sigma^2}{n}\left[1+\frac{\bar X^2}{S_x^2}\right],\quad \hat{\mathrm{SE}}(\hat\beta_1)^2=\frac{\hat\sigma^2}{nS_x^2}\label{eq:se2}
\end{equation}
are the estimates of $\mathrm{SE}(\hat\beta_0)^2$ and $\mathrm{SE}(\hat\beta_1)^2$. The LHSs of (\@ref(eq:norm3)) are called *$t$-statistics* because of their distribution.

### Confidence intervals for the coefficients

Due to (\@ref(eq:norm3)) and (\@ref(eq:se2)), we can have the $100(1-\alpha)\%$ Confidence Intervals (CI) for the coefficients:
\begin{equation}
\left(\hat\beta_j-\hat{\mathrm{SE}}(\hat\beta_j)t_{n-2;\alpha/2},\hat\beta_j+\hat{\mathrm{SE}}(\hat\beta_j)t_{n-2;\alpha/2}\right),\quad j=0,1,\label{eq:ci}
\end{equation}
where $t_{n-2;\alpha/2}$ is the *$\alpha/2$-upper quantile of the distribution $t_{n-2}$* (see Figure \@ref(fig:ttest)). Usually, $\alpha=0.10,0.05,0.01$ are considered.

```{r, ttest, echo = FALSE, out.width = '90%', fig.align = 'center', fig.cap = 'The Student\'s $t$ distribution for the $t$-statistics associated to null intercept and slope, for the `y1 ~ x1` regression of the `assumptions` dataset.'}
knitr::include_graphics("images/R/ttest.png")
```

```{block, type = 'rmdtip'}
Do you need to remember the above equations? **No, althought you need to fully understand them.** `R` + `R Commander` will compute everything for you through the functions `lm`, `summary` and `confint`.
```

This *random* CI *contains the unknown coefficient $\beta_j$ with a probability of $1-\alpha$*. A simple way of understanding this concept is as follows. Suppose you have 100 samples generated according to a linear model. If you compute the CI for a coefficient, then in approximately $100(1-\alpha)$ of the samples the true coefficient would be actually inside the random CI. This is illustrated in Figure \@ref(fig:ci).

```{r, ci, echo = FALSE, fig.cap = 'Illustration of the randomness of the CI for $\\beta_0$ at $100(1-\\alpha)\\%$ confidence. The plot shows 100 rndom CIs for $\\beta_0$, computed from 100 random datasets generated by the same linear model, with intercept $\\beta_0$. The illustration for $\\beta_1$ is completely analogous.', screenshot.opts = list(delay = 10), dev = 'png', cache = TRUE, fig.align = 'center', out.width = '90%'}
knitr::include_app('https://egarpor.shinyapps.io/ci-random/', height = '900px')
```

Let's see how we can compute the CIs for $\beta_0$ and $\beta_1$ in practice. We do it in the first regression of the `assumptions` dataset. Assuming you have loaded the dataset, in `R` we can simply type:
```{r, echo = FALSE}
load("datasets/assumptions.RData")
```
```{r, collapse = TRUE}
mod1 <- lm(y1 ~ x1, data = assumptions)
confint(mod1)
```
In this example, the $95\%$ confidence interval for $\beta_0$ is $(-0.2256901, 0.1572392)$. For $\beta_1$ is $(-0.5587490, -0.2881032)$. Therefore, we can say that **with a $95\%$ confidence `x1` has a negative effect on `y1`**. If the CI for $\beta_1$ was $(-0.2256901, 0.1572392)$, we could not arrive to the same conclusion, since the CI contains both positive and negative numbers.

By default, the confidence interval is computed for $\alpha=0.05$. You can change this on the `level` argument, for example:
```{r, collapse = TRUE}
confint(mod1, level = 0.90) # alpha = 0.10
confint(mod1, level = 0.95) # alpha = 0.05
confint(mod1, level = 0.99) # alpha = 0.01
```

```{block, type = 'rmdinsight'}
Note that the larger the confidence of the interval, the longer -- thus less useful -- it is. For example, the interval $(-\infty,\infty)$ contains any coefficient with a 100% confidence, but is completely useless.
```

If you want to make the CIs through the help of `R Commander` (assuming the dataset has been loaded and is the active one), then do the following:

1. Fit the linear model (`'Statistics' -> 'Fit models' -> 'Linear regression...'`).
2. Go to `'Models' -> 'Confidence intervals...'` and then input the `'Condifence Level'`.

```{block, type = 'rmdexercise'}
Compute the CIs for the coefficients for the coefficients of the regressions `y2 ~ x2`, `y6 ~ x6` and `y7 ~ x7`. Do you think all of them are meaningful? Which ones are and why?
```

```{block, type = 'rmdexercise'}
Compute the CIs for the coefficients of the following regressions:

- `MathMean ~ ScienceMean`. 
- `MathMean ~ ReadingMean`
- `Seats2013.2023 ~ Population2010`
- `CamCom2011 ~ Population2010`

For the above regressions, can we conclude with a $95\%$ confidence that the effect of the predictor is positive in the response?

```

A CI for $\sigma^2$ can be also computed, but is less important in practice. The formula is:
$$
\left(\frac{\hat\sigma^2}{n-2}\chi^2_{n-2;\alpha/2},\frac{\hat\sigma^2}{n-2}\chi^2_{n-2;1-\alpha/2}\right)
$$
where $\chi^2_{n-2;q}$ is the $q$-upper quantiles of the *$\chi^2$ distribution*^[$\chi_n^2$ is the distribution of the sum of the squares of $n$ random variables $\mathcal{N}(0,1)$.] with $n-2$ degrees of freedom, $\chi^2_{n-2}$.

```{block, type = 'rmdexercise'}
Compute the CI for $\sigma^2$ for the regression of `MathMean` on `logGDPp` in the `pisa` dataset. To compute $\chi^2_{n-2;q}$, you can do:

- In `R` by `qchisq(p = q, df = n - 2, lower.tail = FALSE)`.
- In `R Commander`, go to `'Distributions' -> 'Continuous distributions' -> 'Chi-squared distribution' -> 'Chi-squared quantiles'` and then select `'Upper tail'`. Input $q$ as the `'Probabilities'` and $n-2$ as the `'Degrees of freedom'`.
```

### Testing on the coefficients

The distributions in (\@ref(eq:norm3)) also allow to conduct a formal *hypothesis test* on the coefficients $\beta_j$, $j=0,1$. For example the test for *significance* (shortcut for *significantly difference from zero*) is specially important, that is, the test of the hypotheses
$$
H_0:\beta_j=0
$$
for $j=0,1$. The test of $H_0:\beta_1=0$ is specially interesting, since it allows to answer whether *the variable $X$ has a significant linear effect on $Y$*. The statistic used for testing for significance is the $t$-statistic
$$
\frac{\hat\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)},
$$
which is distributed as a $t_{n-2}$ *under the (veracity of) the null hypothesis*. 

```{block, type = 'rmdinsight'}
Remember the analogy of hypothesis testing vs a trial, as given in the table below.
```

|         Hypothesis testing       |                      Trial                        |
|----------------------------------|---------------------------------------------------|
| Null hypothesis $H_0$ | **Accused of comitting a crime**. It has the "presumption of innocence", which means that it is *not guilty* until there is enough evidence to supporting its guilt |
| Sample $X_1,\ldots,X_n$ | Collection of small **evidences supporting innocence and guilt**  |
| Statistic $T_n$ | **Summary of the evicences presented** by the prosecutor and defense lawyer   |
| Distribution of $T_n$ under $H_0$ | The **judge** conducting the trial. Evaluates the evidence presented by both sides and presents a verdict for $H_0$  | 
| Significance level $\alpha$ | $1-\alpha$ is the **strength of evidences required by the judge for condemning $H_0$**. The judge allows evidences that on average condemn $100\alpha\%$ of the innocents! $\alpha=0.05$ is considered a reasonable level |
| $p$-value | **Decision** of the judge. If $p\text{-value}<\alpha$, $H_0$ is declared guilty. Otherwise, is declared not guilty |
| $H_0$ is rejected | $H_0$ is declared guilty: there are **strong evidences supporting its guilt** |
| $H_0$ is not rejected | $H_0$ is declared not guilty: either is **innocent or there are no enough evidences supporting its guilt** |

More formally, the $p$-value is defined as:

> The $p$-value is the probability of obtaining a statistic more unfavourable to $H_0$ than the observed, assuming that $H_0$ is true.

Therefore, **if the $p$-value is small** (smaller than the chosen level $\alpha$), **it is unlikely that the evidence against $H_0$ is due to randomness. As a consequence, $H_0$ is rejected**. If the $p$-value is large (larger than $\alpha$), then it is more possible that the evidences against $H_0$ are merely due to the randomness of the data. In this case, we do not reject $H_0$.

The null hypothesis $H_0$ is tested *against* the *alternative hypothesis*, $H_1$. If $H_0$ is rejected, it is *rejected in favor* of $H_1$. The alternative hypothesis can be *bilateral*, such as 
$$
H_0:\beta_j= 0\quad\text{vs}\quad H_1:\beta_j\neq 0
$$
or *unilateral*, such as
$$
H_0:\beta_j= 0\quad\text{vs}\quad H_1:\beta_j<0\quad(\text{or } H_1:\beta_j>0).
$$
For the moment, we will focus only on the bilateral case.

```{block, type = 'rmdinsight'}
The connection of a $t$-test for $H_0:\beta_j=0$ and the CI for $\beta_j$, both at level $\alpha$, is the following.

**Is $0$ inside the CI for $\beta_j$?**

- **Yes $\rightarrow$ do not reject $H_0$**. 
- **No $\rightarrow$ reject $H_0$**.
```

The tests for significance are built-in in the `summary` function, as we glimpsed in Section \@ref(euus). For `mod1`, we have:
```{r, collapse = TRUE}
summary(mod1)
```
The `Coefficients` block of the output of `summary` contains the next elements regarding the test $H_0:\beta_j=0$ vs $H_1:\beta_j\neq0$:

- `Estimate`: least squares estimate $\hat\beta_j$.
- `Std. Error`: estimated standard error $\hat{\mathrm{SE}}(\hat\beta_j)$.
- `t value`: $t$-statistic $\frac{\hat\beta_j}{\hat{\mathrm{SE}}(\hat\beta_j)}$.
- `Pr(>|t|)`: $p$-value of the $t$-test.
- `Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1`: codes indicating the size of the $p$-value. The more stars, the more evidence supporting that $H_0$ does not hold.

In the above output for `summary(mod1)`, $H_0:\beta_0=0$ is not rejected at any reasonable level for $\alpha$ (that is, $0.10$,$0.05$ and $0.01$). Hence $\beta_0$ is not significant. On the other hand, $H_0:\beta_1=0$ is rejected at any level $\alpha$ larger than the $p$-value, `3.77e-09`. Therefore, it is significant.

```{block, type = 'rmdexercise'}
For the `assumptions` dataset, do the next:

- Regression `y7 ~ x7`. Check that: 
    - The intercept of  is not significant at any reasonable level $\alpha$.
    - The slope is significant for any $\alpha>10^{-7}$.
  
- Regression `y6 ~ x6`. Assume the linear model assumptions are verified. 
    - Check that $\beta_0$ is significantly different from zero at any level
$\alpha$. 
    - For which $\alpha=0.10,0.05,0.01$ is $\beta_1$ significant?
```  

```{block, type = 'rmdexercise'}
Re-analyse the significance of the coefficients in `Seats2013.2023 ~ Population2010` and `Seats2011 ~ Population2010` for the `US` and `EU` datasets, respectively.
```

## Prediction of the mean and individual

`predict`

## Assesing model fit and ANOVA {#fit}

$R^2$

`anova`

## Nonlinear relationships

The linear model is termed *linear* not because the regression curve is a line, but because **the effects of the parameters $\beta_0$ and $\beta_1$ are linear**. Indeed, the predictor $X$ may exhibit a nonlinear effect on the response $Y$ and still be a linear model! For example, the following models can be transformed into simple linear models:

  1. $Y=\beta_0+\beta_1X^2+\varepsilon$
  2. $Y=\beta_0+\beta_1\log(X)+\varepsilon$
  3. $Y=\beta_0+\beta_1(X^3-\log(|X|) + 2^X)+\varepsilon$

The trick is to work with the transformed predictor ($X^2$, $\log(X)$, ...), instead of with the original variable $X$. Then, rather than working with the sample $(X_1,Y_1),\ldots,(X_n,Y_n)$, we consider the transformed sample $(\tilde X_1,Y_1),\ldots,(\tilde X_n,Y_n)$ with (for the above examples):

  1. $\tilde X_i=X_i^2$, $i=1,\ldots,n$.
  2. $\tilde X_i=X_i^3$, $i=1,\ldots,n$.
  3. $\tilde X_i=X_i^3-\log(|X_i|) + 2^{X_i}$, $i=1,\ldots,n$.

```{r, nonlineartransf, echo = FALSE, warning = FALSE, results = 'hide', out.width = '45%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Some common nonlinear transformations and their negative counterparts. Recall the domain of definition of each transformation.'}
x <- seq(-2, 5, l = 200)
plot(x, x, xlab = "x", ylab = "y", type = "l", col = 1, lwd = 2)
lines(x, x^2, col = 2, lwd = 2)
lines(x, x^3, col = 3, lwd = 2)
lines(x, sqrt(x), col = 4, lwd = 2)
lines(x, exp(x), col = 5, lwd = 2)
lines(x, exp(-x), col = 6, lwd = 2)
lines(x, log(x), col = 7, lwd = 2)
legend("bottomright", legend = expression(y == x, y == x^2, y == x^3, y == sqrt(x), y == exp(x), y == exp(-x), y == log(x)), lwd = 2, col = 1:7)
plot(x, -x, xlab = "x", ylab = "y", type = "l", col = 1, lwd = 2)
lines(x, -x^2, col = 2, lwd = 2)
lines(x, -x^3, col = 3, lwd = 2)
lines(x, -sqrt(x), col = 4, lwd = 2)
lines(x, -exp(x), col = 5, lwd = 2)
lines(x, -exp(-x), col = 6, lwd = 2)
lines(x, -log(x), col = 7, lwd = 2)
legend("topright", legend = expression(y == -x, y == -x^2, y == -x^3, y == -sqrt(x), y == -exp(-x), y == -exp(x), y == -log(x)), lwd = 2, col = 1:7)
```

An example of this simple but powerful trick is given as follows. The left panel of Figure \@ref(fig:quadratic) shows the scatterplot for some data `y` and  `x`, together with its fitted regression line. Clearly, the data does not follow a linear pattern, but a nonlinear one. In order to identify which one might be, we compare it against the set of mathematical functions displayed in Figure \@ref(fig:nonlineartransf). We see that the shape of the point cloud is similar to $y=x^2$. Hence, `y` might be better explained by the *square* of `x`, `x^2`, rather than by `x`. Indeed, if we plot `y` against `x^2` in the right panel of Figure \@ref(fig:quadratic), we can see that the fit of the regression line is much better.

In conclusion, with a simple trick we have increased drastically the explanation of the response. However, there is a catch: knowing which transformation is required in order to linearise the relation between response and the predictor is a kind of art which requires some good eye. This is partially alleviated by the extension of this technique to deal with *polynomials* rather than *monomials*, as we will see in the next chapter. For the moment, we will consider only the transformations displayed in Figure \@ref(fig:nonlineartransf).

```{r, quadratic, echo = FALSE, out.width = '45%', fig.show = 'hold', fig.asp = 1, fig.align = 'center', fig.cap = 'Left: quadratic pattern when plotting $Y$ against $X$. Right: linearizsd pattern when plotting $Y$ against $X^2$.'}
set.seed(345607)
x <- round(seq(-2, 5, l = 50), 1)
y <- round(0.5 * x^2 + rnorm(50), 1)
mod1 <- lm(y ~ x)
plot(x, y, pch = 16)
abline(mod1$coefficients, col = 2, lwd = 2)
mod2 <- lm(y ~ I(x^2))
plot(x^2, y, xlab = "x^2", pch = 16)
abline(mod2$coefficients, col = 2, lwd = 2)
```

```{block, type = 'rmdinsight'}
If you apply a nonlinear transformation, namely $f$, and fit the linear model $Y=\beta_0+\beta_1 f(X)+\varepsilon$, then there is no point in fit also the model resulting from the negative transformation $-f$. The model with $-f$ is exactly the same as the one with $f$ but with the sign of $\beta_1$ flipped! 

As a rule of thumb, use the figure with the transformations to compare it with the data pattern, then choose the most similar curve, and finally apply the corresponding function with **positive sign**.
```

Let's see how we can compute transformations of our predictors and perform a linear regression with them. The data for the above example is the following:
```{r}
# Data
x <- c(-2, -1.9, -1.7, -1.6, -1.4, -1.3, -1.1, -1, -0.9, -0.7, -0.6, 
       -0.4, -0.3, -0.1, 0, 0.1, 0.3, 0.4, 0.6, 0.7, 0.9, 1, 1.1, 1.3, 
       1.4, 1.6, 1.7, 1.9, 2, 2.1, 2.3, 2.4, 2.6, 2.7, 2.9, 3, 3.1, 
       3.3, 3.4, 3.6, 3.7, 3.9, 4, 4.1, 4.3, 4.4, 4.6, 4.7, 4.9, 5)
y <- c(1.4, 0.4, 2.4, 1.7, 2.4, 0, 0.3, -1, 1.3, 0.2, -0.7, 1.2, -0.1,
       -1.2, -0.1, 1, -1.1, -0.9, 0.1, 0.8, 0, 1.7, 0.3, 0.8, 1.2, 1.1,
       2.5, 1.5, 2, 3.8, 2.4, 2.9, 2.7, 4.2, 5.8, 4.7, 5.3, 4.9, 5.1, 
       6.3, 8.6, 8.1, 7.1, 7.9, 8.4, 9.2, 12, 10.5, 8.7, 13.5)

# Data frame (a matrix with column names)
nonLinear <- data.frame(x = x, y = y)
```
If we want to compute a new variable that contains the square of `x`, we can do it in two equivalent ways:

1. Through `R Commander`. In Section \@ref(euus) we saw how to create a new variable in our active dataset (remember Figure \@ref(fig:newvar)). Go to `'Data' -> 'Manage variables in active dataset...' -> 'Compute new variable...'`. Set the `'New variable name'` to `x2` and the `'Expression to compute'` to `x^2`.

2. Through `R`. Just type:

    ```{r}
    # We create a new column inside nonLinear, called x2, that contains nonLinear$x^2
    nonLinear$x2 <- nonLinear$x^2
    ```

With either the two previous points you will have a new variable called `x2`. If you wish to remove it, you can do it by either typing
```{r, eval = FALSE}
# Empties the column named x2
nonLinear$x2 <- NULL
```
or, in `R Commander`, by going to `'Data' -> 'Manage variables in active data set' -> 'Delete variables from data set...'` and select to remove  `x2`.

Now we are ready to perform the linear regression. If you do it directly through `R`, you will obtain:
```{r}
mod1 <- lm(y ~ x, data = nonLinear)
summary(mod1)
mod2 <- lm(y ~ x2, data = nonLinear)
summary(mod2)
```



```{block, type = 'rmdtip'}
A fast way of performing and summarizing the quadratic fit is 

    summary(lm(y ~ I(x^2), data = nonLinear))

Some remarks about this expression:

  - The `I()` function wrapping `x^2` is fundamental when applying arithmetic operations in the predictor. The symbols `+`, `\*`, `^`, ... have **diferrent meaning** when inputted in a formula, so is required to use `I()` to indicate that they must be interpreted in their arithmetic meaning and that the result of the expression denotes a new predictor variable. For example, use `I((x - 1)^3 - log(3 \* x))` if you want to apply the transformation `(x - 1)^3 - log(3 * x)`.
  
  - We are computing the `summary` of `lm` directly, without using an intermediate variable for storing the output of `lm`. This is perfectly valid and very handy, but note that you will not be able to access the information outputted by `lm`, only the one from `summary`.

```

```{block, type = 'rmdexercise'}
Load the dataset `assumptions.RData`. We are going to work with the regressions `y2 ~ x2`, `y3 ~ x3`, `y8 ~ x8` and `y9 ~ x9`, in order to identify which transformation gives the best fit. (For the purpose of illustration, we do not care if the assumptions are respected.) For these, do the following:

- Find the transformation that yields the largest $R^2$.
- Compare the original and the transformed linear models.

Some hints:

- `y2 ~ x2` has a negative dependence, so look at the left panel of the transformations figure.
- `y3 ~ x3` seems to have just a subtle nonlinearity...
- For `y9 ~ x9`, try with also with `exp(-abs(x))`, `log(abs(x))` and `2^abs(x)`. (`abs()` computes the absolute value.)
 
```

## Exercises and case studies

